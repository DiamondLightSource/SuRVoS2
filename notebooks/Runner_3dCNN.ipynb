{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcf4e71-ef73-40cf-92bb-9a7057d2e28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43063561-2b15-43b8-92a9-e847d53e0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt\n",
    "from survos2.frontend.nb_utils import start_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd7cb1b-5c02-4f9e-aafb-f151234ae14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b166e4-e7be-41dd-baf1-9c18e81656c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4927e322-0f5f-46de-804c-45bf813653bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ceph/users/fot15858/SuRVoS2/tests'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a4a75-8b35-4c6b-9d46-8b4a06bd911e",
   "metadata": {},
   "source": [
    "# Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31509575-9baf-41f7-8f61-a35aed22a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 8843\n",
    "loss_criterion = 'BCEDiceLoss'\n",
    "encoder_type = 'resnet50'\n",
    "workspace_name = 'vf_main_feb2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af7644a-13a8-478e-b9a7-fe2563c51a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#server_process = start_server(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63ef41-02c4-43c5-b094-1ff09d52ef93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8a0005-1ed8-4c11-9051-ea457ff5f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65484f0d-3274-445d-8f8f-26105cf1f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#server_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc7addb-7110-41b2-8303-8db017ba4046",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO - Running notebook for: U_NET_PLUS_PLUS \u001b[0m\u001b[32m - __main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc5b02e27e644789fd20b6e8eef6149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/47 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [138355]\n",
      "INFO - Started server process [138355]  - uvicorn.server:serve:75\n",
      "INFO:     Waiting for application startup.\n",
      "INFO - Waiting for application startup.  - uvicorn.lifespan.on:startup:47\n",
      "INFO:     Application startup complete.\n",
      "INFO - Application startup complete.  - uvicorn.lifespan.on:startup:61\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8843 (Press CTRL+C to quit)\n",
      "INFO - Uvicorn running on http://127.0.0.1:8843 (Press CTRL+C to quit)  - uvicorn.server:_log_started_message:207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:34656 - \"GET /workspace/set_workspace?workspace=vf_main_feb2023 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:34656 - \"GET /features/existing?workspace=vf_main_feb2023 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:34656 - \"GET /annotations/get_levels?workspace=vf_main_feb2023 HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Creating dataset on /ceph/users/fot15858/chroot/vf_main_feb2023/default/pipelines/011_train_3d_cnn [128, 1450, 1450] float32 None [128, 182, 112]  - survos2.model.dataset:create:241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:34656 - \"GET /pipelines/create?pipeline_type=train_3d_cnn&workspace=vf_main_feb2023 HTTP/1.1\" 200 OK\n",
      "()\n",
      "{'anno_id': '014_level',\n",
      " 'bce_to_dice_weight': 0.3,\n",
      " 'fcn_type': 'unet3d',\n",
      " 'feature_id': '001_raw',\n",
      " 'num_augs': 0,\n",
      " 'num_epochs': 1,\n",
      " 'num_samples': 400,\n",
      " 'objects_id': '002_points',\n",
      " 'patch_overlap': [16, 16, 16],\n",
      " 'patch_size': [64, 64, 64],\n",
      " 'threshold': 0.5,\n",
      " 'workspace': 'vf_main_feb2023'}\n",
      "Reading entity csv: /ceph/users/fot15858/chroot/vf_main_feb2023/default/objects/002_points/7fb633cdea3495da3a2d16d79634345f603562fc6f7fcc5c.csv\n",
      "     Unnamed: 0   z    x    y  class_code\n",
      "0             0  41  422  916           0\n",
      "1             1  52  956  227           0\n",
      "2             2  46  754  731           0\n",
      "3             3  79  756  670           0\n",
      "4             4  38  955  305           0\n",
      "..          ...  ..  ...  ...         ...\n",
      "859         859  60  334  401           0\n",
      "860         860  53  911  189           0\n",
      "861         861  59  339  307           0\n",
      "862         862  30  882  462           0\n",
      "863         863  38  353  195           0\n",
      "\n",
      "[864 rows x 5 columns]\n",
      "False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Making patches for 864 locations\n",
      "Making 864 bvols\n",
      "Plotting at location: 64, 725, 725\n",
      "Plotting at location: 64, 725, 725\n",
      "Generating 864 patch volumes from image of shape (256, 1578, 1578)\n",
      "Generated 864 MarkedPatches of shape (864, 64, 64, 64)\n",
      "Generating 864 patch volumes from image of shape (256, 1578, 1578)\n",
      "Generated 864 MarkedPatches of shape (864, 64, 64, 64)\n",
      "Marked patches, unique label vols [0. 1.], img mean: 0.5283096677361517\n",
      "raw_X_train (691, 64, 64, 64), raw_X_test (173, 64, 64, 64), raw_y_train(691, 64, 64, 64), raw_y_test(173, 64, 64, 64)\n",
      "Figure(1200x1200)\n",
      "Figure(1100x1100)\n",
      "Figure(1100x1100)\n",
      "Figure(1200x1200)\n",
      "Figure(1100x1100)\n",
      "Figure(1100x1100)\n",
      "Figure(800x800)\n",
      "Unique mask values: [0. 1.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0. 1.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0. 1.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0. 1.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0. 1.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0. 1.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0.]\n",
      "Figure(800x800)\n",
      "Unique mask values: [0.]\n",
      "Augmented image vols shape (864, 64, 64, 64), label vols shape (864, 64, 64, 64)\n",
      "Saving image vols /ceph/users/fot15858/chroot/vf_main_feb2023/default/annotations/014_level/vf_main_feb2023_patch_vols864_img_vols_0402_1302.h5\n",
      "Saving image vols /ceph/users/fot15858/chroot/vf_main_feb2023/default/annotations/014_level/vf_main_feb2023_patch_vols864_img_labels_0402_1302.h5\n",
      "Saving image vols /ceph/users/fot15858/chroot/vf_main_feb2023/default/annotations/014_level/vf_main_feb2023_patch_vols_864_mask_gt_0402_1302.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Saving fcn model to: /ceph/users/fot15858/chroot/vf_main_feb2023/fcn/04022023_13_02_18_trained_fcn_model  - survos2.api.pipelines:train_3d_cnn:730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data']>\n",
      "<KeysViewHDF5 ['data']>\n",
      "(864, 64, 64, 64) (864, 64, 64, 64)\n",
      "Prepared train X : (777, 64, 64, 64) and train y: (777, 64, 64, 64)  and test X: (87, 64, 64, 64) and test y (87, 64, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/777 [00:00<?, ?it/s]\n",
      "Training: (loss -0.0523):   0%|          | 0/777 [00:02<?, ?it/s]\n",
      "Training: (loss -0.0523):   0%|          | 1/777 [00:02<37:07,  2.87s/it]\n",
      "Training: (loss 0.2939):   0%|          | 1/777 [00:03<37:07,  2.87s/it] \n",
      "Training: (loss 0.2939):   0%|          | 2/777 [00:03<18:06,  1.40s/it]\n",
      "Training: (loss 0.2984):   0%|          | 2/777 [00:03<18:06,  1.40s/it]\n",
      "Training: (loss 0.2984):   0%|          | 3/777 [00:03<11:48,  1.09it/s]\n",
      "Training: (loss -0.1093):   0%|          | 3/777 [00:03<11:48,  1.09it/s]\n",
      "Training: (loss -0.1093):   1%|          | 4/777 [00:03<09:03,  1.42it/s]\n",
      "Training: (loss 0.2824):   1%|          | 4/777 [00:04<09:03,  1.42it/s] \n",
      "Training: (loss 0.2824):   1%|          | 5/777 [00:04<07:27,  1.73it/s]\n",
      "Training: (loss 0.0867):   1%|          | 5/777 [00:04<07:27,  1.73it/s]\n",
      "Training: (loss 0.0867):   1%|          | 6/777 [00:04<06:31,  1.97it/s]\n",
      "Training: (loss 0.0491):   1%|          | 6/777 [00:05<06:31,  1.97it/s]\n",
      "Training: (loss 0.0491):   1%|          | 7/777 [00:05<06:07,  2.09it/s]\n",
      "Training: (loss -0.0224):   1%|          | 7/777 [00:05<06:07,  2.09it/s]\n",
      "Training: (loss -0.0224):   1%|          | 8/777 [00:05<05:53,  2.18it/s]\n",
      "Training: (loss 0.0878):   1%|          | 8/777 [00:05<05:53,  2.18it/s] \n",
      "Training: (loss 0.0878):   1%|          | 9/777 [00:05<05:26,  2.35it/s]\n",
      "Training: (loss 0.0540):   1%|          | 9/777 [00:06<05:26,  2.35it/s]\n",
      "Training: (loss 0.0540):   1%|▏         | 10/777 [00:06<05:05,  2.51it/s]\n",
      "Training: (loss 0.1257):   1%|▏         | 10/777 [00:06<05:05,  2.51it/s]\n",
      "Training: (loss 0.1257):   1%|▏         | 11/777 [00:06<04:44,  2.69it/s]\n",
      "Training: (loss 0.2100):   1%|▏         | 11/777 [00:06<04:44,  2.69it/s]\n",
      "Training: (loss 0.2100):   2%|▏         | 12/777 [00:06<04:38,  2.74it/s]\n",
      "Training: (loss 0.1159):   2%|▏         | 12/777 [00:07<04:38,  2.74it/s]\n",
      "Training: (loss 0.1159):   2%|▏         | 13/777 [00:07<04:28,  2.85it/s]\n",
      "Training: (loss 0.3037):   2%|▏         | 13/777 [00:07<04:28,  2.85it/s]\n",
      "Training: (loss 0.3037):   2%|▏         | 14/777 [00:07<04:37,  2.75it/s]\n",
      "Training: (loss -0.0283):   2%|▏         | 14/777 [00:07<04:37,  2.75it/s]\n",
      "Training: (loss -0.0283):   2%|▏         | 15/777 [00:07<04:32,  2.80it/s]\n",
      "Training: (loss -0.0832):   2%|▏         | 15/777 [00:08<04:32,  2.80it/s]\n",
      "Training: (loss -0.0832):   2%|▏         | 16/777 [00:08<04:14,  3.00it/s]\n",
      "Training: (loss 0.2079):   2%|▏         | 16/777 [00:08<04:14,  3.00it/s] \n",
      "Training: (loss 0.2079):   2%|▏         | 17/777 [00:08<04:17,  2.95it/s]\n",
      "Training: (loss 0.2079):   2%|▏         | 17/777 [00:08<04:17,  2.95it/s]\n",
      "Training: (loss 0.2079):   2%|▏         | 18/777 [00:08<04:20,  2.91it/s]\n",
      "Training: (loss 0.2079):   2%|▏         | 18/777 [00:09<04:20,  2.91it/s]\n",
      "Training: (loss 0.2079):   2%|▏         | 19/777 [00:09<04:31,  2.79it/s]\n",
      "Training: (loss 0.2079):   2%|▏         | 19/777 [00:09<04:31,  2.79it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 20/777 [00:09<04:19,  2.91it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 20/777 [00:09<04:19,  2.91it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 21/777 [00:09<04:10,  3.02it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 21/777 [00:10<04:10,  3.02it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 22/777 [00:10<04:00,  3.14it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 22/777 [00:10<04:00,  3.14it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 23/777 [00:10<03:59,  3.15it/s]\n",
      "Training: (loss 0.2046):   3%|▎         | 23/777 [00:10<03:59,  3.15it/s]\n",
      "Training: (loss 0.2046):   3%|▎         | 24/777 [00:10<04:15,  2.94it/s]\n",
      "Training: (loss -0.4921):   3%|▎         | 24/777 [00:11<04:15,  2.94it/s]\n",
      "Training: (loss -0.4921):   3%|▎         | 25/777 [00:11<04:16,  2.93it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 25/777 [00:11<04:16,  2.93it/s] \n",
      "Training: (loss 0.2079):   3%|▎         | 26/777 [00:11<04:24,  2.84it/s]\n",
      "Training: (loss -0.4921):   3%|▎         | 26/777 [00:12<04:24,  2.84it/s]\n",
      "Training: (loss -0.4921):   3%|▎         | 27/777 [00:12<04:34,  2.73it/s]\n",
      "Training: (loss 0.2079):   3%|▎         | 27/777 [00:12<04:34,  2.73it/s] \n",
      "Training: (loss 0.2079):   4%|▎         | 28/777 [00:12<04:31,  2.75it/s]\n",
      "Training: (loss 0.2079):   4%|▎         | 28/777 [00:12<04:31,  2.75it/s]\n",
      "Training: (loss 0.2079):   4%|▎         | 29/777 [00:12<04:22,  2.85it/s]\n",
      "Training: (loss 0.2079):   4%|▎         | 29/777 [00:13<04:22,  2.85it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 30/777 [00:13<04:19,  2.88it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 30/777 [00:13<04:19,  2.88it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 31/777 [00:13<04:32,  2.74it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 31/777 [00:13<04:32,  2.74it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 32/777 [00:13<04:44,  2.61it/s]\n",
      "Training: (loss -0.4921):   4%|▍         | 32/777 [00:14<04:44,  2.61it/s]\n",
      "Training: (loss -0.4921):   4%|▍         | 33/777 [00:14<04:41,  2.65it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 33/777 [00:14<04:41,  2.65it/s] \n",
      "Training: (loss 0.2079):   4%|▍         | 34/777 [00:14<04:34,  2.71it/s]\n",
      "Training: (loss 0.2079):   4%|▍         | 34/777 [00:14<04:34,  2.71it/s]\n",
      "Training: (loss 0.2079):   5%|▍         | 35/777 [00:14<04:26,  2.79it/s]\n",
      "Training: (loss 0.2079):   5%|▍         | 35/777 [00:15<04:26,  2.79it/s]\n",
      "Training: (loss 0.2079):   5%|▍         | 36/777 [00:15<04:17,  2.87it/s]\n",
      "Training: (loss 0.2079):   5%|▍         | 36/777 [00:15<04:17,  2.87it/s]\n",
      "Training: (loss 0.2079):   5%|▍         | 37/777 [00:15<04:06,  3.00it/s]\n",
      "Training: (loss -0.4921):   5%|▍         | 37/777 [00:15<04:06,  3.00it/s]\n",
      "Training: (loss -0.4921):   5%|▍         | 38/777 [00:15<04:28,  2.76it/s]\n",
      "Training: (loss 0.2079):   5%|▍         | 38/777 [00:16<04:28,  2.76it/s] \n",
      "Training: (loss 0.2079):   5%|▌         | 39/777 [00:16<04:42,  2.61it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 39/777 [00:16<04:42,  2.61it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 40/777 [00:16<04:36,  2.67it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 40/777 [00:17<04:36,  2.67it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 41/777 [00:17<04:25,  2.78it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 41/777 [00:17<04:25,  2.78it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 42/777 [00:17<04:29,  2.72it/s]\n",
      "Training: (loss 0.2079):   5%|▌         | 42/777 [00:17<04:29,  2.72it/s]\n",
      "Training: (loss 0.2079):   6%|▌         | 43/777 [00:17<04:40,  2.62it/s]\n",
      "Training: (loss -0.4921):   6%|▌         | 43/777 [00:18<04:40,  2.62it/s]\n",
      "Training: (loss -0.4921):   6%|▌         | 44/777 [00:18<04:36,  2.65it/s]\n",
      "Training: (loss 0.2079):   6%|▌         | 44/777 [00:18<04:36,  2.65it/s] \n",
      "Training: (loss 0.2079):   6%|▌         | 45/777 [00:18<04:25,  2.76it/s]\n",
      "Training: (loss -0.4921):   6%|▌         | 45/777 [00:18<04:25,  2.76it/s]\n",
      "Training: (loss -0.4921):   6%|▌         | 46/777 [00:18<04:22,  2.78it/s]\n",
      "Training: (loss 0.2079):   6%|▌         | 46/777 [00:19<04:22,  2.78it/s] \n",
      "Training: (loss 0.2079):   6%|▌         | 47/777 [00:19<04:15,  2.86it/s]\n",
      "Training: (loss 0.2079):   6%|▌         | 47/777 [00:19<04:15,  2.86it/s]\n",
      "Training: (loss 0.2079):   6%|▌         | 48/777 [00:19<04:15,  2.85it/s]\n",
      "Training: (loss 0.2079):   6%|▌         | 48/777 [00:19<04:15,  2.85it/s]\n",
      "Training: (loss 0.2079):   6%|▋         | 49/777 [00:19<04:06,  2.96it/s]\n",
      "Training: (loss 0.2079):   6%|▋         | 49/777 [00:20<04:06,  2.96it/s]\n",
      "Training: (loss 0.2079):   6%|▋         | 50/777 [00:20<03:51,  3.13it/s]\n",
      "Training: (loss 0.2079):   6%|▋         | 50/777 [00:20<03:51,  3.13it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 51/777 [00:20<04:09,  2.90it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 51/777 [00:20<04:09,  2.90it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 52/777 [00:20<04:16,  2.82it/s]\n",
      "Training: (loss -0.4921):   7%|▋         | 52/777 [00:21<04:16,  2.82it/s]\n",
      "Training: (loss -0.4921):   7%|▋         | 53/777 [00:21<04:08,  2.91it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 53/777 [00:21<04:08,  2.91it/s] \n",
      "Training: (loss 0.2079):   7%|▋         | 54/777 [00:21<04:07,  2.92it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 54/777 [00:22<04:07,  2.92it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 55/777 [00:22<04:17,  2.80it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 55/777 [00:22<04:17,  2.80it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 56/777 [00:22<04:24,  2.72it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 56/777 [00:22<04:24,  2.72it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 57/777 [00:22<04:21,  2.75it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 57/777 [00:23<04:21,  2.75it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 58/777 [00:23<04:18,  2.78it/s]\n",
      "Training: (loss 0.2079):   7%|▋         | 58/777 [00:23<04:18,  2.78it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 59/777 [00:23<04:16,  2.79it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 59/777 [00:23<04:16,  2.79it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 60/777 [00:23<04:16,  2.79it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 60/777 [00:24<04:16,  2.79it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 61/777 [00:24<04:15,  2.81it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 61/777 [00:24<04:15,  2.81it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 62/777 [00:24<04:22,  2.72it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 62/777 [00:24<04:22,  2.72it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 63/777 [00:24<04:16,  2.78it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 63/777 [00:25<04:16,  2.78it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 64/777 [00:25<04:05,  2.91it/s]\n",
      "Training: (loss 0.2100):   8%|▊         | 64/777 [00:25<04:05,  2.91it/s]\n",
      "Training: (loss 0.2100):   8%|▊         | 65/777 [00:25<04:11,  2.83it/s]\n",
      "Training: (loss 0.2082):   8%|▊         | 65/777 [00:26<04:11,  2.83it/s]\n",
      "Training: (loss 0.2082):   8%|▊         | 66/777 [00:26<04:19,  2.75it/s]\n",
      "Training: (loss 0.2079):   8%|▊         | 66/777 [00:26<04:19,  2.75it/s]\n",
      "Training: (loss 0.2079):   9%|▊         | 67/777 [00:26<04:13,  2.80it/s]\n",
      "Training: (loss 0.1935):   9%|▊         | 67/777 [00:26<04:13,  2.80it/s]\n",
      "Training: (loss 0.1935):   9%|▉         | 68/777 [00:26<04:04,  2.90it/s]\n",
      "Training: (loss 0.2059):   9%|▉         | 68/777 [00:26<04:04,  2.90it/s]\n",
      "Training: (loss 0.2059):   9%|▉         | 69/777 [00:26<04:00,  2.94it/s]\n",
      "Training: (loss 0.2084):   9%|▉         | 69/777 [00:27<04:00,  2.94it/s]\n",
      "Training: (loss 0.2084):   9%|▉         | 70/777 [00:27<04:06,  2.87it/s]\n",
      "Training: (loss 0.2087):   9%|▉         | 70/777 [00:27<04:06,  2.87it/s]\n",
      "Training: (loss 0.2087):   9%|▉         | 71/777 [00:27<04:21,  2.70it/s]\n",
      "Training: (loss 0.2099):   9%|▉         | 71/777 [00:28<04:21,  2.70it/s]\n",
      "Training: (loss 0.2099):   9%|▉         | 72/777 [00:28<04:18,  2.73it/s]\n",
      "Training: (loss 0.1473):   9%|▉         | 72/777 [00:28<04:18,  2.73it/s]\n",
      "Training: (loss 0.1473):   9%|▉         | 73/777 [00:28<04:30,  2.60it/s]\n",
      "Training: (loss 0.2126):   9%|▉         | 73/777 [00:28<04:30,  2.60it/s]\n",
      "Training: (loss 0.2126):  10%|▉         | 74/777 [00:28<04:29,  2.61it/s]\n",
      "Training: (loss 0.2023):  10%|▉         | 74/777 [00:29<04:29,  2.61it/s]\n",
      "Training: (loss 0.2023):  10%|▉         | 75/777 [00:29<04:22,  2.67it/s]\n",
      "Training: (loss 0.2246):  10%|▉         | 75/777 [00:29<04:22,  2.67it/s]\n",
      "Training: (loss 0.2246):  10%|▉         | 76/777 [00:29<04:15,  2.74it/s]\n",
      "Training: (loss 0.2106):  10%|▉         | 76/777 [00:29<04:15,  2.74it/s]\n",
      "Training: (loss 0.2106):  10%|▉         | 77/777 [00:29<04:14,  2.76it/s]\n",
      "Training: (loss 0.1017):  10%|▉         | 77/777 [00:30<04:14,  2.76it/s]\n",
      "Training: (loss 0.1017):  10%|█         | 78/777 [00:30<04:18,  2.71it/s]\n",
      "Training: (loss 0.1712):  10%|█         | 78/777 [00:30<04:18,  2.71it/s]\n",
      "Training: (loss 0.1712):  10%|█         | 79/777 [00:30<04:07,  2.82it/s]\n",
      "Training: (loss 0.0529):  10%|█         | 79/777 [00:31<04:07,  2.82it/s]\n",
      "Training: (loss 0.0529):  10%|█         | 80/777 [00:31<04:10,  2.78it/s]\n",
      "Training: (loss 0.3010):  10%|█         | 80/777 [00:31<04:10,  2.78it/s]\n",
      "Training: (loss 0.3010):  10%|█         | 81/777 [00:31<04:12,  2.76it/s]\n",
      "Training: (loss -0.0016):  10%|█         | 81/777 [00:31<04:12,  2.76it/s]\n",
      "Training: (loss -0.0016):  11%|█         | 82/777 [00:31<04:14,  2.73it/s]\n",
      "Training: (loss 0.0777):  11%|█         | 82/777 [00:32<04:14,  2.73it/s] \n",
      "Training: (loss 0.0777):  11%|█         | 83/777 [00:32<03:55,  2.94it/s]\n",
      "Training: (loss 0.1509):  11%|█         | 83/777 [00:32<03:55,  2.94it/s]\n",
      "Training: (loss 0.1509):  11%|█         | 84/777 [00:32<03:49,  3.02it/s]\n",
      "Training: (loss 0.3010):  11%|█         | 84/777 [00:32<03:49,  3.02it/s]\n",
      "Training: (loss 0.3010):  11%|█         | 85/777 [00:32<03:52,  2.98it/s]\n",
      "Training: (loss -0.0687):  11%|█         | 85/777 [00:33<03:52,  2.98it/s]\n",
      "Training: (loss -0.0687):  11%|█         | 86/777 [00:33<03:38,  3.16it/s]\n",
      "Training: (loss 0.3010):  11%|█         | 86/777 [00:33<03:38,  3.16it/s] \n",
      "Training: (loss 0.3010):  11%|█         | 87/777 [00:33<03:49,  3.00it/s]\n",
      "Training: (loss 0.0478):  11%|█         | 87/777 [00:33<03:49,  3.00it/s]\n",
      "Training: (loss 0.0478):  11%|█▏        | 88/777 [00:33<03:59,  2.87it/s]\n",
      "Training: (loss 0.1115):  11%|█▏        | 88/777 [00:34<03:59,  2.87it/s]\n",
      "Training: (loss 0.1115):  11%|█▏        | 89/777 [00:34<03:58,  2.88it/s]\n",
      "Training: (loss 0.1870):  11%|█▏        | 89/777 [00:34<03:58,  2.88it/s]\n",
      "Training: (loss 0.1870):  12%|█▏        | 90/777 [00:34<03:58,  2.88it/s]\n",
      "Training: (loss -0.0735):  12%|█▏        | 90/777 [00:34<03:58,  2.88it/s]\n",
      "Training: (loss -0.0735):  12%|█▏        | 91/777 [00:34<04:03,  2.82it/s]\n",
      "Training: (loss 0.3010):  12%|█▏        | 91/777 [00:35<04:03,  2.82it/s] \n",
      "Training: (loss 0.3010):  12%|█▏        | 92/777 [00:35<04:03,  2.82it/s]\n",
      "Training: (loss 0.3010):  12%|█▏        | 92/777 [00:35<04:03,  2.82it/s]\n",
      "Training: (loss 0.3010):  12%|█▏        | 93/777 [00:35<04:17,  2.66it/s]\n",
      "Training: (loss -0.0815):  12%|█▏        | 93/777 [00:36<04:17,  2.66it/s]\n",
      "Training: (loss -0.0815):  12%|█▏        | 94/777 [00:36<04:28,  2.54it/s]\n",
      "Training: (loss -0.0623):  12%|█▏        | 94/777 [00:36<04:28,  2.54it/s]\n",
      "Training: (loss -0.0623):  12%|█▏        | 95/777 [00:36<04:27,  2.55it/s]\n",
      "Training: (loss -0.0193):  12%|█▏        | 95/777 [00:36<04:27,  2.55it/s]\n",
      "Training: (loss -0.0193):  12%|█▏        | 96/777 [00:36<04:25,  2.57it/s]\n",
      "Training: (loss 0.0320):  12%|█▏        | 96/777 [00:37<04:25,  2.57it/s] \n",
      "Training: (loss 0.0320):  12%|█▏        | 97/777 [00:37<04:06,  2.75it/s]\n",
      "Training: (loss 0.1992):  12%|█▏        | 97/777 [00:37<04:06,  2.75it/s]\n",
      "Training: (loss 0.1992):  13%|█▎        | 98/777 [00:37<04:10,  2.71it/s]\n",
      "Training: (loss 0.0081):  13%|█▎        | 98/777 [00:37<04:10,  2.71it/s]\n",
      "Training: (loss 0.0081):  13%|█▎        | 99/777 [00:37<04:14,  2.67it/s]\n",
      "Training: (loss 0.3070):  13%|█▎        | 99/777 [00:38<04:14,  2.67it/s]\n",
      "Training: (loss 0.3070):  13%|█▎        | 100/777 [00:38<04:27,  2.53it/s]\n",
      "Training: (loss 0.3322):  13%|█▎        | 100/777 [00:38<04:27,  2.53it/s]\n",
      "Training: (loss 0.3322):  13%|█▎        | 101/777 [00:38<04:33,  2.47it/s]\n",
      "Training: (loss 0.2466):  13%|█▎        | 101/777 [00:39<04:33,  2.47it/s]\n",
      "Training: (loss 0.2466):  13%|█▎        | 102/777 [00:39<04:24,  2.55it/s]\n",
      "Training: (loss -0.0320):  13%|█▎        | 102/777 [00:39<04:24,  2.55it/s]\n",
      "Training: (loss -0.0320):  13%|█▎        | 103/777 [00:39<04:25,  2.54it/s]\n",
      "Training: (loss 0.0082):  13%|█▎        | 103/777 [00:39<04:25,  2.54it/s] \n",
      "Training: (loss 0.0082):  13%|█▎        | 104/777 [00:39<04:30,  2.49it/s]\n",
      "Training: (loss 0.1705):  13%|█▎        | 104/777 [00:40<04:30,  2.49it/s]\n",
      "Training: (loss 0.1705):  14%|█▎        | 105/777 [00:40<04:12,  2.66it/s]\n",
      "Training: (loss -0.0853):  14%|█▎        | 105/777 [00:40<04:12,  2.66it/s]\n",
      "Training: (loss -0.0853):  14%|█▎        | 106/777 [00:40<04:13,  2.65it/s]\n",
      "Training: (loss 0.0870):  14%|█▎        | 106/777 [00:40<04:13,  2.65it/s] \n",
      "Training: (loss 0.0870):  14%|█▍        | 107/777 [00:40<03:59,  2.80it/s]\n",
      "Training: (loss -0.2443):  14%|█▍        | 107/777 [00:41<03:59,  2.80it/s]\n",
      "Training: (loss -0.2443):  14%|█▍        | 108/777 [00:41<03:47,  2.94it/s]\n",
      "Training: (loss 0.0129):  14%|█▍        | 108/777 [00:41<03:47,  2.94it/s] \n",
      "Training: (loss 0.0129):  14%|█▍        | 109/777 [00:41<03:39,  3.04it/s]\n",
      "Training: (loss 0.0491):  14%|█▍        | 109/777 [00:41<03:39,  3.04it/s]\n",
      "Training: (loss 0.0491):  14%|█▍        | 110/777 [00:41<03:44,  2.98it/s]\n",
      "Training: (loss 0.1396):  14%|█▍        | 110/777 [00:42<03:44,  2.98it/s]\n",
      "Training: (loss 0.1396):  14%|█▍        | 111/777 [00:42<03:42,  2.99it/s]\n",
      "Training: (loss -0.0469):  14%|█▍        | 111/777 [00:42<03:42,  2.99it/s]\n",
      "Training: (loss -0.0469):  14%|█▍        | 112/777 [00:42<03:45,  2.96it/s]\n",
      "Training: (loss 0.0461):  14%|█▍        | 112/777 [00:42<03:45,  2.96it/s] \n",
      "Training: (loss 0.0461):  15%|█▍        | 113/777 [00:42<03:36,  3.06it/s]\n",
      "Training: (loss -0.0446):  15%|█▍        | 113/777 [00:43<03:36,  3.06it/s]\n",
      "Training: (loss -0.0446):  15%|█▍        | 114/777 [00:43<03:37,  3.05it/s]\n",
      "Training: (loss 0.0892):  15%|█▍        | 114/777 [00:43<03:37,  3.05it/s] \n",
      "Training: (loss 0.0892):  15%|█▍        | 115/777 [00:43<03:31,  3.13it/s]\n",
      "Training: (loss 0.2301):  15%|█▍        | 115/777 [00:43<03:31,  3.13it/s]\n",
      "Training: (loss 0.2301):  15%|█▍        | 116/777 [00:43<03:24,  3.23it/s]\n",
      "Training: (loss 0.0844):  15%|█▍        | 116/777 [00:44<03:24,  3.23it/s]\n",
      "Training: (loss 0.0844):  15%|█▌        | 117/777 [00:44<03:23,  3.25it/s]\n",
      "Training: (loss 0.3927):  15%|█▌        | 117/777 [00:44<03:23,  3.25it/s]\n",
      "Training: (loss 0.3927):  15%|█▌        | 118/777 [00:44<03:21,  3.26it/s]\n",
      "Training: (loss 0.3920):  15%|█▌        | 118/777 [00:44<03:21,  3.26it/s]\n",
      "Training: (loss 0.3920):  15%|█▌        | 119/777 [00:44<03:19,  3.29it/s]\n",
      "Training: (loss -0.0755):  15%|█▌        | 119/777 [00:45<03:19,  3.29it/s]\n",
      "Training: (loss -0.0755):  15%|█▌        | 120/777 [00:45<03:26,  3.18it/s]\n",
      "Training: (loss 0.3939):  15%|█▌        | 120/777 [00:45<03:26,  3.18it/s] \n",
      "Training: (loss 0.3939):  16%|█▌        | 121/777 [00:45<03:40,  2.98it/s]\n",
      "Training: (loss -0.1332):  16%|█▌        | 121/777 [00:45<03:40,  2.98it/s]\n",
      "Training: (loss -0.1332):  16%|█▌        | 122/777 [00:45<03:38,  2.99it/s]\n",
      "Training: (loss 0.3936):  16%|█▌        | 122/777 [00:46<03:38,  2.99it/s] \n",
      "Training: (loss 0.3936):  16%|█▌        | 123/777 [00:46<03:36,  3.02it/s]\n",
      "Training: (loss 0.3046):  16%|█▌        | 123/777 [00:46<03:36,  3.02it/s]\n",
      "Training: (loss 0.3046):  16%|█▌        | 124/777 [00:46<03:28,  3.13it/s]\n",
      "Training: (loss -0.1043):  16%|█▌        | 124/777 [00:46<03:28,  3.13it/s]\n",
      "Training: (loss -0.1043):  16%|█▌        | 125/777 [00:46<03:29,  3.12it/s]\n",
      "Training: (loss 0.1479):  16%|█▌        | 125/777 [00:47<03:29,  3.12it/s] \n",
      "Training: (loss 0.1479):  16%|█▌        | 126/777 [00:47<03:31,  3.08it/s]\n",
      "Training: (loss 0.3886):  16%|█▌        | 126/777 [00:47<03:31,  3.08it/s]\n",
      "Training: (loss 0.3886):  16%|█▋        | 127/777 [00:47<03:31,  3.08it/s]\n",
      "Training: (loss 0.3932):  16%|█▋        | 127/777 [00:47<03:31,  3.08it/s]\n",
      "Training: (loss 0.3932):  16%|█▋        | 128/777 [00:47<03:39,  2.96it/s]\n",
      "Training: (loss -0.0773):  16%|█▋        | 128/777 [00:48<03:39,  2.96it/s]\n",
      "Training: (loss -0.0773):  17%|█▋        | 129/777 [00:48<03:53,  2.77it/s]\n",
      "Training: (loss 0.2213):  17%|█▋        | 129/777 [00:48<03:53,  2.77it/s] \n",
      "Training: (loss 0.2213):  17%|█▋        | 130/777 [00:48<03:53,  2.77it/s]\n",
      "Training: (loss 0.3934):  17%|█▋        | 130/777 [00:48<03:53,  2.77it/s]\n",
      "Training: (loss 0.3934):  17%|█▋        | 131/777 [00:48<03:45,  2.87it/s]\n",
      "Training: (loss 0.0084):  17%|█▋        | 131/777 [00:49<03:45,  2.87it/s]\n",
      "Training: (loss 0.0084):  17%|█▋        | 132/777 [00:49<03:46,  2.85it/s]\n",
      "Training: (loss 0.0081):  17%|█▋        | 132/777 [00:49<03:46,  2.85it/s]\n",
      "Training: (loss 0.0081):  17%|█▋        | 133/777 [00:49<03:50,  2.79it/s]\n",
      "Training: (loss -0.0707):  17%|█▋        | 133/777 [00:49<03:50,  2.79it/s]\n",
      "Training: (loss -0.0707):  17%|█▋        | 134/777 [00:49<04:00,  2.68it/s]\n",
      "Training: (loss 0.3940):  17%|█▋        | 134/777 [00:50<04:00,  2.68it/s] \n",
      "Training: (loss 0.3940):  17%|█▋        | 135/777 [00:50<03:48,  2.82it/s]\n",
      "Training: (loss 0.0495):  17%|█▋        | 135/777 [00:50<03:48,  2.82it/s]\n",
      "Training: (loss 0.0495):  18%|█▊        | 136/777 [00:50<03:54,  2.73it/s]\n",
      "Training: (loss 0.3937):  18%|█▊        | 136/777 [00:50<03:54,  2.73it/s]\n",
      "Training: (loss 0.3937):  18%|█▊        | 137/777 [00:50<03:43,  2.87it/s]\n",
      "Training: (loss 0.3939):  18%|█▊        | 137/777 [00:51<03:43,  2.87it/s]\n",
      "Training: (loss 0.3939):  18%|█▊        | 138/777 [00:51<03:43,  2.86it/s]\n",
      "Training: (loss -0.0112):  18%|█▊        | 138/777 [00:51<03:43,  2.86it/s]\n",
      "Training: (loss -0.0112):  18%|█▊        | 139/777 [00:51<03:54,  2.72it/s]\n",
      "Training: (loss 0.1918):  18%|█▊        | 139/777 [00:52<03:54,  2.72it/s] \n",
      "Training: (loss 0.1918):  18%|█▊        | 140/777 [00:52<04:00,  2.65it/s]\n",
      "Training: (loss -0.1251):  18%|█▊        | 140/777 [00:52<04:00,  2.65it/s]\n",
      "Training: (loss -0.1251):  18%|█▊        | 141/777 [00:52<03:59,  2.65it/s]\n",
      "Training: (loss 0.2116):  18%|█▊        | 141/777 [00:52<03:59,  2.65it/s] \n",
      "Training: (loss 0.2116):  18%|█▊        | 142/777 [00:52<03:47,  2.79it/s]\n",
      "Training: (loss -0.0998):  18%|█▊        | 142/777 [00:53<03:47,  2.79it/s]\n",
      "Training: (loss -0.0998):  18%|█▊        | 143/777 [00:53<03:44,  2.82it/s]\n",
      "Training: (loss -0.0497):  18%|█▊        | 143/777 [00:53<03:44,  2.82it/s]\n",
      "Training: (loss -0.0497):  19%|█▊        | 144/777 [00:53<03:52,  2.72it/s]\n",
      "Training: (loss 0.0235):  19%|█▊        | 144/777 [00:54<03:52,  2.72it/s] \n",
      "Training: (loss 0.0235):  19%|█▊        | 145/777 [00:54<04:04,  2.58it/s]\n",
      "Training: (loss -0.1803):  19%|█▊        | 145/777 [00:54<04:04,  2.58it/s]\n",
      "Training: (loss -0.1803):  19%|█▉        | 146/777 [00:54<04:21,  2.41it/s]\n",
      "Training: (loss 0.0511):  19%|█▉        | 146/777 [00:54<04:21,  2.41it/s] \n",
      "Training: (loss 0.0511):  19%|█▉        | 147/777 [00:54<04:11,  2.51it/s]\n",
      "Training: (loss -0.0610):  19%|█▉        | 147/777 [00:55<04:11,  2.51it/s]\n",
      "Training: (loss -0.0610):  19%|█▉        | 148/777 [00:55<03:55,  2.67it/s]\n",
      "Training: (loss 0.0317):  19%|█▉        | 148/777 [00:55<03:55,  2.67it/s] \n",
      "Training: (loss 0.0317):  19%|█▉        | 149/777 [00:55<03:59,  2.62it/s]\n",
      "Training: (loss -0.1008):  19%|█▉        | 149/777 [00:55<03:59,  2.62it/s]\n",
      "Training: (loss -0.1008):  19%|█▉        | 150/777 [00:55<03:48,  2.74it/s]\n",
      "Training: (loss 0.1492):  19%|█▉        | 150/777 [00:56<03:48,  2.74it/s] \n",
      "Training: (loss 0.1492):  19%|█▉        | 151/777 [00:56<03:39,  2.85it/s]\n",
      "Training: (loss 0.2364):  19%|█▉        | 151/777 [00:56<03:39,  2.85it/s]\n",
      "Training: (loss 0.2364):  20%|█▉        | 152/777 [00:56<03:45,  2.77it/s]\n",
      "Training: (loss 0.3922):  20%|█▉        | 152/777 [00:56<03:45,  2.77it/s]\n",
      "Training: (loss 0.3922):  20%|█▉        | 153/777 [00:56<03:44,  2.79it/s]\n",
      "Training: (loss 0.1746):  20%|█▉        | 153/777 [00:57<03:44,  2.79it/s]\n",
      "Training: (loss 0.1746):  20%|█▉        | 154/777 [00:57<03:52,  2.68it/s]\n",
      "Training: (loss 0.3928):  20%|█▉        | 154/777 [00:57<03:52,  2.68it/s]\n",
      "Training: (loss 0.3928):  20%|█▉        | 155/777 [00:57<03:46,  2.74it/s]\n",
      "Training: (loss 0.3927):  20%|█▉        | 155/777 [00:58<03:46,  2.74it/s]\n",
      "Training: (loss 0.3927):  20%|██        | 156/777 [00:58<03:50,  2.69it/s]\n",
      "Training: (loss -0.0522):  20%|██        | 156/777 [00:58<03:50,  2.69it/s]\n",
      "Training: (loss -0.0522):  20%|██        | 157/777 [00:58<03:55,  2.64it/s]\n",
      "Training: (loss -0.0336):  20%|██        | 157/777 [00:58<03:55,  2.64it/s]\n",
      "Training: (loss -0.0336):  20%|██        | 158/777 [00:58<04:02,  2.55it/s]\n",
      "Training: (loss 0.0897):  20%|██        | 158/777 [00:59<04:02,  2.55it/s] \n",
      "Training: (loss 0.0897):  20%|██        | 159/777 [00:59<04:09,  2.47it/s]\n",
      "Training: (loss 0.1653):  20%|██        | 159/777 [00:59<04:09,  2.47it/s]\n",
      "Training: (loss 0.1653):  21%|██        | 160/777 [00:59<04:00,  2.56it/s]\n",
      "Training: (loss 0.1121):  21%|██        | 160/777 [01:00<04:00,  2.56it/s]\n",
      "Training: (loss 0.1121):  21%|██        | 161/777 [01:00<04:15,  2.41it/s]\n",
      "Training: (loss 0.3919):  21%|██        | 161/777 [01:00<04:15,  2.41it/s]\n",
      "Training: (loss 0.3919):  21%|██        | 162/777 [01:00<04:09,  2.46it/s]\n",
      "Training: (loss 0.0556):  21%|██        | 162/777 [01:00<04:09,  2.46it/s]\n",
      "Training: (loss 0.0556):  21%|██        | 163/777 [01:00<03:53,  2.63it/s]\n",
      "Training: (loss 0.3262):  21%|██        | 163/777 [01:01<03:53,  2.63it/s]\n",
      "Training: (loss 0.3262):  21%|██        | 164/777 [01:01<03:55,  2.60it/s]\n",
      "Training: (loss -0.0957):  21%|██        | 164/777 [01:01<03:55,  2.60it/s]\n",
      "Training: (loss -0.0957):  21%|██        | 165/777 [01:01<03:46,  2.71it/s]\n",
      "Training: (loss -0.0754):  21%|██        | 165/777 [01:02<03:46,  2.71it/s]\n",
      "Training: (loss -0.0754):  21%|██▏       | 166/777 [01:02<03:55,  2.60it/s]\n",
      "Training: (loss 0.1948):  21%|██▏       | 166/777 [01:02<03:55,  2.60it/s] \n",
      "Training: (loss 0.1948):  21%|██▏       | 167/777 [01:02<04:06,  2.47it/s]\n",
      "Training: (loss 0.2116):  21%|██▏       | 167/777 [01:02<04:06,  2.47it/s]\n",
      "Training: (loss 0.2116):  22%|██▏       | 168/777 [01:02<04:02,  2.52it/s]\n",
      "Training: (loss -0.1080):  22%|██▏       | 168/777 [01:03<04:02,  2.52it/s]\n",
      "Training: (loss -0.1080):  22%|██▏       | 169/777 [01:03<03:59,  2.54it/s]\n",
      "Training: (loss -0.2813):  22%|██▏       | 169/777 [01:03<03:59,  2.54it/s]\n",
      "Training: (loss -0.2813):  22%|██▏       | 170/777 [01:03<04:09,  2.43it/s]\n",
      "Training: (loss 0.1067):  22%|██▏       | 170/777 [01:04<04:09,  2.43it/s] \n",
      "Training: (loss 0.1067):  22%|██▏       | 171/777 [01:04<04:03,  2.49it/s]\n",
      "Training: (loss 0.2382):  22%|██▏       | 171/777 [01:04<04:03,  2.49it/s]\n",
      "Training: (loss 0.2382):  22%|██▏       | 172/777 [01:04<03:48,  2.64it/s]\n",
      "Training: (loss 0.3903):  22%|██▏       | 172/777 [01:04<03:48,  2.64it/s]\n",
      "Training: (loss 0.3903):  22%|██▏       | 173/777 [01:04<04:09,  2.42it/s]\n",
      "Training: (loss 0.3838):  22%|██▏       | 173/777 [01:05<04:09,  2.42it/s]\n",
      "Training: (loss 0.3838):  22%|██▏       | 174/777 [01:05<04:09,  2.42it/s]\n",
      "Training: (loss 0.0189):  22%|██▏       | 174/777 [01:05<04:09,  2.42it/s]\n",
      "Training: (loss 0.0189):  23%|██▎       | 175/777 [01:05<04:05,  2.45it/s]\n",
      "Training: (loss 0.3710):  23%|██▎       | 175/777 [01:06<04:05,  2.45it/s]\n",
      "Training: (loss 0.3710):  23%|██▎       | 176/777 [01:06<03:47,  2.64it/s]\n",
      "Training: (loss 0.3693):  23%|██▎       | 176/777 [01:06<03:47,  2.64it/s]\n",
      "Training: (loss 0.3693):  23%|██▎       | 177/777 [01:06<03:45,  2.66it/s]\n",
      "Training: (loss -0.0729):  23%|██▎       | 177/777 [01:06<03:45,  2.66it/s]\n",
      "Training: (loss -0.0729):  23%|██▎       | 178/777 [01:06<03:45,  2.66it/s]\n",
      "Training: (loss 0.3724):  23%|██▎       | 178/777 [01:07<03:45,  2.66it/s] \n",
      "Training: (loss 0.3724):  23%|██▎       | 179/777 [01:07<03:32,  2.81it/s]\n",
      "Training: (loss 0.3753):  23%|██▎       | 179/777 [01:07<03:32,  2.81it/s]\n",
      "Training: (loss 0.3753):  23%|██▎       | 180/777 [01:07<03:41,  2.70it/s]\n",
      "Training: (loss -0.0834):  23%|██▎       | 180/777 [01:07<03:41,  2.70it/s]\n",
      "Training: (loss -0.0834):  23%|██▎       | 181/777 [01:07<03:55,  2.53it/s]\n",
      "Training: (loss -0.1042):  23%|██▎       | 181/777 [01:08<03:55,  2.53it/s]\n",
      "Training: (loss -0.1042):  23%|██▎       | 182/777 [01:08<03:58,  2.49it/s]\n",
      "Training: (loss 0.3721):  23%|██▎       | 182/777 [01:08<03:58,  2.49it/s] \n",
      "Training: (loss 0.3721):  24%|██▎       | 183/777 [01:08<03:53,  2.55it/s]\n",
      "Training: (loss 0.0800):  24%|██▎       | 183/777 [01:09<03:53,  2.55it/s]\n",
      "Training: (loss 0.0800):  24%|██▎       | 184/777 [01:09<03:54,  2.53it/s]\n",
      "Training: (loss -0.0946):  24%|██▎       | 184/777 [01:09<03:54,  2.53it/s]\n",
      "Training: (loss -0.0946):  24%|██▍       | 185/777 [01:09<03:53,  2.53it/s]\n",
      "Training: (loss 0.2761):  24%|██▍       | 185/777 [01:09<03:53,  2.53it/s] \n",
      "Training: (loss 0.2761):  24%|██▍       | 186/777 [01:09<03:44,  2.64it/s]\n",
      "Training: (loss -0.0273):  24%|██▍       | 186/777 [01:10<03:44,  2.64it/s]\n",
      "Training: (loss -0.0273):  24%|██▍       | 187/777 [01:10<03:32,  2.78it/s]\n",
      "Training: (loss -0.1953):  24%|██▍       | 187/777 [01:10<03:32,  2.78it/s]\n",
      "Training: (loss -0.1953):  24%|██▍       | 188/777 [01:10<03:33,  2.76it/s]\n",
      "Training: (loss -0.0249):  24%|██▍       | 188/777 [01:10<03:33,  2.76it/s]\n",
      "Training: (loss -0.0249):  24%|██▍       | 189/777 [01:10<03:33,  2.75it/s]\n",
      "Training: (loss -0.1384):  24%|██▍       | 189/777 [01:11<03:33,  2.75it/s]\n",
      "Training: (loss -0.1384):  24%|██▍       | 190/777 [01:11<03:35,  2.72it/s]\n",
      "Training: (loss 0.3338):  24%|██▍       | 190/777 [01:11<03:35,  2.72it/s] \n",
      "Training: (loss 0.3338):  25%|██▍       | 191/777 [01:11<03:18,  2.95it/s]\n",
      "Training: (loss -0.1268):  25%|██▍       | 191/777 [01:11<03:18,  2.95it/s]\n",
      "Training: (loss -0.1268):  25%|██▍       | 192/777 [01:11<03:19,  2.93it/s]\n",
      "Training: (loss 0.3169):  25%|██▍       | 192/777 [01:12<03:19,  2.93it/s] \n",
      "Training: (loss 0.3169):  25%|██▍       | 193/777 [01:12<03:24,  2.86it/s]\n",
      "Training: (loss -0.0039):  25%|██▍       | 193/777 [01:12<03:24,  2.86it/s]\n",
      "Training: (loss -0.0039):  25%|██▍       | 194/777 [01:12<03:18,  2.93it/s]\n",
      "Training: (loss -0.0378):  25%|██▍       | 194/777 [01:12<03:18,  2.93it/s]\n",
      "Training: (loss -0.0378):  25%|██▌       | 195/777 [01:12<03:27,  2.80it/s]\n",
      "Training: (loss 0.1382):  25%|██▌       | 195/777 [01:13<03:27,  2.80it/s] \n",
      "Training: (loss 0.1382):  25%|██▌       | 196/777 [01:13<03:18,  2.93it/s]\n",
      "Training: (loss 0.3109):  25%|██▌       | 196/777 [01:13<03:18,  2.93it/s]\n",
      "Training: (loss 0.3109):  25%|██▌       | 197/777 [01:13<03:24,  2.84it/s]\n",
      "Training: (loss 0.0273):  25%|██▌       | 197/777 [01:13<03:24,  2.84it/s]\n",
      "Training: (loss 0.0273):  25%|██▌       | 198/777 [01:13<03:15,  2.97it/s]\n",
      "Training: (loss -0.0081):  25%|██▌       | 198/777 [01:14<03:15,  2.97it/s]\n",
      "Training: (loss -0.0081):  26%|██▌       | 199/777 [01:14<03:16,  2.95it/s]\n",
      "Training: (loss -0.0064):  26%|██▌       | 199/777 [01:14<03:16,  2.95it/s]\n",
      "Training: (loss -0.0064):  26%|██▌       | 200/777 [01:14<03:14,  2.97it/s]\n",
      "Training: (loss 0.2224):  26%|██▌       | 200/777 [01:15<03:14,  2.97it/s] \n",
      "Training: (loss 0.2224):  26%|██▌       | 201/777 [01:15<03:18,  2.90it/s]\n",
      "Training: (loss 0.1590):  26%|██▌       | 201/777 [01:15<03:18,  2.90it/s]\n",
      "Training: (loss 0.1590):  26%|██▌       | 202/777 [01:15<03:09,  3.03it/s]\n",
      "Training: (loss 0.1026):  26%|██▌       | 202/777 [01:15<03:09,  3.03it/s]\n",
      "Training: (loss 0.1026):  26%|██▌       | 203/777 [01:15<03:05,  3.09it/s]\n",
      "Training: (loss 0.3051):  26%|██▌       | 203/777 [01:16<03:05,  3.09it/s]\n",
      "Training: (loss 0.3051):  26%|██▋       | 204/777 [01:16<03:20,  2.85it/s]\n",
      "Training: (loss 0.2600):  26%|██▋       | 204/777 [01:16<03:20,  2.85it/s]\n",
      "Training: (loss 0.2600):  26%|██▋       | 205/777 [01:16<03:25,  2.79it/s]\n",
      "Training: (loss 0.3179):  26%|██▋       | 205/777 [01:16<03:25,  2.79it/s]\n",
      "Training: (loss 0.3179):  27%|██▋       | 206/777 [01:16<03:27,  2.76it/s]\n",
      "Training: (loss 0.0239):  27%|██▋       | 206/777 [01:17<03:27,  2.76it/s]\n",
      "Training: (loss 0.0239):  27%|██▋       | 207/777 [01:17<03:22,  2.82it/s]\n",
      "Training: (loss 0.1655):  27%|██▋       | 207/777 [01:17<03:22,  2.82it/s]\n",
      "Training: (loss 0.1655):  27%|██▋       | 208/777 [01:17<03:24,  2.78it/s]\n",
      "Training: (loss 0.1414):  27%|██▋       | 208/777 [01:17<03:24,  2.78it/s]\n",
      "Training: (loss 0.1414):  27%|██▋       | 209/777 [01:17<03:30,  2.70it/s]\n",
      "Training: (loss 0.2672):  27%|██▋       | 209/777 [01:18<03:30,  2.70it/s]\n",
      "Training: (loss 0.2672):  27%|██▋       | 210/777 [01:18<03:35,  2.63it/s]\n",
      "Training: (loss 0.3119):  27%|██▋       | 210/777 [01:18<03:35,  2.63it/s]\n",
      "Training: (loss 0.3119):  27%|██▋       | 211/777 [01:18<03:19,  2.84it/s]\n",
      "Training: (loss -0.1356):  27%|██▋       | 211/777 [01:18<03:19,  2.84it/s]\n",
      "Training: (loss -0.1356):  27%|██▋       | 212/777 [01:18<03:11,  2.95it/s]\n",
      "Training: (loss -0.0087):  27%|██▋       | 212/777 [01:19<03:11,  2.95it/s]\n",
      "Training: (loss -0.0087):  27%|██▋       | 213/777 [01:19<03:07,  3.00it/s]\n",
      "Training: (loss -0.0337):  27%|██▋       | 213/777 [01:19<03:07,  3.00it/s]\n",
      "Training: (loss -0.0337):  28%|██▊       | 214/777 [01:19<03:11,  2.94it/s]\n",
      "Training: (loss 0.3028):  28%|██▊       | 214/777 [01:19<03:11,  2.94it/s] \n",
      "Training: (loss 0.3028):  28%|██▊       | 215/777 [01:19<03:11,  2.94it/s]\n",
      "Training: (loss -0.0692):  28%|██▊       | 215/777 [01:20<03:11,  2.94it/s]\n",
      "Training: (loss -0.0692):  28%|██▊       | 216/777 [01:20<03:09,  2.97it/s]\n",
      "Training: (loss 0.3099):  28%|██▊       | 216/777 [01:20<03:09,  2.97it/s] \n",
      "Training: (loss 0.3099):  28%|██▊       | 217/777 [01:20<03:06,  3.00it/s]\n",
      "Training: (loss -0.0363):  28%|██▊       | 217/777 [01:20<03:06,  3.00it/s]\n",
      "Training: (loss -0.0363):  28%|██▊       | 218/777 [01:20<03:13,  2.88it/s]\n",
      "Training: (loss 0.2214):  28%|██▊       | 218/777 [01:21<03:13,  2.88it/s] \n",
      "Training: (loss 0.2214):  28%|██▊       | 219/777 [01:21<03:16,  2.85it/s]\n",
      "Training: (loss -0.1036):  28%|██▊       | 219/777 [01:21<03:16,  2.85it/s]\n",
      "Training: (loss -0.1036):  28%|██▊       | 220/777 [01:21<03:29,  2.66it/s]\n",
      "Training: (loss 0.1210):  28%|██▊       | 220/777 [01:22<03:29,  2.66it/s] \n",
      "Training: (loss 0.1210):  28%|██▊       | 221/777 [01:22<03:23,  2.74it/s]\n",
      "Training: (loss 0.1617):  28%|██▊       | 221/777 [01:22<03:23,  2.74it/s]\n",
      "Training: (loss 0.1617):  29%|██▊       | 222/777 [01:22<03:24,  2.72it/s]\n",
      "Training: (loss 0.2318):  29%|██▊       | 222/777 [01:22<03:24,  2.72it/s]\n",
      "Training: (loss 0.2318):  29%|██▊       | 223/777 [01:22<03:15,  2.84it/s]\n",
      "Training: (loss 0.3154):  29%|██▊       | 223/777 [01:23<03:15,  2.84it/s]\n",
      "Training: (loss 0.3154):  29%|██▉       | 224/777 [01:23<03:03,  3.02it/s]\n",
      "Training: (loss -0.0691):  29%|██▉       | 224/777 [01:23<03:03,  3.02it/s]\n",
      "Training: (loss -0.0691):  29%|██▉       | 225/777 [01:23<03:07,  2.95it/s]\n",
      "Training: (loss 0.0536):  29%|██▉       | 225/777 [01:23<03:07,  2.95it/s] \n",
      "Training: (loss 0.0536):  29%|██▉       | 226/777 [01:23<03:15,  2.81it/s]\n",
      "Training: (loss -0.0910):  29%|██▉       | 226/777 [01:24<03:15,  2.81it/s]\n",
      "Training: (loss -0.0910):  29%|██▉       | 227/777 [01:24<03:11,  2.87it/s]\n",
      "Training: (loss -0.2022):  29%|██▉       | 227/777 [01:24<03:11,  2.87it/s]\n",
      "Training: (loss -0.2022):  29%|██▉       | 228/777 [01:24<03:23,  2.70it/s]\n",
      "Training: (loss -0.1699):  29%|██▉       | 228/777 [01:24<03:23,  2.70it/s]\n",
      "Training: (loss -0.1699):  29%|██▉       | 229/777 [01:24<03:19,  2.74it/s]\n",
      "Training: (loss 0.3332):  29%|██▉       | 229/777 [01:25<03:19,  2.74it/s] \n",
      "Training: (loss 0.3332):  30%|██▉       | 230/777 [01:25<03:23,  2.68it/s]\n",
      "Training: (loss 0.0635):  30%|██▉       | 230/777 [01:25<03:23,  2.68it/s]\n",
      "Training: (loss 0.0635):  30%|██▉       | 231/777 [01:25<03:14,  2.81it/s]\n",
      "Training: (loss 0.3341):  30%|██▉       | 231/777 [01:26<03:14,  2.81it/s]\n",
      "Training: (loss 0.3341):  30%|██▉       | 232/777 [01:26<03:23,  2.68it/s]\n",
      "Training: (loss -0.2086):  30%|██▉       | 232/777 [01:26<03:23,  2.68it/s]\n",
      "Training: (loss -0.2086):  30%|██▉       | 233/777 [01:26<03:22,  2.68it/s]\n",
      "Training: (loss -0.0766):  30%|██▉       | 233/777 [01:26<03:22,  2.68it/s]\n",
      "Training: (loss -0.0766):  30%|███       | 234/777 [01:26<03:35,  2.52it/s]\n",
      "Training: (loss 0.0657):  30%|███       | 234/777 [01:27<03:35,  2.52it/s] \n",
      "Training: (loss 0.0657):  30%|███       | 235/777 [01:27<03:39,  2.47it/s]\n",
      "Training: (loss 0.3330):  30%|███       | 235/777 [01:27<03:39,  2.47it/s]\n",
      "Training: (loss 0.3330):  30%|███       | 236/777 [01:27<03:26,  2.62it/s]\n",
      "Training: (loss -0.1283):  30%|███       | 236/777 [01:27<03:26,  2.62it/s]\n",
      "Training: (loss -0.1283):  31%|███       | 237/777 [01:27<03:14,  2.78it/s]\n",
      "Training: (loss -0.1216):  31%|███       | 237/777 [01:28<03:14,  2.78it/s]\n",
      "Training: (loss -0.1216):  31%|███       | 238/777 [01:28<03:02,  2.96it/s]\n",
      "Training: (loss 0.3367):  31%|███       | 238/777 [01:28<03:02,  2.96it/s] \n",
      "Training: (loss 0.3367):  31%|███       | 239/777 [01:28<03:18,  2.71it/s]\n",
      "Training: (loss -0.2291):  31%|███       | 239/777 [01:29<03:18,  2.71it/s]\n",
      "Training: (loss -0.2291):  31%|███       | 240/777 [01:29<03:31,  2.54it/s]\n",
      "Training: (loss 0.3432):  31%|███       | 240/777 [01:29<03:31,  2.54it/s] \n",
      "Training: (loss 0.3432):  31%|███       | 241/777 [01:29<03:15,  2.74it/s]\n",
      "Training: (loss 0.3382):  31%|███       | 241/777 [01:29<03:15,  2.74it/s]\n",
      "Training: (loss 0.3382):  31%|███       | 242/777 [01:29<03:06,  2.87it/s]\n",
      "Training: (loss 0.3451):  31%|███       | 242/777 [01:30<03:06,  2.87it/s]\n",
      "Training: (loss 0.3451):  31%|███▏      | 243/777 [01:30<03:08,  2.83it/s]\n",
      "Training: (loss -0.0260):  31%|███▏      | 243/777 [01:30<03:08,  2.83it/s]\n",
      "Training: (loss -0.0260):  31%|███▏      | 244/777 [01:30<03:09,  2.82it/s]\n",
      "Training: (loss 0.0636):  31%|███▏      | 244/777 [01:30<03:09,  2.82it/s] \n",
      "Training: (loss 0.0636):  32%|███▏      | 245/777 [01:30<03:09,  2.81it/s]\n",
      "Training: (loss -0.0744):  32%|███▏      | 245/777 [01:31<03:09,  2.81it/s]\n",
      "Training: (loss -0.0744):  32%|███▏      | 246/777 [01:31<03:05,  2.87it/s]\n",
      "Training: (loss -0.1687):  32%|███▏      | 246/777 [01:31<03:05,  2.87it/s]\n",
      "Training: (loss -0.1687):  32%|███▏      | 247/777 [01:31<03:16,  2.70it/s]\n",
      "Training: (loss 0.1380):  32%|███▏      | 247/777 [01:31<03:16,  2.70it/s] \n",
      "Training: (loss 0.1380):  32%|███▏      | 248/777 [01:31<03:13,  2.74it/s]\n",
      "Training: (loss 0.2293):  32%|███▏      | 248/777 [01:32<03:13,  2.74it/s]\n",
      "Training: (loss 0.2293):  32%|███▏      | 249/777 [01:32<03:12,  2.75it/s]\n",
      "Training: (loss -0.1637):  32%|███▏      | 249/777 [01:32<03:12,  2.75it/s]\n",
      "Training: (loss -0.1637):  32%|███▏      | 250/777 [01:32<03:18,  2.65it/s]\n",
      "Training: (loss -0.0028):  32%|███▏      | 250/777 [01:33<03:18,  2.65it/s]\n",
      "Training: (loss -0.0028):  32%|███▏      | 251/777 [01:33<03:15,  2.69it/s]\n",
      "Training: (loss 0.1013):  32%|███▏      | 251/777 [01:33<03:15,  2.69it/s] \n",
      "Training: (loss 0.1013):  32%|███▏      | 252/777 [01:33<03:06,  2.82it/s]\n",
      "Training: (loss 0.1300):  32%|███▏      | 252/777 [01:33<03:06,  2.82it/s]\n",
      "Training: (loss 0.1300):  33%|███▎      | 253/777 [01:33<03:01,  2.88it/s]\n",
      "Training: (loss 0.0234):  33%|███▎      | 253/777 [01:33<03:01,  2.88it/s]\n",
      "Training: (loss 0.0234):  33%|███▎      | 254/777 [01:33<02:49,  3.09it/s]\n",
      "Training: (loss 0.3294):  33%|███▎      | 254/777 [01:34<02:49,  3.09it/s]\n",
      "Training: (loss 0.3294):  33%|███▎      | 255/777 [01:34<02:51,  3.04it/s]\n",
      "Training: (loss 0.3397):  33%|███▎      | 255/777 [01:34<02:51,  3.04it/s]\n",
      "Training: (loss 0.3397):  33%|███▎      | 256/777 [01:34<02:51,  3.03it/s]\n",
      "Training: (loss -0.1948):  33%|███▎      | 256/777 [01:34<02:51,  3.03it/s]\n",
      "Training: (loss -0.1948):  33%|███▎      | 257/777 [01:34<03:03,  2.84it/s]\n",
      "Training: (loss -0.2336):  33%|███▎      | 257/777 [01:35<03:03,  2.84it/s]\n",
      "Training: (loss -0.2336):  33%|███▎      | 258/777 [01:35<02:56,  2.94it/s]\n",
      "Training: (loss 0.3329):  33%|███▎      | 258/777 [01:35<02:56,  2.94it/s] \n",
      "Training: (loss 0.3329):  33%|███▎      | 259/777 [01:35<02:56,  2.94it/s]\n",
      "Training: (loss 0.3373):  33%|███▎      | 259/777 [01:36<02:56,  2.94it/s]\n",
      "Training: (loss 0.3373):  33%|███▎      | 260/777 [01:36<02:59,  2.87it/s]\n",
      "Training: (loss 0.1989):  33%|███▎      | 260/777 [01:36<02:59,  2.87it/s]\n",
      "Training: (loss 0.1989):  34%|███▎      | 261/777 [01:36<03:04,  2.80it/s]\n",
      "Training: (loss 0.0656):  34%|███▎      | 261/777 [01:36<03:04,  2.80it/s]\n",
      "Training: (loss 0.0656):  34%|███▎      | 262/777 [01:36<03:15,  2.63it/s]\n",
      "Training: (loss -0.1523):  34%|███▎      | 262/777 [01:37<03:15,  2.63it/s]\n",
      "Training: (loss -0.1523):  34%|███▍      | 263/777 [01:37<03:10,  2.69it/s]\n",
      "Training: (loss 0.0743):  34%|███▍      | 263/777 [01:37<03:10,  2.69it/s] \n",
      "Training: (loss 0.0743):  34%|███▍      | 264/777 [01:37<03:10,  2.69it/s]\n",
      "Training: (loss -0.0230):  34%|███▍      | 264/777 [01:37<03:10,  2.69it/s]\n",
      "Training: (loss -0.0230):  34%|███▍      | 265/777 [01:37<03:03,  2.79it/s]\n",
      "Training: (loss -0.0393):  34%|███▍      | 265/777 [01:38<03:03,  2.79it/s]\n",
      "Training: (loss -0.0393):  34%|███▍      | 266/777 [01:38<03:08,  2.71it/s]\n",
      "Training: (loss 0.0889):  34%|███▍      | 266/777 [01:38<03:08,  2.71it/s] \n",
      "Training: (loss 0.0889):  34%|███▍      | 267/777 [01:38<03:11,  2.67it/s]\n",
      "Training: (loss 0.0769):  34%|███▍      | 267/777 [01:39<03:11,  2.67it/s]\n",
      "Training: (loss 0.0769):  34%|███▍      | 268/777 [01:39<03:06,  2.73it/s]\n",
      "Training: (loss 0.0057):  34%|███▍      | 268/777 [01:39<03:06,  2.73it/s]\n",
      "Training: (loss 0.0057):  35%|███▍      | 269/777 [01:39<02:58,  2.84it/s]\n",
      "Training: (loss 0.0382):  35%|███▍      | 269/777 [01:39<02:58,  2.84it/s]\n",
      "Training: (loss 0.0382):  35%|███▍      | 270/777 [01:39<03:03,  2.77it/s]\n",
      "Training: (loss 0.3317):  35%|███▍      | 270/777 [01:40<03:03,  2.77it/s]\n",
      "Training: (loss 0.3317):  35%|███▍      | 271/777 [01:40<02:58,  2.84it/s]\n",
      "Training: (loss -0.1825):  35%|███▍      | 271/777 [01:40<02:58,  2.84it/s]\n",
      "Training: (loss -0.1825):  35%|███▌      | 272/777 [01:40<02:47,  3.01it/s]\n",
      "Training: (loss -0.0206):  35%|███▌      | 272/777 [01:40<02:47,  3.01it/s]\n",
      "Training: (loss -0.0206):  35%|███▌      | 273/777 [01:40<03:00,  2.79it/s]\n",
      "Training: (loss 0.2949):  35%|███▌      | 273/777 [01:41<03:00,  2.79it/s] \n",
      "Training: (loss 0.2949):  35%|███▌      | 274/777 [01:41<02:57,  2.83it/s]\n",
      "Training: (loss -0.0582):  35%|███▌      | 274/777 [01:41<02:57,  2.83it/s]\n",
      "Training: (loss -0.0582):  35%|███▌      | 275/777 [01:41<02:51,  2.93it/s]\n",
      "Training: (loss 0.1115):  35%|███▌      | 275/777 [01:41<02:51,  2.93it/s] \n",
      "Training: (loss 0.1115):  36%|███▌      | 276/777 [01:41<02:47,  2.99it/s]\n",
      "Training: (loss -0.0921):  36%|███▌      | 276/777 [01:42<02:47,  2.99it/s]\n",
      "Training: (loss -0.0921):  36%|███▌      | 277/777 [01:42<02:49,  2.96it/s]\n",
      "Training: (loss 0.0256):  36%|███▌      | 277/777 [01:42<02:49,  2.96it/s] \n",
      "Training: (loss 0.0256):  36%|███▌      | 278/777 [01:42<02:45,  3.01it/s]\n",
      "Training: (loss 0.1143):  36%|███▌      | 278/777 [01:42<02:45,  3.01it/s]\n",
      "Training: (loss 0.1143):  36%|███▌      | 279/777 [01:42<02:49,  2.93it/s]\n",
      "Training: (loss -0.0924):  36%|███▌      | 279/777 [01:43<02:49,  2.93it/s]\n",
      "Training: (loss -0.0924):  36%|███▌      | 280/777 [01:43<02:53,  2.86it/s]\n",
      "Training: (loss 0.0050):  36%|███▌      | 280/777 [01:43<02:53,  2.86it/s] \n",
      "Training: (loss 0.0050):  36%|███▌      | 281/777 [01:43<02:56,  2.81it/s]\n",
      "Training: (loss 0.0864):  36%|███▌      | 281/777 [01:43<02:56,  2.81it/s]\n",
      "Training: (loss 0.0864):  36%|███▋      | 282/777 [01:43<02:59,  2.76it/s]\n",
      "Training: (loss -0.0698):  36%|███▋      | 282/777 [01:44<02:59,  2.76it/s]\n",
      "Training: (loss -0.0698):  36%|███▋      | 283/777 [01:44<02:55,  2.82it/s]\n",
      "Training: (loss -0.0820):  36%|███▋      | 283/777 [01:44<02:55,  2.82it/s]\n",
      "Training: (loss -0.0820):  37%|███▋      | 284/777 [01:44<02:59,  2.75it/s]\n",
      "Training: (loss -0.1090):  37%|███▋      | 284/777 [01:44<02:59,  2.75it/s]\n",
      "Training: (loss -0.1090):  37%|███▋      | 285/777 [01:44<02:57,  2.78it/s]\n",
      "Training: (loss -0.1080):  37%|███▋      | 285/777 [01:45<02:57,  2.78it/s]\n",
      "Training: (loss -0.1080):  37%|███▋      | 286/777 [01:45<02:58,  2.75it/s]\n",
      "Training: (loss -0.1238):  37%|███▋      | 286/777 [01:45<02:58,  2.75it/s]\n",
      "Training: (loss -0.1238):  37%|███▋      | 287/777 [01:45<02:55,  2.79it/s]\n",
      "Training: (loss 0.0056):  37%|███▋      | 287/777 [01:45<02:55,  2.79it/s] \n",
      "Training: (loss 0.0056):  37%|███▋      | 288/777 [01:45<02:53,  2.82it/s]\n",
      "Training: (loss 0.0408):  37%|███▋      | 288/777 [01:46<02:53,  2.82it/s]\n",
      "Training: (loss 0.0408):  37%|███▋      | 289/777 [01:46<03:03,  2.66it/s]\n",
      "Training: (loss 0.3213):  37%|███▋      | 289/777 [01:46<03:03,  2.66it/s]\n",
      "Training: (loss 0.3213):  37%|███▋      | 290/777 [01:46<02:50,  2.86it/s]\n",
      "Training: (loss 0.3230):  37%|███▋      | 290/777 [01:47<02:50,  2.86it/s]\n",
      "Training: (loss 0.3230):  37%|███▋      | 291/777 [01:47<02:43,  2.97it/s]\n",
      "Training: (loss 0.3182):  37%|███▋      | 291/777 [01:47<02:43,  2.97it/s]\n",
      "Training: (loss 0.3182):  38%|███▊      | 292/777 [01:47<02:44,  2.95it/s]\n",
      "Training: (loss 0.3225):  38%|███▊      | 292/777 [01:47<02:44,  2.95it/s]\n",
      "Training: (loss 0.3225):  38%|███▊      | 293/777 [01:47<02:41,  3.00it/s]\n",
      "Training: (loss -0.1769):  38%|███▊      | 293/777 [01:48<02:41,  3.00it/s]\n",
      "Training: (loss -0.1769):  38%|███▊      | 294/777 [01:48<02:49,  2.85it/s]\n",
      "Training: (loss -0.1058):  38%|███▊      | 294/777 [01:48<02:49,  2.85it/s]\n",
      "Training: (loss -0.1058):  38%|███▊      | 295/777 [01:48<03:00,  2.67it/s]\n",
      "Training: (loss -0.0288):  38%|███▊      | 295/777 [01:48<03:00,  2.67it/s]\n",
      "Training: (loss -0.0288):  38%|███▊      | 296/777 [01:48<03:12,  2.50it/s]\n",
      "Training: (loss -0.0407):  38%|███▊      | 296/777 [01:49<03:12,  2.50it/s]\n",
      "Training: (loss -0.0407):  38%|███▊      | 297/777 [01:49<03:12,  2.50it/s]\n",
      "Training: (loss -0.1698):  38%|███▊      | 297/777 [01:49<03:12,  2.50it/s]\n",
      "Training: (loss -0.1698):  38%|███▊      | 298/777 [01:49<03:04,  2.59it/s]\n",
      "Training: (loss 0.0045):  38%|███▊      | 298/777 [01:50<03:04,  2.59it/s] \n",
      "Training: (loss 0.0045):  38%|███▊      | 299/777 [01:50<03:03,  2.61it/s]\n",
      "Training: (loss -0.0690):  38%|███▊      | 299/777 [01:50<03:03,  2.61it/s]\n",
      "Training: (loss -0.0690):  39%|███▊      | 300/777 [01:50<02:55,  2.72it/s]\n",
      "Training: (loss 0.3197):  39%|███▊      | 300/777 [01:50<02:55,  2.72it/s] \n",
      "Training: (loss 0.3197):  39%|███▊      | 301/777 [01:50<02:47,  2.85it/s]\n",
      "Training: (loss 0.3207):  39%|███▊      | 301/777 [01:51<02:47,  2.85it/s]\n",
      "Training: (loss 0.3207):  39%|███▉      | 302/777 [01:51<02:52,  2.75it/s]\n",
      "Training: (loss -0.0660):  39%|███▉      | 302/777 [01:51<02:52,  2.75it/s]\n",
      "Training: (loss -0.0660):  39%|███▉      | 303/777 [01:51<03:01,  2.62it/s]\n",
      "Training: (loss 0.0026):  39%|███▉      | 303/777 [01:51<03:01,  2.62it/s] \n",
      "Training: (loss 0.0026):  39%|███▉      | 304/777 [01:51<03:03,  2.58it/s]\n",
      "Training: (loss -0.2199):  39%|███▉      | 304/777 [01:52<03:03,  2.58it/s]\n",
      "Training: (loss -0.2199):  39%|███▉      | 305/777 [01:52<03:07,  2.52it/s]\n",
      "Training: (loss -0.0460):  39%|███▉      | 305/777 [01:52<03:07,  2.52it/s]\n",
      "Training: (loss -0.0460):  39%|███▉      | 306/777 [01:52<03:07,  2.51it/s]\n",
      "Training: (loss -0.0309):  39%|███▉      | 306/777 [01:53<03:07,  2.51it/s]\n",
      "Training: (loss -0.0309):  40%|███▉      | 307/777 [01:53<03:01,  2.59it/s]\n",
      "Training: (loss -0.0047):  40%|███▉      | 307/777 [01:53<03:01,  2.59it/s]\n",
      "Training: (loss -0.0047):  40%|███▉      | 308/777 [01:53<03:05,  2.53it/s]\n",
      "Training: (loss -0.0683):  40%|███▉      | 308/777 [01:53<03:05,  2.53it/s]\n",
      "Training: (loss -0.0683):  40%|███▉      | 309/777 [01:53<03:10,  2.46it/s]\n",
      "Training: (loss 0.0780):  40%|███▉      | 309/777 [01:54<03:10,  2.46it/s] \n",
      "Training: (loss 0.0780):  40%|███▉      | 310/777 [01:54<03:13,  2.41it/s]\n",
      "Training: (loss -0.0213):  40%|███▉      | 310/777 [01:54<03:13,  2.41it/s]\n",
      "Training: (loss -0.0213):  40%|████      | 311/777 [01:54<03:06,  2.50it/s]\n",
      "Training: (loss 0.0651):  40%|████      | 311/777 [01:55<03:06,  2.50it/s] \n",
      "Training: (loss 0.0651):  40%|████      | 312/777 [01:55<03:04,  2.52it/s]\n",
      "Training: (loss -0.1435):  40%|████      | 312/777 [01:55<03:04,  2.52it/s]\n",
      "Training: (loss -0.1435):  40%|████      | 313/777 [01:55<02:59,  2.58it/s]\n",
      "Training: (loss 0.3293):  40%|████      | 313/777 [01:55<02:59,  2.58it/s] \n",
      "Training: (loss 0.3293):  40%|████      | 314/777 [01:55<02:48,  2.75it/s]\n",
      "Training: (loss 0.3279):  40%|████      | 314/777 [01:56<02:48,  2.75it/s]\n",
      "Training: (loss 0.3279):  41%|████      | 315/777 [01:56<02:46,  2.78it/s]\n",
      "Training: (loss 0.0274):  41%|████      | 315/777 [01:56<02:46,  2.78it/s]\n",
      "Training: (loss 0.0274):  41%|████      | 316/777 [01:56<02:38,  2.90it/s]\n",
      "Training: (loss -0.1496):  41%|████      | 316/777 [01:56<02:38,  2.90it/s]\n",
      "Training: (loss -0.1496):  41%|████      | 317/777 [01:56<02:48,  2.73it/s]\n",
      "Training: (loss 0.1165):  41%|████      | 317/777 [01:57<02:48,  2.73it/s] \n",
      "Training: (loss 0.1165):  41%|████      | 318/777 [01:57<02:43,  2.81it/s]\n",
      "Training: (loss 0.1370):  41%|████      | 318/777 [01:57<02:43,  2.81it/s]\n",
      "Training: (loss 0.1370):  41%|████      | 319/777 [01:57<02:44,  2.78it/s]\n",
      "Training: (loss -0.1884):  41%|████      | 319/777 [01:58<02:44,  2.78it/s]\n",
      "Training: (loss -0.1884):  41%|████      | 320/777 [01:58<02:49,  2.69it/s]\n",
      "Training: (loss -0.0905):  41%|████      | 320/777 [01:58<02:49,  2.69it/s]\n",
      "Training: (loss -0.0905):  41%|████▏     | 321/777 [01:58<02:48,  2.70it/s]\n",
      "Training: (loss -0.0856):  41%|████▏     | 321/777 [01:58<02:48,  2.70it/s]\n",
      "Training: (loss -0.0856):  41%|████▏     | 322/777 [01:58<02:44,  2.76it/s]\n",
      "Training: (loss -0.0611):  41%|████▏     | 322/777 [01:59<02:44,  2.76it/s]\n",
      "Training: (loss -0.0611):  42%|████▏     | 323/777 [01:59<02:41,  2.81it/s]\n",
      "Training: (loss 0.1458):  42%|████▏     | 323/777 [01:59<02:41,  2.81it/s] \n",
      "Training: (loss 0.1458):  42%|████▏     | 324/777 [01:59<02:39,  2.85it/s]\n",
      "Training: (loss 0.2796):  42%|████▏     | 324/777 [01:59<02:39,  2.85it/s]\n",
      "Training: (loss 0.2796):  42%|████▏     | 325/777 [01:59<02:40,  2.81it/s]\n",
      "Training: (loss 0.3313):  42%|████▏     | 325/777 [02:00<02:40,  2.81it/s]\n",
      "Training: (loss 0.3313):  42%|████▏     | 326/777 [02:00<02:44,  2.75it/s]\n",
      "Training: (loss 0.3319):  42%|████▏     | 326/777 [02:00<02:44,  2.75it/s]\n",
      "Training: (loss 0.3319):  42%|████▏     | 327/777 [02:00<02:41,  2.79it/s]\n",
      "Training: (loss 0.3251):  42%|████▏     | 327/777 [02:00<02:41,  2.79it/s]\n",
      "Training: (loss 0.3251):  42%|████▏     | 328/777 [02:00<02:47,  2.68it/s]\n",
      "Training: (loss -0.0011):  42%|████▏     | 328/777 [02:01<02:47,  2.68it/s]\n",
      "Training: (loss -0.0011):  42%|████▏     | 329/777 [02:01<02:44,  2.72it/s]\n",
      "Training: (loss 0.3333):  42%|████▏     | 329/777 [02:01<02:44,  2.72it/s] \n",
      "Training: (loss 0.3333):  42%|████▏     | 330/777 [02:01<02:39,  2.80it/s]\n",
      "Training: (loss -0.0828):  42%|████▏     | 330/777 [02:01<02:39,  2.80it/s]\n",
      "Training: (loss -0.0828):  43%|████▎     | 331/777 [02:01<02:42,  2.75it/s]\n",
      "Training: (loss -0.2373):  43%|████▎     | 331/777 [02:02<02:42,  2.75it/s]\n",
      "Training: (loss -0.2373):  43%|████▎     | 332/777 [02:02<02:40,  2.77it/s]\n",
      "Training: (loss -0.1451):  43%|████▎     | 332/777 [02:02<02:40,  2.77it/s]\n",
      "Training: (loss -0.1451):  43%|████▎     | 333/777 [02:02<02:35,  2.85it/s]\n",
      "Training: (loss -0.1037):  43%|████▎     | 333/777 [02:03<02:35,  2.85it/s]\n",
      "Training: (loss -0.1037):  43%|████▎     | 334/777 [02:03<02:39,  2.78it/s]\n",
      "Training: (loss 0.2212):  43%|████▎     | 334/777 [02:03<02:39,  2.78it/s] \n",
      "Training: (loss 0.2212):  43%|████▎     | 335/777 [02:03<02:42,  2.72it/s]\n",
      "Training: (loss 0.3185):  43%|████▎     | 335/777 [02:03<02:42,  2.72it/s]\n",
      "Training: (loss 0.3185):  43%|████▎     | 336/777 [02:03<02:42,  2.71it/s]\n",
      "Training: (loss 0.2541):  43%|████▎     | 336/777 [02:04<02:42,  2.71it/s]\n",
      "Training: (loss 0.2541):  43%|████▎     | 337/777 [02:04<02:32,  2.88it/s]\n",
      "Training: (loss 0.3181):  43%|████▎     | 337/777 [02:04<02:32,  2.88it/s]\n",
      "Training: (loss 0.3181):  44%|████▎     | 338/777 [02:04<02:37,  2.79it/s]\n",
      "Training: (loss -0.0154):  44%|████▎     | 338/777 [02:04<02:37,  2.79it/s]\n",
      "Training: (loss -0.0154):  44%|████▎     | 339/777 [02:04<02:43,  2.68it/s]\n",
      "Training: (loss -0.1829):  44%|████▎     | 339/777 [02:05<02:43,  2.68it/s]\n",
      "Training: (loss -0.1829):  44%|████▍     | 340/777 [02:05<02:35,  2.81it/s]\n",
      "Training: (loss -0.0787):  44%|████▍     | 340/777 [02:05<02:35,  2.81it/s]\n",
      "Training: (loss -0.0787):  44%|████▍     | 341/777 [02:05<02:43,  2.67it/s]\n",
      "Training: (loss -0.0732):  44%|████▍     | 341/777 [02:05<02:43,  2.67it/s]\n",
      "Training: (loss -0.0732):  44%|████▍     | 342/777 [02:05<02:35,  2.79it/s]\n",
      "Training: (loss 0.3287):  44%|████▍     | 342/777 [02:06<02:35,  2.79it/s] \n",
      "Training: (loss 0.3287):  44%|████▍     | 343/777 [02:06<02:35,  2.80it/s]\n",
      "Training: (loss -0.0946):  44%|████▍     | 343/777 [02:06<02:35,  2.80it/s]\n",
      "Training: (loss -0.0946):  44%|████▍     | 344/777 [02:06<02:34,  2.81it/s]\n",
      "Training: (loss -0.0483):  44%|████▍     | 344/777 [02:07<02:34,  2.81it/s]\n",
      "Training: (loss -0.0483):  44%|████▍     | 345/777 [02:07<02:43,  2.64it/s]\n",
      "Training: (loss 0.2391):  44%|████▍     | 345/777 [02:07<02:43,  2.64it/s] \n",
      "Training: (loss 0.2391):  45%|████▍     | 346/777 [02:07<02:35,  2.77it/s]\n",
      "Training: (loss -0.1262):  45%|████▍     | 346/777 [02:07<02:35,  2.77it/s]\n",
      "Training: (loss -0.1262):  45%|████▍     | 347/777 [02:07<02:43,  2.64it/s]\n",
      "Training: (loss 0.0548):  45%|████▍     | 347/777 [02:08<02:43,  2.64it/s] \n",
      "Training: (loss 0.0548):  45%|████▍     | 348/777 [02:08<02:49,  2.53it/s]\n",
      "Training: (loss -0.0228):  45%|████▍     | 348/777 [02:08<02:49,  2.53it/s]\n",
      "Training: (loss -0.0228):  45%|████▍     | 349/777 [02:08<02:41,  2.64it/s]\n",
      "Training: (loss 0.1256):  45%|████▍     | 349/777 [02:08<02:41,  2.64it/s] \n",
      "Training: (loss 0.1256):  45%|████▌     | 350/777 [02:08<02:38,  2.70it/s]\n",
      "Training: (loss -0.2099):  45%|████▌     | 350/777 [02:09<02:38,  2.70it/s]\n",
      "Training: (loss -0.2099):  45%|████▌     | 351/777 [02:09<02:42,  2.61it/s]\n",
      "Training: (loss -0.0616):  45%|████▌     | 351/777 [02:09<02:42,  2.61it/s]\n",
      "Training: (loss -0.0616):  45%|████▌     | 352/777 [02:09<02:51,  2.47it/s]\n",
      "Training: (loss -0.0577):  45%|████▌     | 352/777 [02:10<02:51,  2.47it/s]\n",
      "Training: (loss -0.0577):  45%|████▌     | 353/777 [02:10<02:52,  2.46it/s]\n",
      "Training: (loss -0.0808):  45%|████▌     | 353/777 [02:10<02:52,  2.46it/s]\n",
      "Training: (loss -0.0808):  46%|████▌     | 354/777 [02:10<02:44,  2.57it/s]\n",
      "Training: (loss 0.0381):  46%|████▌     | 354/777 [02:10<02:44,  2.57it/s] \n",
      "Training: (loss 0.0381):  46%|████▌     | 355/777 [02:10<02:40,  2.63it/s]\n",
      "Training: (loss -0.0568):  46%|████▌     | 355/777 [02:11<02:40,  2.63it/s]\n",
      "Training: (loss -0.0568):  46%|████▌     | 356/777 [02:11<02:43,  2.57it/s]\n",
      "Training: (loss -0.0035):  46%|████▌     | 356/777 [02:11<02:43,  2.57it/s]\n",
      "Training: (loss -0.0035):  46%|████▌     | 357/777 [02:11<02:46,  2.53it/s]\n",
      "Training: (loss -0.0724):  46%|████▌     | 357/777 [02:12<02:46,  2.53it/s]\n",
      "Training: (loss -0.0724):  46%|████▌     | 358/777 [02:12<02:36,  2.67it/s]\n",
      "Training: (loss -0.1165):  46%|████▌     | 358/777 [02:12<02:36,  2.67it/s]\n",
      "Training: (loss -0.1165):  46%|████▌     | 359/777 [02:12<02:43,  2.55it/s]\n",
      "Training: (loss -0.0323):  46%|████▌     | 359/777 [02:12<02:43,  2.55it/s]\n",
      "Training: (loss -0.0323):  46%|████▋     | 360/777 [02:12<02:41,  2.58it/s]\n",
      "Training: (loss 0.3244):  46%|████▋     | 360/777 [02:13<02:41,  2.58it/s] \n",
      "Training: (loss 0.3244):  46%|████▋     | 361/777 [02:13<02:36,  2.66it/s]\n",
      "Training: (loss -0.0517):  46%|████▋     | 361/777 [02:13<02:36,  2.66it/s]\n",
      "Training: (loss -0.0517):  47%|████▋     | 362/777 [02:13<02:35,  2.67it/s]\n",
      "Training: (loss 0.0657):  47%|████▋     | 362/777 [02:14<02:35,  2.67it/s] \n",
      "Training: (loss 0.0657):  47%|████▋     | 363/777 [02:14<02:38,  2.62it/s]\n",
      "Training: (loss 0.3304):  47%|████▋     | 363/777 [02:14<02:38,  2.62it/s]\n",
      "Training: (loss 0.3304):  47%|████▋     | 364/777 [02:14<02:43,  2.53it/s]\n",
      "Training: (loss -0.1344):  47%|████▋     | 364/777 [02:14<02:43,  2.53it/s]\n",
      "Training: (loss -0.1344):  47%|████▋     | 365/777 [02:14<02:35,  2.65it/s]\n",
      "Training: (loss 0.1236):  47%|████▋     | 365/777 [02:15<02:35,  2.65it/s] \n",
      "Training: (loss 0.1236):  47%|████▋     | 366/777 [02:15<02:31,  2.71it/s]\n",
      "Training: (loss 0.3407):  47%|████▋     | 366/777 [02:15<02:31,  2.71it/s]\n",
      "Training: (loss 0.3407):  47%|████▋     | 367/777 [02:15<02:35,  2.64it/s]\n",
      "Training: (loss -0.1189):  47%|████▋     | 367/777 [02:16<02:35,  2.64it/s]\n",
      "Training: (loss -0.1189):  47%|████▋     | 368/777 [02:16<02:46,  2.46it/s]\n",
      "Training: (loss -0.0475):  47%|████▋     | 368/777 [02:16<02:46,  2.46it/s]\n",
      "Training: (loss -0.0475):  47%|████▋     | 369/777 [02:16<02:47,  2.43it/s]\n",
      "Training: (loss 0.3246):  47%|████▋     | 369/777 [02:16<02:47,  2.43it/s] \n",
      "Training: (loss 0.3246):  48%|████▊     | 370/777 [02:16<02:41,  2.52it/s]\n",
      "Training: (loss 0.3285):  48%|████▊     | 370/777 [02:17<02:41,  2.52it/s]\n",
      "Training: (loss 0.3285):  48%|████▊     | 371/777 [02:17<02:39,  2.54it/s]\n",
      "Training: (loss -0.0763):  48%|████▊     | 371/777 [02:17<02:39,  2.54it/s]\n",
      "Training: (loss -0.0763):  48%|████▊     | 372/777 [02:17<02:29,  2.70it/s]\n",
      "Training: (loss -0.1859):  48%|████▊     | 372/777 [02:17<02:29,  2.70it/s]\n",
      "Training: (loss -0.1859):  48%|████▊     | 373/777 [02:17<02:23,  2.82it/s]\n",
      "Training: (loss 0.1445):  48%|████▊     | 373/777 [02:18<02:23,  2.82it/s] \n",
      "Training: (loss 0.1445):  48%|████▊     | 374/777 [02:18<02:31,  2.66it/s]\n",
      "Training: (loss -0.0188):  48%|████▊     | 374/777 [02:18<02:31,  2.66it/s]\n",
      "Training: (loss -0.0188):  48%|████▊     | 375/777 [02:18<02:23,  2.81it/s]\n",
      "Training: (loss 0.0587):  48%|████▊     | 375/777 [02:18<02:23,  2.81it/s] \n",
      "Training: (loss 0.0587):  48%|████▊     | 376/777 [02:18<02:30,  2.67it/s]\n",
      "Training: (loss -0.0241):  48%|████▊     | 376/777 [02:19<02:30,  2.67it/s]\n",
      "Training: (loss -0.0241):  49%|████▊     | 377/777 [02:19<02:28,  2.69it/s]\n",
      "Training: (loss -0.1317):  49%|████▊     | 377/777 [02:19<02:28,  2.69it/s]\n",
      "Training: (loss -0.1317):  49%|████▊     | 378/777 [02:19<02:40,  2.49it/s]\n",
      "Training: (loss 0.3251):  49%|████▊     | 378/777 [02:20<02:40,  2.49it/s] \n",
      "Training: (loss 0.3251):  49%|████▉     | 379/777 [02:20<02:29,  2.66it/s]\n",
      "Training: (loss 0.0849):  49%|████▉     | 379/777 [02:20<02:29,  2.66it/s]\n",
      "Training: (loss 0.0849):  49%|████▉     | 380/777 [02:20<02:28,  2.67it/s]\n",
      "Training: (loss -0.0867):  49%|████▉     | 380/777 [02:20<02:28,  2.67it/s]\n",
      "Training: (loss -0.0867):  49%|████▉     | 381/777 [02:20<02:27,  2.68it/s]\n",
      "Training: (loss 0.3287):  49%|████▉     | 381/777 [02:21<02:27,  2.68it/s] \n",
      "Training: (loss 0.3287):  49%|████▉     | 382/777 [02:21<02:28,  2.67it/s]\n",
      "Training: (loss 0.0658):  49%|████▉     | 382/777 [02:21<02:28,  2.67it/s]\n",
      "Training: (loss 0.0658):  49%|████▉     | 383/777 [02:21<02:26,  2.69it/s]\n",
      "Training: (loss -0.0890):  49%|████▉     | 383/777 [02:22<02:26,  2.69it/s]\n",
      "Training: (loss -0.0890):  49%|████▉     | 384/777 [02:22<02:30,  2.62it/s]\n",
      "Training: (loss -0.1769):  49%|████▉     | 384/777 [02:22<02:30,  2.62it/s]\n",
      "Training: (loss -0.1769):  50%|████▉     | 385/777 [02:22<02:33,  2.56it/s]\n",
      "Training: (loss -0.0717):  50%|████▉     | 385/777 [02:22<02:33,  2.56it/s]\n",
      "Training: (loss -0.0717):  50%|████▉     | 386/777 [02:22<02:36,  2.50it/s]\n",
      "Training: (loss -0.1294):  50%|████▉     | 386/777 [02:23<02:36,  2.50it/s]\n",
      "Training: (loss -0.1294):  50%|████▉     | 387/777 [02:23<02:34,  2.52it/s]\n",
      "Training: (loss -0.0963):  50%|████▉     | 387/777 [02:23<02:34,  2.52it/s]\n",
      "Training: (loss -0.0963):  50%|████▉     | 388/777 [02:23<02:22,  2.73it/s]\n",
      "Training: (loss -0.2012):  50%|████▉     | 388/777 [02:23<02:22,  2.73it/s]\n",
      "Training: (loss -0.2012):  50%|█████     | 389/777 [02:23<02:13,  2.91it/s]\n",
      "Training: (loss -0.0921):  50%|█████     | 389/777 [02:24<02:13,  2.91it/s]\n",
      "Training: (loss -0.0921):  50%|█████     | 390/777 [02:24<02:12,  2.92it/s]\n",
      "Training: (loss -0.0807):  50%|█████     | 390/777 [02:24<02:12,  2.92it/s]\n",
      "Training: (loss -0.0807):  50%|█████     | 391/777 [02:24<02:07,  3.04it/s]\n",
      "Training: (loss 0.2034):  50%|█████     | 391/777 [02:24<02:07,  3.04it/s] \n",
      "Training: (loss 0.2034):  50%|█████     | 392/777 [02:24<02:13,  2.89it/s]\n",
      "Training: (loss 0.3243):  50%|█████     | 392/777 [02:25<02:13,  2.89it/s]\n",
      "Training: (loss 0.3243):  51%|█████     | 393/777 [02:25<02:14,  2.85it/s]\n",
      "Training: (loss -0.0057):  51%|█████     | 393/777 [02:25<02:14,  2.85it/s]\n",
      "Training: (loss -0.0057):  51%|█████     | 394/777 [02:25<02:15,  2.83it/s]\n",
      "Training: (loss 0.3275):  51%|█████     | 394/777 [02:25<02:15,  2.83it/s] \n",
      "Training: (loss 0.3275):  51%|█████     | 395/777 [02:25<02:19,  2.75it/s]\n",
      "Training: (loss 0.3251):  51%|█████     | 395/777 [02:26<02:19,  2.75it/s]\n",
      "Training: (loss 0.3251):  51%|█████     | 396/777 [02:26<02:18,  2.75it/s]\n",
      "Training: (loss -0.0849):  51%|█████     | 396/777 [02:26<02:18,  2.75it/s]\n",
      "Training: (loss -0.0849):  51%|█████     | 397/777 [02:26<02:21,  2.69it/s]\n",
      "Training: (loss -0.0209):  51%|█████     | 397/777 [02:27<02:21,  2.69it/s]\n",
      "Training: (loss -0.0209):  51%|█████     | 398/777 [02:27<02:22,  2.65it/s]\n",
      "Training: (loss -0.1561):  51%|█████     | 398/777 [02:27<02:22,  2.65it/s]\n",
      "Training: (loss -0.1561):  51%|█████▏    | 399/777 [02:27<02:24,  2.61it/s]\n",
      "Training: (loss -0.0164):  51%|█████▏    | 399/777 [02:27<02:24,  2.61it/s]\n",
      "Training: (loss -0.0164):  51%|█████▏    | 400/777 [02:27<02:18,  2.72it/s]\n",
      "Training: (loss -0.0139):  51%|█████▏    | 400/777 [02:28<02:18,  2.72it/s]\n",
      "Training: (loss -0.0139):  52%|█████▏    | 401/777 [02:28<02:19,  2.70it/s]\n",
      "Training: (loss -0.1106):  52%|█████▏    | 401/777 [02:28<02:19,  2.70it/s]\n",
      "Training: (loss -0.1106):  52%|█████▏    | 402/777 [02:28<02:12,  2.83it/s]\n",
      "Training: (loss -0.1277):  52%|█████▏    | 402/777 [02:28<02:12,  2.83it/s]\n",
      "Training: (loss -0.1277):  52%|█████▏    | 403/777 [02:28<02:11,  2.85it/s]\n",
      "Training: (loss 0.3270):  52%|█████▏    | 403/777 [02:29<02:11,  2.85it/s] \n",
      "Training: (loss 0.3270):  52%|█████▏    | 404/777 [02:29<02:14,  2.77it/s]\n",
      "Training: (loss -0.1403):  52%|█████▏    | 404/777 [02:29<02:14,  2.77it/s]\n",
      "Training: (loss -0.1403):  52%|█████▏    | 405/777 [02:29<02:08,  2.90it/s]\n",
      "Training: (loss -0.0568):  52%|█████▏    | 405/777 [02:29<02:08,  2.90it/s]\n",
      "Training: (loss -0.0568):  52%|█████▏    | 406/777 [02:29<02:01,  3.06it/s]\n",
      "Training: (loss 0.3226):  52%|█████▏    | 406/777 [02:30<02:01,  3.06it/s] \n",
      "Training: (loss 0.3226):  52%|█████▏    | 407/777 [02:30<02:04,  2.98it/s]\n",
      "Training: (loss 0.0616):  52%|█████▏    | 407/777 [02:30<02:04,  2.98it/s]\n",
      "Training: (loss 0.0616):  53%|█████▎    | 408/777 [02:30<02:09,  2.84it/s]\n",
      "Training: (loss -0.0329):  53%|█████▎    | 408/777 [02:30<02:09,  2.84it/s]\n",
      "Training: (loss -0.0329):  53%|█████▎    | 409/777 [02:30<02:13,  2.75it/s]\n",
      "Training: (loss 0.1067):  53%|█████▎    | 409/777 [02:31<02:13,  2.75it/s] \n",
      "Training: (loss 0.1067):  53%|█████▎    | 410/777 [02:31<02:13,  2.76it/s]\n",
      "Training: (loss 0.3068):  53%|█████▎    | 410/777 [02:31<02:13,  2.76it/s]\n",
      "Training: (loss 0.3068):  53%|█████▎    | 411/777 [02:31<02:15,  2.70it/s]\n",
      "Training: (loss 0.2504):  53%|█████▎    | 411/777 [02:32<02:15,  2.70it/s]\n",
      "Training: (loss 0.2504):  53%|█████▎    | 412/777 [02:32<02:17,  2.66it/s]\n",
      "Training: (loss 0.3298):  53%|█████▎    | 412/777 [02:32<02:17,  2.66it/s]\n",
      "Training: (loss 0.3298):  53%|█████▎    | 413/777 [02:32<02:20,  2.59it/s]\n",
      "Training: (loss -0.0268):  53%|█████▎    | 413/777 [02:32<02:20,  2.59it/s]\n",
      "Training: (loss -0.0268):  53%|█████▎    | 414/777 [02:32<02:24,  2.51it/s]\n",
      "Training: (loss -0.1124):  53%|█████▎    | 414/777 [02:33<02:24,  2.51it/s]\n",
      "Training: (loss -0.1124):  53%|█████▎    | 415/777 [02:33<02:21,  2.56it/s]\n",
      "Training: (loss 0.2299):  53%|█████▎    | 415/777 [02:33<02:21,  2.56it/s] \n",
      "Training: (loss 0.2299):  54%|█████▎    | 416/777 [02:33<02:08,  2.81it/s]\n",
      "Training: (loss -0.0379):  54%|█████▎    | 416/777 [02:33<02:08,  2.81it/s]\n",
      "Training: (loss -0.0379):  54%|█████▎    | 417/777 [02:33<02:08,  2.80it/s]\n",
      "Training: (loss -0.1561):  54%|█████▎    | 417/777 [02:34<02:08,  2.80it/s]\n",
      "Training: (loss -0.1561):  54%|█████▍    | 418/777 [02:34<02:09,  2.77it/s]\n",
      "Training: (loss 0.3286):  54%|█████▍    | 418/777 [02:34<02:09,  2.77it/s] \n",
      "Training: (loss 0.3286):  54%|█████▍    | 419/777 [02:34<02:15,  2.64it/s]\n",
      "Training: (loss -0.0508):  54%|█████▍    | 419/777 [02:35<02:15,  2.64it/s]\n",
      "Training: (loss -0.0508):  54%|█████▍    | 420/777 [02:35<02:10,  2.74it/s]\n",
      "Training: (loss -0.1043):  54%|█████▍    | 420/777 [02:35<02:10,  2.74it/s]\n",
      "Training: (loss -0.1043):  54%|█████▍    | 421/777 [02:35<02:03,  2.89it/s]\n",
      "Training: (loss -0.1084):  54%|█████▍    | 421/777 [02:35<02:03,  2.89it/s]\n",
      "Training: (loss -0.1084):  54%|█████▍    | 422/777 [02:35<02:10,  2.71it/s]\n",
      "Training: (loss 0.3343):  54%|█████▍    | 422/777 [02:36<02:10,  2.71it/s] \n",
      "Training: (loss 0.3343):  54%|█████▍    | 423/777 [02:36<02:08,  2.75it/s]\n",
      "Training: (loss -0.1166):  54%|█████▍    | 423/777 [02:36<02:08,  2.75it/s]\n",
      "Training: (loss -0.1166):  55%|█████▍    | 424/777 [02:36<02:07,  2.77it/s]\n",
      "Training: (loss -0.0214):  55%|█████▍    | 424/777 [02:36<02:07,  2.77it/s]\n",
      "Training: (loss -0.0214):  55%|█████▍    | 425/777 [02:36<02:07,  2.76it/s]\n",
      "Training: (loss 0.3473):  55%|█████▍    | 425/777 [02:37<02:07,  2.76it/s] \n",
      "Training: (loss 0.3473):  55%|█████▍    | 426/777 [02:37<02:06,  2.78it/s]\n",
      "Training: (loss -0.1481):  55%|█████▍    | 426/777 [02:37<02:06,  2.78it/s]\n",
      "Training: (loss -0.1481):  55%|█████▍    | 427/777 [02:37<01:58,  2.95it/s]\n",
      "Training: (loss 0.3280):  55%|█████▍    | 427/777 [02:37<01:58,  2.95it/s] \n",
      "Training: (loss 0.3280):  55%|█████▌    | 428/777 [02:37<01:55,  3.01it/s]\n",
      "Training: (loss -0.1971):  55%|█████▌    | 428/777 [02:38<01:55,  3.01it/s]\n",
      "Training: (loss -0.1971):  55%|█████▌    | 429/777 [02:38<01:56,  2.98it/s]\n",
      "Training: (loss -0.0728):  55%|█████▌    | 429/777 [02:38<01:56,  2.98it/s]\n",
      "Training: (loss -0.0728):  55%|█████▌    | 430/777 [02:38<01:54,  3.02it/s]\n",
      "Training: (loss -0.1445):  55%|█████▌    | 430/777 [02:38<01:54,  3.02it/s]\n",
      "Training: (loss -0.1445):  55%|█████▌    | 431/777 [02:38<01:54,  3.03it/s]\n",
      "Training: (loss 0.2091):  55%|█████▌    | 431/777 [02:39<01:54,  3.03it/s] \n",
      "Training: (loss 0.2091):  56%|█████▌    | 432/777 [02:39<01:54,  3.00it/s]\n",
      "Training: (loss -0.0298):  56%|█████▌    | 432/777 [02:39<01:54,  3.00it/s]\n",
      "Training: (loss -0.0298):  56%|█████▌    | 433/777 [02:39<01:52,  3.05it/s]\n",
      "Training: (loss -0.1767):  56%|█████▌    | 433/777 [02:39<01:52,  3.05it/s]\n",
      "Training: (loss -0.1767):  56%|█████▌    | 434/777 [02:39<01:50,  3.10it/s]\n",
      "Training: (loss -0.1134):  56%|█████▌    | 434/777 [02:40<01:50,  3.10it/s]\n",
      "Training: (loss -0.1134):  56%|█████▌    | 435/777 [02:40<01:50,  3.09it/s]\n",
      "Training: (loss -0.1103):  56%|█████▌    | 435/777 [02:40<01:50,  3.09it/s]\n",
      "Training: (loss -0.1103):  56%|█████▌    | 436/777 [02:40<01:53,  3.02it/s]\n",
      "Training: (loss -0.0723):  56%|█████▌    | 436/777 [02:40<01:53,  3.02it/s]\n",
      "Training: (loss -0.0723):  56%|█████▌    | 437/777 [02:40<01:54,  2.98it/s]\n",
      "Training: (loss 0.3398):  56%|█████▌    | 437/777 [02:41<01:54,  2.98it/s] \n",
      "Training: (loss 0.3398):  56%|█████▋    | 438/777 [02:41<02:04,  2.73it/s]\n",
      "Training: (loss 0.0373):  56%|█████▋    | 438/777 [02:41<02:04,  2.73it/s]\n",
      "Training: (loss 0.0373):  56%|█████▋    | 439/777 [02:41<02:07,  2.64it/s]\n",
      "Training: (loss 0.1076):  56%|█████▋    | 439/777 [02:42<02:07,  2.64it/s]\n",
      "Training: (loss 0.1076):  57%|█████▋    | 440/777 [02:42<02:11,  2.56it/s]\n",
      "Training: (loss 0.0875):  57%|█████▋    | 440/777 [02:42<02:11,  2.56it/s]\n",
      "Training: (loss 0.0875):  57%|█████▋    | 441/777 [02:42<02:08,  2.62it/s]\n",
      "Training: (loss -0.1901):  57%|█████▋    | 441/777 [02:42<02:08,  2.62it/s]\n",
      "Training: (loss -0.1901):  57%|█████▋    | 442/777 [02:42<02:08,  2.61it/s]\n",
      "Training: (loss -0.0983):  57%|█████▋    | 442/777 [02:43<02:08,  2.61it/s]\n",
      "Training: (loss -0.0983):  57%|█████▋    | 443/777 [02:43<02:07,  2.63it/s]\n",
      "Training: (loss -0.1733):  57%|█████▋    | 443/777 [02:43<02:07,  2.63it/s]\n",
      "Training: (loss -0.1733):  57%|█████▋    | 444/777 [02:43<02:18,  2.41it/s]\n",
      "Training: (loss 0.0899):  57%|█████▋    | 444/777 [02:44<02:18,  2.41it/s] \n",
      "Training: (loss 0.0899):  57%|█████▋    | 445/777 [02:44<02:16,  2.44it/s]\n",
      "Training: (loss -0.0015):  57%|█████▋    | 445/777 [02:44<02:16,  2.44it/s]\n",
      "Training: (loss -0.0015):  57%|█████▋    | 446/777 [02:44<02:13,  2.48it/s]\n",
      "Training: (loss -0.1868):  57%|█████▋    | 446/777 [02:44<02:13,  2.48it/s]\n",
      "Training: (loss -0.1868):  58%|█████▊    | 447/777 [02:44<02:16,  2.42it/s]\n",
      "Training: (loss -0.1969):  58%|█████▊    | 447/777 [02:45<02:16,  2.42it/s]\n",
      "Training: (loss -0.1969):  58%|█████▊    | 448/777 [02:45<02:15,  2.43it/s]\n",
      "Training: (loss -0.1478):  58%|█████▊    | 448/777 [02:45<02:15,  2.43it/s]\n",
      "Training: (loss -0.1478):  58%|█████▊    | 449/777 [02:45<02:20,  2.33it/s]\n",
      "Training: (loss -0.0708):  58%|█████▊    | 449/777 [02:46<02:20,  2.33it/s]\n",
      "Training: (loss -0.0708):  58%|█████▊    | 450/777 [02:46<02:14,  2.43it/s]\n",
      "Training: (loss -0.0927):  58%|█████▊    | 450/777 [02:46<02:14,  2.43it/s]\n",
      "Training: (loss -0.0927):  58%|█████▊    | 451/777 [02:46<02:14,  2.43it/s]\n",
      "Training: (loss -0.1506):  58%|█████▊    | 451/777 [02:46<02:14,  2.43it/s]\n",
      "Training: (loss -0.1506):  58%|█████▊    | 452/777 [02:46<02:05,  2.58it/s]\n",
      "Training: (loss 0.3312):  58%|█████▊    | 452/777 [02:47<02:05,  2.58it/s] \n",
      "Training: (loss 0.3312):  58%|█████▊    | 453/777 [02:47<02:03,  2.62it/s]\n",
      "Training: (loss 0.0004):  58%|█████▊    | 453/777 [02:47<02:03,  2.62it/s]\n",
      "Training: (loss 0.0004):  58%|█████▊    | 454/777 [02:47<01:57,  2.74it/s]\n",
      "Training: (loss -0.1071):  58%|█████▊    | 454/777 [02:47<01:57,  2.74it/s]\n",
      "Training: (loss -0.1071):  59%|█████▊    | 455/777 [02:47<01:52,  2.87it/s]\n",
      "Training: (loss -0.1958):  59%|█████▊    | 455/777 [02:48<01:52,  2.87it/s]\n",
      "Training: (loss -0.1958):  59%|█████▊    | 456/777 [02:48<01:56,  2.76it/s]\n",
      "Training: (loss -0.1698):  59%|█████▊    | 456/777 [02:48<01:56,  2.76it/s]\n",
      "Training: (loss -0.1698):  59%|█████▉    | 457/777 [02:48<01:51,  2.87it/s]\n",
      "Training: (loss -0.0647):  59%|█████▉    | 457/777 [02:49<01:51,  2.87it/s]\n",
      "Training: (loss -0.0647):  59%|█████▉    | 458/777 [02:49<01:57,  2.73it/s]\n",
      "Training: (loss 0.3421):  59%|█████▉    | 458/777 [02:49<01:57,  2.73it/s] \n",
      "Training: (loss 0.3421):  59%|█████▉    | 459/777 [02:49<01:58,  2.68it/s]\n",
      "Training: (loss 0.1956):  59%|█████▉    | 459/777 [02:49<01:58,  2.68it/s]\n",
      "Training: (loss 0.1956):  59%|█████▉    | 460/777 [02:49<01:55,  2.73it/s]\n",
      "Training: (loss -0.0598):  59%|█████▉    | 460/777 [02:50<01:55,  2.73it/s]\n",
      "Training: (loss -0.0598):  59%|█████▉    | 461/777 [02:50<02:00,  2.62it/s]\n",
      "Training: (loss 0.0075):  59%|█████▉    | 461/777 [02:50<02:00,  2.62it/s] \n",
      "Training: (loss 0.0075):  59%|█████▉    | 462/777 [02:50<01:54,  2.76it/s]\n",
      "Training: (loss 0.3465):  59%|█████▉    | 462/777 [02:50<01:54,  2.76it/s]\n",
      "Training: (loss 0.3465):  60%|█████▉    | 463/777 [02:50<02:00,  2.60it/s]\n",
      "Training: (loss -0.1737):  60%|█████▉    | 463/777 [02:51<02:00,  2.60it/s]\n",
      "Training: (loss -0.1737):  60%|█████▉    | 464/777 [02:51<01:58,  2.63it/s]\n",
      "Training: (loss 0.1665):  60%|█████▉    | 464/777 [02:51<01:58,  2.63it/s] \n",
      "Training: (loss 0.1665):  60%|█████▉    | 465/777 [02:51<01:58,  2.64it/s]\n",
      "Training: (loss -0.1807):  60%|█████▉    | 465/777 [02:51<01:58,  2.64it/s]\n",
      "Training: (loss -0.1807):  60%|█████▉    | 466/777 [02:51<01:51,  2.80it/s]\n",
      "Training: (loss -0.1006):  60%|█████▉    | 466/777 [02:52<01:51,  2.80it/s]\n",
      "Training: (loss -0.1006):  60%|██████    | 467/777 [02:52<01:46,  2.92it/s]\n",
      "Training: (loss -0.1382):  60%|██████    | 467/777 [02:52<01:46,  2.92it/s]\n",
      "Training: (loss -0.1382):  60%|██████    | 468/777 [02:52<01:42,  3.03it/s]\n",
      "Training: (loss -0.1042):  60%|██████    | 468/777 [02:52<01:42,  3.03it/s]\n",
      "Training: (loss -0.1042):  60%|██████    | 469/777 [02:52<01:38,  3.14it/s]\n",
      "Training: (loss 0.1181):  60%|██████    | 469/777 [02:53<01:38,  3.14it/s] \n",
      "Training: (loss 0.1181):  60%|██████    | 470/777 [02:53<01:39,  3.09it/s]\n",
      "Training: (loss -0.0993):  60%|██████    | 470/777 [02:53<01:39,  3.09it/s]\n",
      "Training: (loss -0.0993):  61%|██████    | 471/777 [02:53<01:45,  2.91it/s]\n",
      "Training: (loss 0.3445):  61%|██████    | 471/777 [02:54<01:45,  2.91it/s] \n",
      "Training: (loss 0.3445):  61%|██████    | 472/777 [02:54<01:51,  2.74it/s]\n",
      "Training: (loss -0.0956):  61%|██████    | 472/777 [02:54<01:51,  2.74it/s]\n",
      "Training: (loss -0.0956):  61%|██████    | 473/777 [02:54<01:52,  2.71it/s]\n",
      "Training: (loss 0.3385):  61%|██████    | 473/777 [02:54<01:52,  2.71it/s] \n",
      "Training: (loss 0.3385):  61%|██████    | 474/777 [02:54<01:54,  2.64it/s]\n",
      "Training: (loss 0.1649):  61%|██████    | 474/777 [02:55<01:54,  2.64it/s]\n",
      "Training: (loss 0.1649):  61%|██████    | 475/777 [02:55<01:54,  2.64it/s]\n",
      "Training: (loss 0.3361):  61%|██████    | 475/777 [02:55<01:54,  2.64it/s]\n",
      "Training: (loss 0.3361):  61%|██████▏   | 476/777 [02:55<01:53,  2.65it/s]\n",
      "Training: (loss 0.1791):  61%|██████▏   | 476/777 [02:55<01:53,  2.65it/s]\n",
      "Training: (loss 0.1791):  61%|██████▏   | 477/777 [02:55<01:51,  2.70it/s]\n",
      "Training: (loss 0.2568):  61%|██████▏   | 477/777 [02:56<01:51,  2.70it/s]\n",
      "Training: (loss 0.2568):  62%|██████▏   | 478/777 [02:56<01:49,  2.74it/s]\n",
      "Training: (loss 0.0225):  62%|██████▏   | 478/777 [02:56<01:49,  2.74it/s]\n",
      "Training: (loss 0.0225):  62%|██████▏   | 479/777 [02:56<01:48,  2.75it/s]\n",
      "Training: (loss 0.1486):  62%|██████▏   | 479/777 [02:56<01:48,  2.75it/s]\n",
      "Training: (loss 0.1486):  62%|██████▏   | 480/777 [02:56<01:46,  2.80it/s]\n",
      "Training: (loss 0.0499):  62%|██████▏   | 480/777 [02:57<01:46,  2.80it/s]\n",
      "Training: (loss 0.0499):  62%|██████▏   | 481/777 [02:57<01:51,  2.64it/s]\n",
      "Training: (loss -0.0499):  62%|██████▏   | 481/777 [02:57<01:51,  2.64it/s]\n",
      "Training: (loss -0.0499):  62%|██████▏   | 482/777 [02:57<01:50,  2.67it/s]\n",
      "Training: (loss 0.3393):  62%|██████▏   | 482/777 [02:58<01:50,  2.67it/s] \n",
      "Training: (loss 0.3393):  62%|██████▏   | 483/777 [02:58<01:48,  2.72it/s]\n",
      "Training: (loss 0.0647):  62%|██████▏   | 483/777 [02:58<01:48,  2.72it/s]\n",
      "Training: (loss 0.0647):  62%|██████▏   | 484/777 [02:58<01:49,  2.68it/s]\n",
      "Training: (loss 0.3353):  62%|██████▏   | 484/777 [02:58<01:49,  2.68it/s]\n",
      "Training: (loss 0.3353):  62%|██████▏   | 485/777 [02:58<01:42,  2.84it/s]\n",
      "Training: (loss -0.1396):  62%|██████▏   | 485/777 [02:59<01:42,  2.84it/s]\n",
      "Training: (loss -0.1396):  63%|██████▎   | 486/777 [02:59<01:42,  2.85it/s]\n",
      "Training: (loss 0.3490):  63%|██████▎   | 486/777 [02:59<01:42,  2.85it/s] \n",
      "Training: (loss 0.3490):  63%|██████▎   | 487/777 [02:59<01:44,  2.76it/s]\n",
      "Training: (loss -0.1769):  63%|██████▎   | 487/777 [02:59<01:44,  2.76it/s]\n",
      "Training: (loss -0.1769):  63%|██████▎   | 488/777 [02:59<01:48,  2.67it/s]\n",
      "Training: (loss 0.3389):  63%|██████▎   | 488/777 [03:00<01:48,  2.67it/s] \n",
      "Training: (loss 0.3389):  63%|██████▎   | 489/777 [03:00<01:47,  2.67it/s]\n",
      "Training: (loss -0.0397):  63%|██████▎   | 489/777 [03:00<01:47,  2.67it/s]\n",
      "Training: (loss -0.0397):  63%|██████▎   | 490/777 [03:00<01:46,  2.70it/s]\n",
      "Training: (loss 0.0087):  63%|██████▎   | 490/777 [03:01<01:46,  2.70it/s] \n",
      "Training: (loss 0.0087):  63%|██████▎   | 491/777 [03:01<01:47,  2.65it/s]\n",
      "Training: (loss -0.2477):  63%|██████▎   | 491/777 [03:01<01:47,  2.65it/s]\n",
      "Training: (loss -0.2477):  63%|██████▎   | 492/777 [03:01<01:47,  2.65it/s]\n",
      "Training: (loss -0.1173):  63%|██████▎   | 492/777 [03:01<01:47,  2.65it/s]\n",
      "Training: (loss -0.1173):  63%|██████▎   | 493/777 [03:01<01:39,  2.85it/s]\n",
      "Training: (loss -0.1664):  63%|██████▎   | 493/777 [03:02<01:39,  2.85it/s]\n",
      "Training: (loss -0.1664):  64%|██████▎   | 494/777 [03:02<01:36,  2.92it/s]\n",
      "Training: (loss 0.1879):  64%|██████▎   | 494/777 [03:02<01:36,  2.92it/s] \n",
      "Training: (loss 0.1879):  64%|██████▎   | 495/777 [03:02<01:38,  2.85it/s]\n",
      "Training: (loss 0.0758):  64%|██████▎   | 495/777 [03:02<01:38,  2.85it/s]\n",
      "Training: (loss 0.0758):  64%|██████▍   | 496/777 [03:02<01:40,  2.80it/s]\n",
      "Training: (loss -0.0710):  64%|██████▍   | 496/777 [03:03<01:40,  2.80it/s]\n",
      "Training: (loss -0.0710):  64%|██████▍   | 497/777 [03:03<01:42,  2.72it/s]\n",
      "Training: (loss 0.1568):  64%|██████▍   | 497/777 [03:03<01:42,  2.72it/s] \n",
      "Training: (loss 0.1568):  64%|██████▍   | 498/777 [03:03<01:48,  2.56it/s]\n",
      "Training: (loss -0.1700):  64%|██████▍   | 498/777 [03:04<01:48,  2.56it/s]\n",
      "Training: (loss -0.1700):  64%|██████▍   | 499/777 [03:04<01:46,  2.61it/s]\n",
      "Training: (loss -0.0907):  64%|██████▍   | 499/777 [03:04<01:46,  2.61it/s]\n",
      "Training: (loss -0.0907):  64%|██████▍   | 500/777 [03:04<01:43,  2.69it/s]\n",
      "Training: (loss 0.3340):  64%|██████▍   | 500/777 [03:04<01:43,  2.69it/s] \n",
      "Training: (loss 0.3340):  64%|██████▍   | 501/777 [03:04<01:48,  2.54it/s]\n",
      "Training: (loss -0.0617):  64%|██████▍   | 501/777 [03:05<01:48,  2.54it/s]\n",
      "Training: (loss -0.0617):  65%|██████▍   | 502/777 [03:05<01:46,  2.58it/s]\n",
      "Training: (loss 0.2592):  65%|██████▍   | 502/777 [03:05<01:46,  2.58it/s] \n",
      "Training: (loss 0.2592):  65%|██████▍   | 503/777 [03:05<01:44,  2.61it/s]\n",
      "Training: (loss -0.1484):  65%|██████▍   | 503/777 [03:05<01:44,  2.61it/s]\n",
      "Training: (loss -0.1484):  65%|██████▍   | 504/777 [03:05<01:45,  2.60it/s]\n",
      "Training: (loss -0.1306):  65%|██████▍   | 504/777 [03:06<01:45,  2.60it/s]\n",
      "Training: (loss -0.1306):  65%|██████▍   | 505/777 [03:06<01:34,  2.86it/s]\n",
      "Training: (loss -0.1814):  65%|██████▍   | 505/777 [03:06<01:34,  2.86it/s]\n",
      "Training: (loss -0.1814):  65%|██████▌   | 506/777 [03:06<01:36,  2.80it/s]\n",
      "Training: (loss 0.3213):  65%|██████▌   | 506/777 [03:06<01:36,  2.80it/s] \n",
      "Training: (loss 0.3213):  65%|██████▌   | 507/777 [03:06<01:36,  2.80it/s]\n",
      "Training: (loss -0.2321):  65%|██████▌   | 507/777 [03:07<01:36,  2.80it/s]\n",
      "Training: (loss -0.2321):  65%|██████▌   | 508/777 [03:07<01:38,  2.73it/s]\n",
      "Training: (loss -0.1035):  65%|██████▌   | 508/777 [03:07<01:38,  2.73it/s]\n",
      "Training: (loss -0.1035):  66%|██████▌   | 509/777 [03:07<01:39,  2.70it/s]\n",
      "Training: (loss 0.3437):  66%|██████▌   | 509/777 [03:08<01:39,  2.70it/s] \n",
      "Training: (loss 0.3437):  66%|██████▌   | 510/777 [03:08<01:39,  2.68it/s]\n",
      "Training: (loss -0.0050):  66%|██████▌   | 510/777 [03:08<01:39,  2.68it/s]\n",
      "Training: (loss -0.0050):  66%|██████▌   | 511/777 [03:08<01:39,  2.68it/s]\n",
      "Training: (loss 0.1659):  66%|██████▌   | 511/777 [03:08<01:39,  2.68it/s] \n",
      "Training: (loss 0.1659):  66%|██████▌   | 512/777 [03:08<01:37,  2.72it/s]\n",
      "Training: (loss -0.0198):  66%|██████▌   | 512/777 [03:09<01:37,  2.72it/s]\n",
      "Training: (loss -0.0198):  66%|██████▌   | 513/777 [03:09<01:41,  2.61it/s]\n",
      "Training: (loss -0.0381):  66%|██████▌   | 513/777 [03:09<01:41,  2.61it/s]\n",
      "Training: (loss -0.0381):  66%|██████▌   | 514/777 [03:09<01:39,  2.66it/s]\n",
      "Training: (loss 0.3012):  66%|██████▌   | 514/777 [03:09<01:39,  2.66it/s] \n",
      "Training: (loss 0.3012):  66%|██████▋   | 515/777 [03:09<01:37,  2.69it/s]\n",
      "Training: (loss 0.3282):  66%|██████▋   | 515/777 [03:10<01:37,  2.69it/s]\n",
      "Training: (loss 0.3282):  66%|██████▋   | 516/777 [03:10<01:35,  2.75it/s]\n",
      "Training: (loss 0.3303):  66%|██████▋   | 516/777 [03:10<01:35,  2.75it/s]\n",
      "Training: (loss 0.3303):  67%|██████▋   | 517/777 [03:10<01:35,  2.73it/s]\n",
      "Training: (loss -0.1571):  67%|██████▋   | 517/777 [03:11<01:35,  2.73it/s]\n",
      "Training: (loss -0.1571):  67%|██████▋   | 518/777 [03:11<01:36,  2.68it/s]\n",
      "Training: (loss 0.0474):  67%|██████▋   | 518/777 [03:11<01:36,  2.68it/s] \n",
      "Training: (loss 0.0474):  67%|██████▋   | 519/777 [03:11<01:31,  2.80it/s]\n",
      "Training: (loss -0.1136):  67%|██████▋   | 519/777 [03:11<01:31,  2.80it/s]\n",
      "Training: (loss -0.1136):  67%|██████▋   | 520/777 [03:11<01:29,  2.87it/s]\n",
      "Training: (loss -0.0956):  67%|██████▋   | 520/777 [03:12<01:29,  2.87it/s]\n",
      "Training: (loss -0.0956):  67%|██████▋   | 521/777 [03:12<01:30,  2.82it/s]\n",
      "Training: (loss 0.0428):  67%|██████▋   | 521/777 [03:12<01:30,  2.82it/s] \n",
      "Training: (loss 0.0428):  67%|██████▋   | 522/777 [03:12<01:36,  2.64it/s]\n",
      "Training: (loss 0.1571):  67%|██████▋   | 522/777 [03:12<01:36,  2.64it/s]\n",
      "Training: (loss 0.1571):  67%|██████▋   | 523/777 [03:12<01:33,  2.71it/s]\n",
      "Training: (loss -0.1310):  67%|██████▋   | 523/777 [03:13<01:33,  2.71it/s]\n",
      "Training: (loss -0.1310):  67%|██████▋   | 524/777 [03:13<01:34,  2.66it/s]\n",
      "Training: (loss -0.0391):  67%|██████▋   | 524/777 [03:13<01:34,  2.66it/s]\n",
      "Training: (loss -0.0391):  68%|██████▊   | 525/777 [03:13<01:34,  2.67it/s]\n",
      "Training: (loss 0.1307):  68%|██████▊   | 525/777 [03:13<01:34,  2.67it/s] \n",
      "Training: (loss 0.1307):  68%|██████▊   | 526/777 [03:13<01:26,  2.89it/s]\n",
      "Training: (loss -0.1798):  68%|██████▊   | 526/777 [03:14<01:26,  2.89it/s]\n",
      "Training: (loss -0.1798):  68%|██████▊   | 527/777 [03:14<01:29,  2.80it/s]\n",
      "Training: (loss 0.1420):  68%|██████▊   | 527/777 [03:14<01:29,  2.80it/s] \n",
      "Training: (loss 0.1420):  68%|██████▊   | 528/777 [03:14<01:28,  2.83it/s]\n",
      "Training: (loss 0.1009):  68%|██████▊   | 528/777 [03:15<01:28,  2.83it/s]\n",
      "Training: (loss 0.1009):  68%|██████▊   | 529/777 [03:15<01:33,  2.64it/s]\n",
      "Training: (loss -0.1135):  68%|██████▊   | 529/777 [03:15<01:33,  2.64it/s]\n",
      "Training: (loss -0.1135):  68%|██████▊   | 530/777 [03:15<01:27,  2.84it/s]\n",
      "Training: (loss 0.0679):  68%|██████▊   | 530/777 [03:15<01:27,  2.84it/s] \n",
      "Training: (loss 0.0679):  68%|██████▊   | 531/777 [03:15<01:26,  2.86it/s]\n",
      "Training: (loss 0.0191):  68%|██████▊   | 531/777 [03:15<01:26,  2.86it/s]\n",
      "Training: (loss 0.0191):  68%|██████▊   | 532/777 [03:15<01:22,  2.99it/s]\n",
      "Training: (loss -0.0264):  68%|██████▊   | 532/777 [03:16<01:22,  2.99it/s]\n",
      "Training: (loss -0.0264):  69%|██████▊   | 533/777 [03:16<01:22,  2.97it/s]\n",
      "Training: (loss 0.0249):  69%|██████▊   | 533/777 [03:16<01:22,  2.97it/s] \n",
      "Training: (loss 0.0249):  69%|██████▊   | 534/777 [03:16<01:22,  2.96it/s]\n",
      "Training: (loss 0.0415):  69%|██████▊   | 534/777 [03:17<01:22,  2.96it/s]\n",
      "Training: (loss 0.0415):  69%|██████▉   | 535/777 [03:17<01:25,  2.85it/s]\n",
      "Training: (loss -0.0866):  69%|██████▉   | 535/777 [03:17<01:25,  2.85it/s]\n",
      "Training: (loss -0.0866):  69%|██████▉   | 536/777 [03:17<01:24,  2.85it/s]\n",
      "Training: (loss -0.0893):  69%|██████▉   | 536/777 [03:17<01:24,  2.85it/s]\n",
      "Training: (loss -0.0893):  69%|██████▉   | 537/777 [03:17<01:22,  2.89it/s]\n",
      "Training: (loss -0.0882):  69%|██████▉   | 537/777 [03:18<01:22,  2.89it/s]\n",
      "Training: (loss -0.0882):  69%|██████▉   | 538/777 [03:18<01:25,  2.80it/s]\n",
      "Training: (loss 0.2503):  69%|██████▉   | 538/777 [03:18<01:25,  2.80it/s] \n",
      "Training: (loss 0.2503):  69%|██████▉   | 539/777 [03:18<01:19,  3.01it/s]\n",
      "Training: (loss -0.0599):  69%|██████▉   | 539/777 [03:18<01:19,  3.01it/s]\n",
      "Training: (loss -0.0599):  69%|██████▉   | 540/777 [03:18<01:18,  3.03it/s]\n",
      "Training: (loss 0.3217):  69%|██████▉   | 540/777 [03:19<01:18,  3.03it/s] \n",
      "Training: (loss 0.3217):  70%|██████▉   | 541/777 [03:19<01:22,  2.87it/s]\n",
      "Training: (loss 0.3193):  70%|██████▉   | 541/777 [03:19<01:22,  2.87it/s]\n",
      "Training: (loss 0.3193):  70%|██████▉   | 542/777 [03:19<01:26,  2.72it/s]\n",
      "Training: (loss -0.1223):  70%|██████▉   | 542/777 [03:19<01:26,  2.72it/s]\n",
      "Training: (loss -0.1223):  70%|██████▉   | 543/777 [03:19<01:23,  2.79it/s]\n",
      "Training: (loss -0.2249):  70%|██████▉   | 543/777 [03:20<01:23,  2.79it/s]\n",
      "Training: (loss -0.2249):  70%|███████   | 544/777 [03:20<01:21,  2.84it/s]\n",
      "Training: (loss -0.1798):  70%|███████   | 544/777 [03:20<01:21,  2.84it/s]\n",
      "Training: (loss -0.1798):  70%|███████   | 545/777 [03:20<01:23,  2.79it/s]\n",
      "Training: (loss -0.1521):  70%|███████   | 545/777 [03:20<01:23,  2.79it/s]\n",
      "Training: (loss -0.1521):  70%|███████   | 546/777 [03:20<01:19,  2.90it/s]\n",
      "Training: (loss -0.1915):  70%|███████   | 546/777 [03:21<01:19,  2.90it/s]\n",
      "Training: (loss -0.1915):  70%|███████   | 547/777 [03:21<01:19,  2.90it/s]\n",
      "Training: (loss -0.1296):  70%|███████   | 547/777 [03:21<01:19,  2.90it/s]\n",
      "Training: (loss -0.1296):  71%|███████   | 548/777 [03:21<01:22,  2.78it/s]\n",
      "Training: (loss -0.0790):  71%|███████   | 548/777 [03:22<01:22,  2.78it/s]\n",
      "Training: (loss -0.0790):  71%|███████   | 549/777 [03:22<01:24,  2.70it/s]\n",
      "Training: (loss 0.0137):  71%|███████   | 549/777 [03:22<01:24,  2.70it/s] \n",
      "Training: (loss 0.0137):  71%|███████   | 550/777 [03:22<01:25,  2.66it/s]\n",
      "Training: (loss 0.1897):  71%|███████   | 550/777 [03:22<01:25,  2.66it/s]\n",
      "Training: (loss 0.1897):  71%|███████   | 551/777 [03:22<01:27,  2.58it/s]\n",
      "Training: (loss 0.0110):  71%|███████   | 551/777 [03:23<01:27,  2.58it/s]\n",
      "Training: (loss 0.0110):  71%|███████   | 552/777 [03:23<01:24,  2.65it/s]\n",
      "Training: (loss -0.1246):  71%|███████   | 552/777 [03:23<01:24,  2.65it/s]\n",
      "Training: (loss -0.1246):  71%|███████   | 553/777 [03:23<01:25,  2.62it/s]\n",
      "Training: (loss -0.0846):  71%|███████   | 553/777 [03:23<01:25,  2.62it/s]\n",
      "Training: (loss -0.0846):  71%|███████▏  | 554/777 [03:23<01:23,  2.68it/s]\n",
      "Training: (loss 0.3229):  71%|███████▏  | 554/777 [03:24<01:23,  2.68it/s] \n",
      "Training: (loss 0.3229):  71%|███████▏  | 555/777 [03:24<01:20,  2.76it/s]\n",
      "Training: (loss -0.0251):  71%|███████▏  | 555/777 [03:24<01:20,  2.76it/s]\n",
      "Training: (loss -0.0251):  72%|███████▏  | 556/777 [03:24<01:17,  2.87it/s]\n",
      "Training: (loss 0.0421):  72%|███████▏  | 556/777 [03:24<01:17,  2.87it/s] \n",
      "Training: (loss 0.0421):  72%|███████▏  | 557/777 [03:24<01:21,  2.71it/s]\n",
      "Training: (loss -0.0150):  72%|███████▏  | 557/777 [03:25<01:21,  2.71it/s]\n",
      "Training: (loss -0.0150):  72%|███████▏  | 558/777 [03:25<01:21,  2.69it/s]\n",
      "Training: (loss 0.0128):  72%|███████▏  | 558/777 [03:25<01:21,  2.69it/s] \n",
      "Training: (loss 0.0128):  72%|███████▏  | 559/777 [03:25<01:22,  2.65it/s]\n",
      "Training: (loss -0.0298):  72%|███████▏  | 559/777 [03:26<01:22,  2.65it/s]\n",
      "Training: (loss -0.0298):  72%|███████▏  | 560/777 [03:26<01:25,  2.54it/s]\n",
      "Training: (loss 0.3340):  72%|███████▏  | 560/777 [03:26<01:25,  2.54it/s] \n",
      "Training: (loss 0.3340):  72%|███████▏  | 561/777 [03:26<01:19,  2.72it/s]\n",
      "Training: (loss 0.3339):  72%|███████▏  | 561/777 [03:26<01:19,  2.72it/s]\n",
      "Training: (loss 0.3339):  72%|███████▏  | 562/777 [03:26<01:15,  2.87it/s]\n",
      "Training: (loss 0.1611):  72%|███████▏  | 562/777 [03:27<01:15,  2.87it/s]\n",
      "Training: (loss 0.1611):  72%|███████▏  | 563/777 [03:27<01:17,  2.77it/s]\n",
      "Training: (loss 0.1345):  72%|███████▏  | 563/777 [03:27<01:17,  2.77it/s]\n",
      "Training: (loss 0.1345):  73%|███████▎  | 564/777 [03:27<01:13,  2.89it/s]\n",
      "Training: (loss -0.1167):  73%|███████▎  | 564/777 [03:27<01:13,  2.89it/s]\n",
      "Training: (loss -0.1167):  73%|███████▎  | 565/777 [03:27<01:11,  2.98it/s]\n",
      "Training: (loss -0.0396):  73%|███████▎  | 565/777 [03:28<01:11,  2.98it/s]\n",
      "Training: (loss -0.0396):  73%|███████▎  | 566/777 [03:28<01:11,  2.96it/s]\n",
      "Training: (loss -0.0946):  73%|███████▎  | 566/777 [03:28<01:11,  2.96it/s]\n",
      "Training: (loss -0.0946):  73%|███████▎  | 567/777 [03:28<01:08,  3.07it/s]\n",
      "Training: (loss 0.0656):  73%|███████▎  | 567/777 [03:28<01:08,  3.07it/s] \n",
      "Training: (loss 0.0656):  73%|███████▎  | 568/777 [03:28<01:14,  2.80it/s]\n",
      "Training: (loss 0.0842):  73%|███████▎  | 568/777 [03:29<01:14,  2.80it/s]\n",
      "Training: (loss 0.0842):  73%|███████▎  | 569/777 [03:29<01:17,  2.67it/s]\n",
      "Training: (loss -0.1096):  73%|███████▎  | 569/777 [03:29<01:17,  2.67it/s]\n",
      "Training: (loss -0.1096):  73%|███████▎  | 570/777 [03:29<01:18,  2.63it/s]\n",
      "Training: (loss 0.1496):  73%|███████▎  | 570/777 [03:30<01:18,  2.63it/s] \n",
      "Training: (loss 0.1496):  73%|███████▎  | 571/777 [03:30<01:18,  2.62it/s]\n",
      "Training: (loss 0.0209):  73%|███████▎  | 571/777 [03:30<01:18,  2.62it/s]\n",
      "Training: (loss 0.0209):  74%|███████▎  | 572/777 [03:30<01:21,  2.51it/s]\n",
      "Training: (loss -0.1182):  74%|███████▎  | 572/777 [03:30<01:21,  2.51it/s]\n",
      "Training: (loss -0.1182):  74%|███████▎  | 573/777 [03:30<01:24,  2.42it/s]\n",
      "Training: (loss -0.1985):  74%|███████▎  | 573/777 [03:31<01:24,  2.42it/s]\n",
      "Training: (loss -0.1985):  74%|███████▍  | 574/777 [03:31<01:24,  2.39it/s]\n",
      "Training: (loss 0.0811):  74%|███████▍  | 574/777 [03:31<01:24,  2.39it/s] \n",
      "Training: (loss 0.0811):  74%|███████▍  | 575/777 [03:31<01:23,  2.43it/s]\n",
      "Training: (loss 0.2384):  74%|███████▍  | 575/777 [03:32<01:23,  2.43it/s]\n",
      "Training: (loss 0.2384):  74%|███████▍  | 576/777 [03:32<01:22,  2.43it/s]\n",
      "Training: (loss 0.1198):  74%|███████▍  | 576/777 [03:32<01:22,  2.43it/s]\n",
      "Training: (loss 0.1198):  74%|███████▍  | 577/777 [03:32<01:17,  2.59it/s]\n",
      "Training: (loss -0.1974):  74%|███████▍  | 577/777 [03:32<01:17,  2.59it/s]\n",
      "Training: (loss -0.1974):  74%|███████▍  | 578/777 [03:32<01:13,  2.70it/s]\n",
      "Training: (loss 0.3033):  74%|███████▍  | 578/777 [03:33<01:13,  2.70it/s] \n",
      "Training: (loss 0.3033):  75%|███████▍  | 579/777 [03:33<01:14,  2.65it/s]\n",
      "Training: (loss 0.3410):  75%|███████▍  | 579/777 [03:33<01:14,  2.65it/s]\n",
      "Training: (loss 0.3410):  75%|███████▍  | 580/777 [03:33<01:11,  2.74it/s]\n",
      "Training: (loss 0.3382):  75%|███████▍  | 580/777 [03:33<01:11,  2.74it/s]\n",
      "Training: (loss 0.3382):  75%|███████▍  | 581/777 [03:33<01:13,  2.67it/s]\n",
      "Training: (loss 0.3354):  75%|███████▍  | 581/777 [03:34<01:13,  2.67it/s]\n",
      "Training: (loss 0.3354):  75%|███████▍  | 582/777 [03:34<01:11,  2.72it/s]\n",
      "Training: (loss -0.0066):  75%|███████▍  | 582/777 [03:34<01:11,  2.72it/s]\n",
      "Training: (loss -0.0066):  75%|███████▌  | 583/777 [03:34<01:08,  2.84it/s]\n",
      "Training: (loss 0.3318):  75%|███████▌  | 583/777 [03:35<01:08,  2.84it/s] \n",
      "Training: (loss 0.3318):  75%|███████▌  | 584/777 [03:35<01:09,  2.77it/s]\n",
      "Training: (loss -0.0020):  75%|███████▌  | 584/777 [03:35<01:09,  2.77it/s]\n",
      "Training: (loss -0.0020):  75%|███████▌  | 585/777 [03:35<01:09,  2.75it/s]\n",
      "Training: (loss 0.0483):  75%|███████▌  | 585/777 [03:35<01:09,  2.75it/s] \n",
      "Training: (loss 0.0483):  75%|███████▌  | 586/777 [03:35<01:09,  2.75it/s]\n",
      "Training: (loss 0.0427):  75%|███████▌  | 586/777 [03:36<01:09,  2.75it/s]\n",
      "Training: (loss 0.0427):  76%|███████▌  | 587/777 [03:36<01:11,  2.65it/s]\n",
      "Training: (loss -0.0765):  76%|███████▌  | 587/777 [03:36<01:11,  2.65it/s]\n",
      "Training: (loss -0.0765):  76%|███████▌  | 588/777 [03:36<01:14,  2.53it/s]\n",
      "Training: (loss 0.2929):  76%|███████▌  | 588/777 [03:36<01:14,  2.53it/s] \n",
      "Training: (loss 0.2929):  76%|███████▌  | 589/777 [03:36<01:11,  2.65it/s]\n",
      "Training: (loss -0.1561):  76%|███████▌  | 589/777 [03:37<01:11,  2.65it/s]\n",
      "Training: (loss -0.1561):  76%|███████▌  | 590/777 [03:37<01:10,  2.66it/s]\n",
      "Training: (loss 0.0449):  76%|███████▌  | 590/777 [03:37<01:10,  2.66it/s] \n",
      "Training: (loss 0.0449):  76%|███████▌  | 591/777 [03:37<01:08,  2.72it/s]\n",
      "Training: (loss 0.3328):  76%|███████▌  | 591/777 [03:37<01:08,  2.72it/s]\n",
      "Training: (loss 0.3328):  76%|███████▌  | 592/777 [03:37<01:04,  2.86it/s]\n",
      "Training: (loss 0.3299):  76%|███████▌  | 592/777 [03:38<01:04,  2.86it/s]\n",
      "Training: (loss 0.3299):  76%|███████▋  | 593/777 [03:38<01:04,  2.84it/s]\n",
      "Training: (loss 0.3408):  76%|███████▋  | 593/777 [03:38<01:04,  2.84it/s]\n",
      "Training: (loss 0.3408):  76%|███████▋  | 594/777 [03:38<01:04,  2.84it/s]\n",
      "Training: (loss 0.3354):  76%|███████▋  | 594/777 [03:39<01:04,  2.84it/s]\n",
      "Training: (loss 0.3354):  77%|███████▋  | 595/777 [03:39<01:06,  2.75it/s]\n",
      "Training: (loss -0.0561):  77%|███████▋  | 595/777 [03:39<01:06,  2.75it/s]\n",
      "Training: (loss -0.0561):  77%|███████▋  | 596/777 [03:39<01:08,  2.63it/s]\n",
      "Training: (loss -0.1342):  77%|███████▋  | 596/777 [03:39<01:08,  2.63it/s]\n",
      "Training: (loss -0.1342):  77%|███████▋  | 597/777 [03:39<01:04,  2.78it/s]\n",
      "Training: (loss -0.1795):  77%|███████▋  | 597/777 [03:40<01:04,  2.78it/s]\n",
      "Training: (loss -0.1795):  77%|███████▋  | 598/777 [03:40<01:03,  2.81it/s]\n",
      "Training: (loss -0.0292):  77%|███████▋  | 598/777 [03:40<01:03,  2.81it/s]\n",
      "Training: (loss -0.0292):  77%|███████▋  | 599/777 [03:40<01:03,  2.82it/s]\n",
      "Training: (loss 0.3330):  77%|███████▋  | 599/777 [03:40<01:03,  2.82it/s] \n",
      "Training: (loss 0.3330):  77%|███████▋  | 600/777 [03:40<01:05,  2.69it/s]\n",
      "Training: (loss 0.2812):  77%|███████▋  | 600/777 [03:41<01:05,  2.69it/s]\n",
      "Training: (loss 0.2812):  77%|███████▋  | 601/777 [03:41<01:02,  2.82it/s]\n",
      "Training: (loss 0.3367):  77%|███████▋  | 601/777 [03:41<01:02,  2.82it/s]\n",
      "Training: (loss 0.3367):  77%|███████▋  | 602/777 [03:41<01:02,  2.80it/s]\n",
      "Training: (loss -0.1676):  77%|███████▋  | 602/777 [03:41<01:02,  2.80it/s]\n",
      "Training: (loss -0.1676):  78%|███████▊  | 603/777 [03:41<01:01,  2.83it/s]\n",
      "Training: (loss 0.2066):  78%|███████▊  | 603/777 [03:42<01:01,  2.83it/s] \n",
      "Training: (loss 0.2066):  78%|███████▊  | 604/777 [03:42<01:02,  2.77it/s]\n",
      "Training: (loss 0.1077):  78%|███████▊  | 604/777 [03:42<01:02,  2.77it/s]\n",
      "Training: (loss 0.1077):  78%|███████▊  | 605/777 [03:42<01:04,  2.66it/s]\n",
      "Training: (loss -0.1987):  78%|███████▊  | 605/777 [03:43<01:04,  2.66it/s]\n",
      "Training: (loss -0.1987):  78%|███████▊  | 606/777 [03:43<01:03,  2.68it/s]\n",
      "Training: (loss 0.3313):  78%|███████▊  | 606/777 [03:43<01:03,  2.68it/s] \n",
      "Training: (loss 0.3313):  78%|███████▊  | 607/777 [03:43<00:59,  2.85it/s]\n",
      "Training: (loss 0.0191):  78%|███████▊  | 607/777 [03:43<00:59,  2.85it/s]\n",
      "Training: (loss 0.0191):  78%|███████▊  | 608/777 [03:43<00:56,  2.99it/s]\n",
      "Training: (loss -0.1236):  78%|███████▊  | 608/777 [03:44<00:56,  2.99it/s]\n",
      "Training: (loss -0.1236):  78%|███████▊  | 609/777 [03:44<00:57,  2.94it/s]\n",
      "Training: (loss -0.0345):  78%|███████▊  | 609/777 [03:44<00:57,  2.94it/s]\n",
      "Training: (loss -0.0345):  79%|███████▊  | 610/777 [03:44<00:58,  2.86it/s]\n",
      "Training: (loss -0.1200):  79%|███████▊  | 610/777 [03:44<00:58,  2.86it/s]\n",
      "Training: (loss -0.1200):  79%|███████▊  | 611/777 [03:44<00:59,  2.80it/s]\n",
      "Training: (loss 0.0601):  79%|███████▊  | 611/777 [03:45<00:59,  2.80it/s] \n",
      "Training: (loss 0.0601):  79%|███████▉  | 612/777 [03:45<00:57,  2.87it/s]\n",
      "Training: (loss -0.1104):  79%|███████▉  | 612/777 [03:45<00:57,  2.87it/s]\n",
      "Training: (loss -0.1104):  79%|███████▉  | 613/777 [03:45<00:59,  2.74it/s]\n",
      "Training: (loss 0.3207):  79%|███████▉  | 613/777 [03:45<00:59,  2.74it/s] \n",
      "Training: (loss 0.3207):  79%|███████▉  | 614/777 [03:45<00:58,  2.79it/s]\n",
      "Training: (loss 0.3308):  79%|███████▉  | 614/777 [03:46<00:58,  2.79it/s]\n",
      "Training: (loss 0.3308):  79%|███████▉  | 615/777 [03:46<00:59,  2.71it/s]\n",
      "Training: (loss -0.0099):  79%|███████▉  | 615/777 [03:46<00:59,  2.71it/s]\n",
      "Training: (loss -0.0099):  79%|███████▉  | 616/777 [03:46<01:01,  2.61it/s]\n",
      "Training: (loss 0.3323):  79%|███████▉  | 616/777 [03:47<01:01,  2.61it/s] \n",
      "Training: (loss 0.3323):  79%|███████▉  | 617/777 [03:47<01:00,  2.63it/s]\n",
      "Training: (loss -0.1511):  79%|███████▉  | 617/777 [03:47<01:00,  2.63it/s]\n",
      "Training: (loss -0.1511):  80%|███████▉  | 618/777 [03:47<00:59,  2.69it/s]\n",
      "Training: (loss -0.0786):  80%|███████▉  | 618/777 [03:47<00:59,  2.69it/s]\n",
      "Training: (loss -0.0786):  80%|███████▉  | 619/777 [03:47<00:58,  2.68it/s]\n",
      "Training: (loss -0.1713):  80%|███████▉  | 619/777 [03:48<00:58,  2.68it/s]\n",
      "Training: (loss -0.1713):  80%|███████▉  | 620/777 [03:48<01:00,  2.61it/s]\n",
      "Training: (loss 0.1187):  80%|███████▉  | 620/777 [03:48<01:00,  2.61it/s] \n",
      "Training: (loss 0.1187):  80%|███████▉  | 621/777 [03:48<01:01,  2.55it/s]\n",
      "Training: (loss 0.0427):  80%|███████▉  | 621/777 [03:49<01:01,  2.55it/s]\n",
      "Training: (loss 0.0427):  80%|████████  | 622/777 [03:49<01:02,  2.49it/s]\n",
      "Training: (loss 0.0869):  80%|████████  | 622/777 [03:49<01:02,  2.49it/s]\n",
      "Training: (loss 0.0869):  80%|████████  | 623/777 [03:49<01:03,  2.44it/s]\n",
      "Training: (loss -0.1396):  80%|████████  | 623/777 [03:49<01:03,  2.44it/s]\n",
      "Training: (loss -0.1396):  80%|████████  | 624/777 [03:49<01:02,  2.46it/s]\n",
      "Training: (loss -0.0506):  80%|████████  | 624/777 [03:50<01:02,  2.46it/s]\n",
      "Training: (loss -0.0506):  80%|████████  | 625/777 [03:50<00:59,  2.56it/s]\n",
      "Training: (loss 0.2193):  80%|████████  | 625/777 [03:50<00:59,  2.56it/s] \n",
      "Training: (loss 0.2193):  81%|████████  | 626/777 [03:50<00:59,  2.55it/s]\n",
      "Training: (loss 0.2888):  81%|████████  | 626/777 [03:50<00:59,  2.55it/s]\n",
      "Training: (loss 0.2888):  81%|████████  | 627/777 [03:50<00:57,  2.60it/s]\n",
      "Training: (loss 0.0547):  81%|████████  | 627/777 [03:51<00:57,  2.60it/s]\n",
      "Training: (loss 0.0547):  81%|████████  | 628/777 [03:51<00:54,  2.74it/s]\n",
      "Training: (loss 0.1368):  81%|████████  | 628/777 [03:51<00:54,  2.74it/s]\n",
      "Training: (loss 0.1368):  81%|████████  | 629/777 [03:51<00:53,  2.77it/s]\n",
      "Training: (loss -0.0934):  81%|████████  | 629/777 [03:52<00:53,  2.77it/s]\n",
      "Training: (loss -0.0934):  81%|████████  | 630/777 [03:52<00:56,  2.60it/s]\n",
      "Training: (loss 0.0884):  81%|████████  | 630/777 [03:52<00:56,  2.60it/s] \n",
      "Training: (loss 0.0884):  81%|████████  | 631/777 [03:52<00:56,  2.57it/s]\n",
      "Training: (loss -0.0249):  81%|████████  | 631/777 [03:52<00:56,  2.57it/s]\n",
      "Training: (loss -0.0249):  81%|████████▏ | 632/777 [03:52<00:56,  2.57it/s]\n",
      "Training: (loss -0.1304):  81%|████████▏ | 632/777 [03:53<00:56,  2.57it/s]\n",
      "Training: (loss -0.1304):  81%|████████▏ | 633/777 [03:53<00:56,  2.53it/s]\n",
      "Training: (loss -0.2021):  81%|████████▏ | 633/777 [03:53<00:56,  2.53it/s]\n",
      "Training: (loss -0.2021):  82%|████████▏ | 634/777 [03:53<00:56,  2.53it/s]\n",
      "Training: (loss 0.3274):  82%|████████▏ | 634/777 [03:54<00:56,  2.53it/s] \n",
      "Training: (loss 0.3274):  82%|████████▏ | 635/777 [03:54<00:56,  2.53it/s]\n",
      "Training: (loss -0.0540):  82%|████████▏ | 635/777 [03:54<00:56,  2.53it/s]\n",
      "Training: (loss -0.0540):  82%|████████▏ | 636/777 [03:54<00:52,  2.67it/s]\n",
      "Training: (loss 0.0687):  82%|████████▏ | 636/777 [03:54<00:52,  2.67it/s] \n",
      "Training: (loss 0.0687):  82%|████████▏ | 637/777 [03:54<00:53,  2.62it/s]\n",
      "Training: (loss 0.0735):  82%|████████▏ | 637/777 [03:55<00:53,  2.62it/s]\n",
      "Training: (loss 0.0735):  82%|████████▏ | 638/777 [03:55<00:54,  2.56it/s]\n",
      "Training: (loss -0.1048):  82%|████████▏ | 638/777 [03:55<00:54,  2.56it/s]\n",
      "Training: (loss -0.1048):  82%|████████▏ | 639/777 [03:55<00:52,  2.61it/s]\n",
      "Training: (loss 0.0028):  82%|████████▏ | 639/777 [03:55<00:52,  2.61it/s] \n",
      "Training: (loss 0.0028):  82%|████████▏ | 640/777 [03:55<00:52,  2.63it/s]\n",
      "Training: (loss 0.1008):  82%|████████▏ | 640/777 [03:56<00:52,  2.63it/s]\n",
      "Training: (loss 0.1008):  82%|████████▏ | 641/777 [03:56<00:49,  2.72it/s]\n",
      "Training: (loss 0.3230):  82%|████████▏ | 641/777 [03:56<00:49,  2.72it/s]\n",
      "Training: (loss 0.3230):  83%|████████▎ | 642/777 [03:56<00:48,  2.81it/s]\n",
      "Training: (loss 0.2126):  83%|████████▎ | 642/777 [03:57<00:48,  2.81it/s]\n",
      "Training: (loss 0.2126):  83%|████████▎ | 643/777 [03:57<00:50,  2.67it/s]\n",
      "Training: (loss 0.0164):  83%|████████▎ | 643/777 [03:57<00:50,  2.67it/s]\n",
      "Training: (loss 0.0164):  83%|████████▎ | 644/777 [03:57<00:47,  2.77it/s]\n",
      "Training: (loss 0.3290):  83%|████████▎ | 644/777 [03:57<00:47,  2.77it/s]\n",
      "Training: (loss 0.3290):  83%|████████▎ | 645/777 [03:57<00:50,  2.62it/s]\n",
      "Training: (loss -0.1101):  83%|████████▎ | 645/777 [03:58<00:50,  2.62it/s]\n",
      "Training: (loss -0.1101):  83%|████████▎ | 646/777 [03:58<00:50,  2.60it/s]\n",
      "Training: (loss 0.2532):  83%|████████▎ | 646/777 [03:58<00:50,  2.60it/s] \n",
      "Training: (loss 0.2532):  83%|████████▎ | 647/777 [03:58<00:51,  2.53it/s]\n",
      "Training: (loss -0.0874):  83%|████████▎ | 647/777 [03:58<00:51,  2.53it/s]\n",
      "Training: (loss -0.0874):  83%|████████▎ | 648/777 [03:58<00:50,  2.55it/s]\n",
      "Training: (loss -0.0087):  83%|████████▎ | 648/777 [03:59<00:50,  2.55it/s]\n",
      "Training: (loss -0.0087):  84%|████████▎ | 649/777 [03:59<00:52,  2.45it/s]\n",
      "Training: (loss -0.1091):  84%|████████▎ | 649/777 [03:59<00:52,  2.45it/s]\n",
      "Training: (loss -0.1091):  84%|████████▎ | 650/777 [03:59<00:51,  2.47it/s]\n",
      "Training: (loss -0.0879):  84%|████████▎ | 650/777 [04:00<00:51,  2.47it/s]\n",
      "Training: (loss -0.0879):  84%|████████▍ | 651/777 [04:00<00:52,  2.41it/s]\n",
      "Training: (loss 0.0543):  84%|████████▍ | 651/777 [04:00<00:52,  2.41it/s] \n",
      "Training: (loss 0.0543):  84%|████████▍ | 652/777 [04:00<00:51,  2.42it/s]\n",
      "Training: (loss -0.0151):  84%|████████▍ | 652/777 [04:01<00:51,  2.42it/s]\n",
      "Training: (loss -0.0151):  84%|████████▍ | 653/777 [04:01<00:51,  2.40it/s]\n",
      "Training: (loss -0.0918):  84%|████████▍ | 653/777 [04:01<00:51,  2.40it/s]\n",
      "Training: (loss -0.0918):  84%|████████▍ | 654/777 [04:01<00:50,  2.44it/s]\n",
      "Training: (loss -0.0792):  84%|████████▍ | 654/777 [04:01<00:50,  2.44it/s]\n",
      "Training: (loss -0.0792):  84%|████████▍ | 655/777 [04:01<00:50,  2.44it/s]\n",
      "Training: (loss 0.0421):  84%|████████▍ | 655/777 [04:02<00:50,  2.44it/s] \n",
      "Training: (loss 0.0421):  84%|████████▍ | 656/777 [04:02<00:48,  2.51it/s]\n",
      "Training: (loss -0.1163):  84%|████████▍ | 656/777 [04:02<00:48,  2.51it/s]\n",
      "Training: (loss -0.1163):  85%|████████▍ | 657/777 [04:02<00:45,  2.65it/s]\n",
      "Training: (loss -0.2320):  85%|████████▍ | 657/777 [04:03<00:45,  2.65it/s]\n",
      "Training: (loss -0.2320):  85%|████████▍ | 658/777 [04:03<00:46,  2.54it/s]\n",
      "Training: (loss 0.3283):  85%|████████▍ | 658/777 [04:03<00:46,  2.54it/s] \n",
      "Training: (loss 0.3283):  85%|████████▍ | 659/777 [04:03<00:46,  2.53it/s]\n",
      "Training: (loss 0.2283):  85%|████████▍ | 659/777 [04:03<00:46,  2.53it/s]\n",
      "Training: (loss 0.2283):  85%|████████▍ | 660/777 [04:03<00:47,  2.49it/s]\n",
      "Training: (loss 0.0320):  85%|████████▍ | 660/777 [04:04<00:47,  2.49it/s]\n",
      "Training: (loss 0.0320):  85%|████████▌ | 661/777 [04:04<00:46,  2.48it/s]\n",
      "Training: (loss 0.0648):  85%|████████▌ | 661/777 [04:04<00:46,  2.48it/s]\n",
      "Training: (loss 0.0648):  85%|████████▌ | 662/777 [04:04<00:46,  2.46it/s]\n",
      "Training: (loss 0.3278):  85%|████████▌ | 662/777 [04:05<00:46,  2.46it/s]\n",
      "Training: (loss 0.3278):  85%|████████▌ | 663/777 [04:05<00:45,  2.48it/s]\n",
      "Training: (loss -0.0237):  85%|████████▌ | 663/777 [04:05<00:45,  2.48it/s]\n",
      "Training: (loss -0.0237):  85%|████████▌ | 664/777 [04:05<00:46,  2.43it/s]\n",
      "Training: (loss -0.0599):  85%|████████▌ | 664/777 [04:05<00:46,  2.43it/s]\n",
      "Training: (loss -0.0599):  86%|████████▌ | 665/777 [04:05<00:47,  2.38it/s]\n",
      "Training: (loss 0.1217):  86%|████████▌ | 665/777 [04:06<00:47,  2.38it/s] \n",
      "Training: (loss 0.1217):  86%|████████▌ | 666/777 [04:06<00:44,  2.50it/s]\n",
      "Training: (loss -0.1637):  86%|████████▌ | 666/777 [04:06<00:44,  2.50it/s]\n",
      "Training: (loss -0.1637):  86%|████████▌ | 667/777 [04:06<00:43,  2.50it/s]\n",
      "Training: (loss -0.0283):  86%|████████▌ | 667/777 [04:07<00:43,  2.50it/s]\n",
      "Training: (loss -0.0283):  86%|████████▌ | 668/777 [04:07<00:43,  2.50it/s]\n",
      "Training: (loss 0.0797):  86%|████████▌ | 668/777 [04:07<00:43,  2.50it/s] \n",
      "Training: (loss 0.0797):  86%|████████▌ | 669/777 [04:07<00:42,  2.53it/s]\n",
      "Training: (loss -0.1040):  86%|████████▌ | 669/777 [04:07<00:42,  2.53it/s]\n",
      "Training: (loss -0.1040):  86%|████████▌ | 670/777 [04:07<00:43,  2.47it/s]\n",
      "Training: (loss 0.3306):  86%|████████▌ | 670/777 [04:08<00:43,  2.47it/s] \n",
      "Training: (loss 0.3306):  86%|████████▋ | 671/777 [04:08<00:40,  2.59it/s]\n",
      "Training: (loss -0.1595):  86%|████████▋ | 671/777 [04:08<00:40,  2.59it/s]\n",
      "Training: (loss -0.1595):  86%|████████▋ | 672/777 [04:08<00:40,  2.58it/s]\n",
      "Training: (loss -0.0795):  86%|████████▋ | 672/777 [04:09<00:40,  2.58it/s]\n",
      "Training: (loss -0.0795):  87%|████████▋ | 673/777 [04:09<00:39,  2.61it/s]\n",
      "Training: (loss -0.1818):  87%|████████▋ | 673/777 [04:09<00:39,  2.61it/s]\n",
      "Training: (loss -0.1818):  87%|████████▋ | 674/777 [04:09<00:39,  2.63it/s]\n",
      "Training: (loss -0.1151):  87%|████████▋ | 674/777 [04:09<00:39,  2.63it/s]\n",
      "Training: (loss -0.1151):  87%|████████▋ | 675/777 [04:09<00:39,  2.58it/s]\n",
      "Training: (loss -0.1432):  87%|████████▋ | 675/777 [04:10<00:39,  2.58it/s]\n",
      "Training: (loss -0.1432):  87%|████████▋ | 676/777 [04:10<00:35,  2.85it/s]\n",
      "Training: (loss 0.2369):  87%|████████▋ | 676/777 [04:10<00:35,  2.85it/s] \n",
      "Training: (loss 0.2369):  87%|████████▋ | 677/777 [04:10<00:36,  2.74it/s]\n",
      "Training: (loss -0.1939):  87%|████████▋ | 677/777 [04:10<00:36,  2.74it/s]\n",
      "Training: (loss -0.1939):  87%|████████▋ | 678/777 [04:10<00:36,  2.73it/s]\n",
      "Training: (loss 0.1152):  87%|████████▋ | 678/777 [04:11<00:36,  2.73it/s] \n",
      "Training: (loss 0.1152):  87%|████████▋ | 679/777 [04:11<00:34,  2.82it/s]\n",
      "Training: (loss 0.0165):  87%|████████▋ | 679/777 [04:11<00:34,  2.82it/s]\n",
      "Training: (loss 0.0165):  88%|████████▊ | 680/777 [04:11<00:33,  2.90it/s]\n",
      "Training: (loss 0.2433):  88%|████████▊ | 680/777 [04:11<00:33,  2.90it/s]\n",
      "Training: (loss 0.2433):  88%|████████▊ | 681/777 [04:11<00:31,  3.01it/s]\n",
      "Training: (loss -0.1012):  88%|████████▊ | 681/777 [04:12<00:31,  3.01it/s]\n",
      "Training: (loss -0.1012):  88%|████████▊ | 682/777 [04:12<00:33,  2.87it/s]\n",
      "Training: (loss -0.0716):  88%|████████▊ | 682/777 [04:12<00:33,  2.87it/s]\n",
      "Training: (loss -0.0716):  88%|████████▊ | 683/777 [04:12<00:31,  2.94it/s]\n",
      "Training: (loss 0.0068):  88%|████████▊ | 683/777 [04:12<00:31,  2.94it/s] \n",
      "Training: (loss 0.0068):  88%|████████▊ | 684/777 [04:12<00:31,  2.95it/s]\n",
      "Training: (loss 0.0711):  88%|████████▊ | 684/777 [04:13<00:31,  2.95it/s]\n",
      "Training: (loss 0.0711):  88%|████████▊ | 685/777 [04:13<00:32,  2.82it/s]\n",
      "Training: (loss 0.3315):  88%|████████▊ | 685/777 [04:13<00:32,  2.82it/s]\n",
      "Training: (loss 0.3315):  88%|████████▊ | 686/777 [04:13<00:33,  2.75it/s]\n",
      "Training: (loss 0.3064):  88%|████████▊ | 686/777 [04:13<00:33,  2.75it/s]\n",
      "Training: (loss 0.3064):  88%|████████▊ | 687/777 [04:13<00:33,  2.68it/s]\n",
      "Training: (loss 0.0493):  88%|████████▊ | 687/777 [04:14<00:33,  2.68it/s]\n",
      "Training: (loss 0.0493):  89%|████████▊ | 688/777 [04:14<00:32,  2.73it/s]\n",
      "Training: (loss -0.1108):  89%|████████▊ | 688/777 [04:14<00:32,  2.73it/s]\n",
      "Training: (loss -0.1108):  89%|████████▊ | 689/777 [04:14<00:33,  2.59it/s]\n",
      "Training: (loss 0.2350):  89%|████████▊ | 689/777 [04:15<00:33,  2.59it/s] \n",
      "Training: (loss 0.2350):  89%|████████▉ | 690/777 [04:15<00:33,  2.63it/s]\n",
      "Training: (loss 0.0524):  89%|████████▉ | 690/777 [04:15<00:33,  2.63it/s]\n",
      "Training: (loss 0.0524):  89%|████████▉ | 691/777 [04:15<00:31,  2.72it/s]\n",
      "Training: (loss -0.0664):  89%|████████▉ | 691/777 [04:15<00:31,  2.72it/s]\n",
      "Training: (loss -0.0664):  89%|████████▉ | 692/777 [04:15<00:30,  2.83it/s]\n",
      "Training: (loss 0.3391):  89%|████████▉ | 692/777 [04:16<00:30,  2.83it/s] \n",
      "Training: (loss 0.3391):  89%|████████▉ | 693/777 [04:16<00:30,  2.76it/s]\n",
      "Training: (loss 0.3057):  89%|████████▉ | 693/777 [04:16<00:30,  2.76it/s]\n",
      "Training: (loss 0.3057):  89%|████████▉ | 694/777 [04:16<00:29,  2.85it/s]\n",
      "Training: (loss 0.0699):  89%|████████▉ | 694/777 [04:16<00:29,  2.85it/s]\n",
      "Training: (loss 0.0699):  89%|████████▉ | 695/777 [04:16<00:30,  2.68it/s]\n",
      "Training: (loss -0.0222):  89%|████████▉ | 695/777 [04:17<00:30,  2.68it/s]\n",
      "Training: (loss -0.0222):  90%|████████▉ | 696/777 [04:17<00:30,  2.68it/s]\n",
      "Training: (loss 0.1619):  90%|████████▉ | 696/777 [04:17<00:30,  2.68it/s] \n",
      "Training: (loss 0.1619):  90%|████████▉ | 697/777 [04:17<00:30,  2.63it/s]\n",
      "Training: (loss 0.0273):  90%|████████▉ | 697/777 [04:18<00:30,  2.63it/s]\n",
      "Training: (loss 0.0273):  90%|████████▉ | 698/777 [04:18<00:28,  2.75it/s]\n",
      "Training: (loss 0.1960):  90%|████████▉ | 698/777 [04:18<00:28,  2.75it/s]\n",
      "Training: (loss 0.1960):  90%|████████▉ | 699/777 [04:18<00:27,  2.83it/s]\n",
      "Training: (loss -0.0305):  90%|████████▉ | 699/777 [04:18<00:27,  2.83it/s]\n",
      "Training: (loss -0.0305):  90%|█████████ | 700/777 [04:18<00:28,  2.75it/s]\n",
      "Training: (loss -0.0400):  90%|█████████ | 700/777 [04:19<00:28,  2.75it/s]\n",
      "Training: (loss -0.0400):  90%|█████████ | 701/777 [04:19<00:27,  2.79it/s]\n",
      "Training: (loss 0.3298):  90%|█████████ | 701/777 [04:19<00:27,  2.79it/s] \n",
      "Training: (loss 0.3298):  90%|█████████ | 702/777 [04:19<00:27,  2.72it/s]\n",
      "Training: (loss -0.1259):  90%|█████████ | 702/777 [04:19<00:27,  2.72it/s]\n",
      "Training: (loss -0.1259):  90%|█████████ | 703/777 [04:19<00:27,  2.67it/s]\n",
      "Training: (loss -0.1825):  90%|█████████ | 703/777 [04:20<00:27,  2.67it/s]\n",
      "Training: (loss -0.1825):  91%|█████████ | 704/777 [04:20<00:27,  2.63it/s]\n",
      "Training: (loss 0.3158):  91%|█████████ | 704/777 [04:20<00:27,  2.63it/s] \n",
      "Training: (loss 0.3158):  91%|█████████ | 705/777 [04:20<00:29,  2.47it/s]\n",
      "Training: (loss 0.0761):  91%|█████████ | 705/777 [04:21<00:29,  2.47it/s]\n",
      "Training: (loss 0.0761):  91%|█████████ | 706/777 [04:21<00:28,  2.51it/s]\n",
      "Training: (loss -0.1536):  91%|█████████ | 706/777 [04:21<00:28,  2.51it/s]\n",
      "Training: (loss -0.1536):  91%|█████████ | 707/777 [04:21<00:27,  2.58it/s]\n",
      "Training: (loss 0.2920):  91%|█████████ | 707/777 [04:21<00:27,  2.58it/s] \n",
      "Training: (loss 0.2920):  91%|█████████ | 708/777 [04:21<00:25,  2.70it/s]\n",
      "Training: (loss 0.0444):  91%|█████████ | 708/777 [04:22<00:25,  2.70it/s]\n",
      "Training: (loss 0.0444):  91%|█████████ | 709/777 [04:22<00:25,  2.70it/s]\n",
      "Training: (loss 0.1574):  91%|█████████ | 709/777 [04:22<00:25,  2.70it/s]\n",
      "Training: (loss 0.1574):  91%|█████████▏| 710/777 [04:22<00:25,  2.60it/s]\n",
      "Training: (loss -0.0625):  91%|█████████▏| 710/777 [04:22<00:25,  2.60it/s]\n",
      "Training: (loss -0.0625):  92%|█████████▏| 711/777 [04:22<00:25,  2.62it/s]\n",
      "Training: (loss -0.0486):  92%|█████████▏| 711/777 [04:23<00:25,  2.62it/s]\n",
      "Training: (loss -0.0486):  92%|█████████▏| 712/777 [04:23<00:24,  2.65it/s]\n",
      "Training: (loss 0.0504):  92%|█████████▏| 712/777 [04:23<00:24,  2.65it/s] \n",
      "Training: (loss 0.0504):  92%|█████████▏| 713/777 [04:23<00:24,  2.58it/s]\n",
      "Training: (loss -0.2026):  92%|█████████▏| 713/777 [04:24<00:24,  2.58it/s]\n",
      "Training: (loss -0.2026):  92%|█████████▏| 714/777 [04:24<00:24,  2.59it/s]\n",
      "Training: (loss 0.3318):  92%|█████████▏| 714/777 [04:24<00:24,  2.59it/s] \n",
      "Training: (loss 0.3318):  92%|█████████▏| 715/777 [04:24<00:23,  2.69it/s]\n",
      "Training: (loss 0.1415):  92%|█████████▏| 715/777 [04:24<00:23,  2.69it/s]\n",
      "Training: (loss 0.1415):  92%|█████████▏| 716/777 [04:24<00:22,  2.69it/s]\n",
      "Training: (loss 0.3242):  92%|█████████▏| 716/777 [04:25<00:22,  2.69it/s]\n",
      "Training: (loss 0.3242):  92%|█████████▏| 717/777 [04:25<00:22,  2.62it/s]\n",
      "Training: (loss 0.0572):  92%|█████████▏| 717/777 [04:25<00:22,  2.62it/s]\n",
      "Training: (loss 0.0572):  92%|█████████▏| 718/777 [04:25<00:22,  2.57it/s]\n",
      "Training: (loss -0.1041):  92%|█████████▏| 718/777 [04:26<00:22,  2.57it/s]\n",
      "Training: (loss -0.1041):  93%|█████████▎| 719/777 [04:26<00:22,  2.59it/s]\n",
      "Training: (loss 0.3338):  93%|█████████▎| 719/777 [04:26<00:22,  2.59it/s] \n",
      "Training: (loss 0.3338):  93%|█████████▎| 720/777 [04:26<00:20,  2.74it/s]\n",
      "Training: (loss -0.0787):  93%|█████████▎| 720/777 [04:26<00:20,  2.74it/s]\n",
      "Training: (loss -0.0787):  93%|█████████▎| 721/777 [04:26<00:19,  2.80it/s]\n",
      "Training: (loss -0.0895):  93%|█████████▎| 721/777 [04:27<00:19,  2.80it/s]\n",
      "Training: (loss -0.0895):  93%|█████████▎| 722/777 [04:27<00:19,  2.80it/s]\n",
      "Training: (loss 0.3155):  93%|█████████▎| 722/777 [04:27<00:19,  2.80it/s] \n",
      "Training: (loss 0.3155):  93%|█████████▎| 723/777 [04:27<00:19,  2.81it/s]\n",
      "Training: (loss 0.3304):  93%|█████████▎| 723/777 [04:27<00:19,  2.81it/s]\n",
      "Training: (loss 0.3304):  93%|█████████▎| 724/777 [04:27<00:18,  2.85it/s]\n",
      "Training: (loss -0.0534):  93%|█████████▎| 724/777 [04:28<00:18,  2.85it/s]\n",
      "Training: (loss -0.0534):  93%|█████████▎| 725/777 [04:28<00:19,  2.73it/s]\n",
      "Training: (loss 0.1664):  93%|█████████▎| 725/777 [04:28<00:19,  2.73it/s] \n",
      "Training: (loss 0.1664):  93%|█████████▎| 726/777 [04:28<00:18,  2.70it/s]\n",
      "Training: (loss -0.1307):  93%|█████████▎| 726/777 [04:28<00:18,  2.70it/s]\n",
      "Training: (loss -0.1307):  94%|█████████▎| 727/777 [04:28<00:19,  2.52it/s]\n",
      "Training: (loss 0.3271):  94%|█████████▎| 727/777 [04:29<00:19,  2.52it/s] \n",
      "Training: (loss 0.3271):  94%|█████████▎| 728/777 [04:29<00:18,  2.60it/s]\n",
      "Training: (loss -0.0048):  94%|█████████▎| 728/777 [04:29<00:18,  2.60it/s]\n",
      "Training: (loss -0.0048):  94%|█████████▍| 729/777 [04:29<00:18,  2.60it/s]\n",
      "Training: (loss -0.0285):  94%|█████████▍| 729/777 [04:30<00:18,  2.60it/s]\n",
      "Training: (loss -0.0285):  94%|█████████▍| 730/777 [04:30<00:17,  2.62it/s]\n",
      "Training: (loss -0.1104):  94%|█████████▍| 730/777 [04:30<00:17,  2.62it/s]\n",
      "Training: (loss -0.1104):  94%|█████████▍| 731/777 [04:30<00:18,  2.55it/s]\n",
      "Training: (loss 0.1898):  94%|█████████▍| 731/777 [04:30<00:18,  2.55it/s] \n",
      "Training: (loss 0.1898):  94%|█████████▍| 732/777 [04:30<00:17,  2.53it/s]\n",
      "Training: (loss 0.2179):  94%|█████████▍| 732/777 [04:31<00:17,  2.53it/s]\n",
      "Training: (loss 0.2179):  94%|█████████▍| 733/777 [04:31<00:16,  2.70it/s]\n",
      "Training: (loss 0.3261):  94%|█████████▍| 733/777 [04:31<00:16,  2.70it/s]\n",
      "Training: (loss 0.3261):  94%|█████████▍| 734/777 [04:31<00:16,  2.54it/s]\n",
      "Training: (loss 0.3163):  94%|█████████▍| 734/777 [04:32<00:16,  2.54it/s]\n",
      "Training: (loss 0.3163):  95%|█████████▍| 735/777 [04:32<00:17,  2.45it/s]\n",
      "Training: (loss 0.3267):  95%|█████████▍| 735/777 [04:32<00:17,  2.45it/s]\n",
      "Training: (loss 0.3267):  95%|█████████▍| 736/777 [04:32<00:17,  2.36it/s]\n",
      "Training: (loss 0.1992):  95%|█████████▍| 736/777 [04:32<00:17,  2.36it/s]\n",
      "Training: (loss 0.1992):  95%|█████████▍| 737/777 [04:32<00:16,  2.37it/s]\n",
      "Training: (loss -0.1816):  95%|█████████▍| 737/777 [04:33<00:16,  2.37it/s]\n",
      "Training: (loss -0.1816):  95%|█████████▍| 738/777 [04:33<00:16,  2.33it/s]\n",
      "Training: (loss 0.3146):  95%|█████████▍| 738/777 [04:33<00:16,  2.33it/s] \n",
      "Training: (loss 0.3146):  95%|█████████▌| 739/777 [04:33<00:15,  2.43it/s]\n",
      "Training: (loss 0.3188):  95%|█████████▌| 739/777 [04:34<00:15,  2.43it/s]\n",
      "Training: (loss 0.3188):  95%|█████████▌| 740/777 [04:34<00:14,  2.47it/s]\n",
      "Training: (loss 0.3192):  95%|█████████▌| 740/777 [04:34<00:14,  2.47it/s]\n",
      "Training: (loss 0.3192):  95%|█████████▌| 741/777 [04:34<00:13,  2.63it/s]\n",
      "Training: (loss 0.3106):  95%|█████████▌| 741/777 [04:34<00:13,  2.63it/s]\n",
      "Training: (loss 0.3106):  95%|█████████▌| 742/777 [04:34<00:13,  2.52it/s]\n",
      "Training: (loss 0.3241):  95%|█████████▌| 742/777 [04:35<00:13,  2.52it/s]\n",
      "Training: (loss 0.3241):  96%|█████████▌| 743/777 [04:35<00:13,  2.48it/s]\n",
      "Training: (loss -0.1141):  96%|█████████▌| 743/777 [04:35<00:13,  2.48it/s]\n",
      "Training: (loss -0.1141):  96%|█████████▌| 744/777 [04:35<00:13,  2.45it/s]\n",
      "Training: (loss -0.0569):  96%|█████████▌| 744/777 [04:36<00:13,  2.45it/s]\n",
      "Training: (loss -0.0569):  96%|█████████▌| 745/777 [04:36<00:12,  2.56it/s]\n",
      "Training: (loss -0.1511):  96%|█████████▌| 745/777 [04:36<00:12,  2.56it/s]\n",
      "Training: (loss -0.1511):  96%|█████████▌| 746/777 [04:36<00:11,  2.70it/s]\n",
      "Training: (loss 0.0996):  96%|█████████▌| 746/777 [04:36<00:11,  2.70it/s] \n",
      "Training: (loss 0.0996):  96%|█████████▌| 747/777 [04:36<00:10,  2.78it/s]\n",
      "Training: (loss -0.1099):  96%|█████████▌| 747/777 [04:37<00:10,  2.78it/s]\n",
      "Training: (loss -0.1099):  96%|█████████▋| 748/777 [04:37<00:09,  2.90it/s]\n",
      "Training: (loss -0.1254):  96%|█████████▋| 748/777 [04:37<00:09,  2.90it/s]\n",
      "Training: (loss -0.1254):  96%|█████████▋| 749/777 [04:37<00:09,  2.80it/s]\n",
      "Training: (loss 0.0522):  96%|█████████▋| 749/777 [04:37<00:09,  2.80it/s] \n",
      "Training: (loss 0.0522):  97%|█████████▋| 750/777 [04:37<00:09,  2.84it/s]\n",
      "Training: (loss 0.3154):  97%|█████████▋| 750/777 [04:38<00:09,  2.84it/s]\n",
      "Training: (loss 0.3154):  97%|█████████▋| 751/777 [04:38<00:09,  2.78it/s]\n",
      "Training: (loss -0.0286):  97%|█████████▋| 751/777 [04:38<00:09,  2.78it/s]\n",
      "Training: (loss -0.0286):  97%|█████████▋| 752/777 [04:38<00:09,  2.77it/s]\n",
      "Training: (loss -0.1931):  97%|█████████▋| 752/777 [04:38<00:09,  2.77it/s]\n",
      "Training: (loss -0.1931):  97%|█████████▋| 753/777 [04:38<00:09,  2.64it/s]\n",
      "Training: (loss -0.1064):  97%|█████████▋| 753/777 [04:39<00:09,  2.64it/s]\n",
      "Training: (loss -0.1064):  97%|█████████▋| 754/777 [04:39<00:08,  2.80it/s]\n",
      "Training: (loss 0.3121):  97%|█████████▋| 754/777 [04:39<00:08,  2.80it/s] \n",
      "Training: (loss 0.3121):  97%|█████████▋| 755/777 [04:39<00:08,  2.69it/s]\n",
      "Training: (loss -0.0993):  97%|█████████▋| 755/777 [04:39<00:08,  2.69it/s]\n",
      "Training: (loss -0.0993):  97%|█████████▋| 756/777 [04:39<00:07,  2.95it/s]\n",
      "Training: (loss 0.0618):  97%|█████████▋| 756/777 [04:40<00:07,  2.95it/s] \n",
      "Training: (loss 0.0618):  97%|█████████▋| 757/777 [04:40<00:07,  2.72it/s]\n",
      "Training: (loss 0.0462):  97%|█████████▋| 757/777 [04:40<00:07,  2.72it/s]\n",
      "Training: (loss 0.0462):  98%|█████████▊| 758/777 [04:40<00:06,  2.77it/s]\n",
      "Training: (loss 0.3170):  98%|█████████▊| 758/777 [04:41<00:06,  2.77it/s]\n",
      "Training: (loss 0.3170):  98%|█████████▊| 759/777 [04:41<00:06,  2.68it/s]\n",
      "Training: (loss -0.0522):  98%|█████████▊| 759/777 [04:41<00:06,  2.68it/s]\n",
      "Training: (loss -0.0522):  98%|█████████▊| 760/777 [04:41<00:06,  2.74it/s]\n",
      "Training: (loss 0.3102):  98%|█████████▊| 760/777 [04:41<00:06,  2.74it/s] \n",
      "Training: (loss 0.3102):  98%|█████████▊| 761/777 [04:41<00:05,  2.87it/s]\n",
      "Training: (loss 0.0120):  98%|█████████▊| 761/777 [04:42<00:05,  2.87it/s]\n",
      "Training: (loss 0.0120):  98%|█████████▊| 762/777 [04:42<00:05,  2.81it/s]\n",
      "Training: (loss -0.0343):  98%|█████████▊| 762/777 [04:42<00:05,  2.81it/s]\n",
      "Training: (loss -0.0343):  98%|█████████▊| 763/777 [04:42<00:05,  2.67it/s]\n",
      "Training: (loss 0.3173):  98%|█████████▊| 763/777 [04:42<00:05,  2.67it/s] \n",
      "Training: (loss 0.3173):  98%|█████████▊| 764/777 [04:42<00:04,  2.64it/s]\n",
      "Training: (loss -0.0338):  98%|█████████▊| 764/777 [04:43<00:04,  2.64it/s]\n",
      "Training: (loss -0.0338):  98%|█████████▊| 765/777 [04:43<00:04,  2.60it/s]\n",
      "Training: (loss -0.0117):  98%|█████████▊| 765/777 [04:43<00:04,  2.60it/s]\n",
      "Training: (loss -0.0117):  99%|█████████▊| 766/777 [04:43<00:04,  2.68it/s]\n",
      "Training: (loss 0.1732):  99%|█████████▊| 766/777 [04:44<00:04,  2.68it/s] \n",
      "Training: (loss 0.1732):  99%|█████████▊| 767/777 [04:44<00:03,  2.79it/s]\n",
      "Training: (loss -0.0192):  99%|█████████▊| 767/777 [04:44<00:03,  2.79it/s]\n",
      "Training: (loss -0.0192):  99%|█████████▉| 768/777 [04:44<00:03,  2.73it/s]\n",
      "Training: (loss -0.0128):  99%|█████████▉| 768/777 [04:44<00:03,  2.73it/s]\n",
      "Training: (loss -0.0128):  99%|█████████▉| 769/777 [04:44<00:03,  2.66it/s]\n",
      "Training: (loss -0.0673):  99%|█████████▉| 769/777 [04:45<00:03,  2.66it/s]\n",
      "Training: (loss -0.0673):  99%|█████████▉| 770/777 [04:45<00:02,  2.70it/s]\n",
      "Training: (loss -0.1557):  99%|█████████▉| 770/777 [04:45<00:02,  2.70it/s]\n",
      "Training: (loss -0.1557):  99%|█████████▉| 771/777 [04:45<00:02,  2.91it/s]\n",
      "Training: (loss 0.1352):  99%|█████████▉| 771/777 [04:45<00:02,  2.91it/s] \n",
      "Training: (loss 0.1352):  99%|█████████▉| 772/777 [04:45<00:01,  2.99it/s]\n",
      "Training: (loss 0.2836):  99%|█████████▉| 772/777 [04:46<00:01,  2.99it/s]\n",
      "Training: (loss 0.2836):  99%|█████████▉| 773/777 [04:46<00:01,  2.74it/s]\n",
      "Training: (loss 0.3217):  99%|█████████▉| 773/777 [04:46<00:01,  2.74it/s]\n",
      "Training: (loss 0.3217): 100%|█████████▉| 774/777 [04:46<00:01,  2.67it/s]\n",
      "Training: (loss 0.0354): 100%|█████████▉| 774/777 [04:46<00:01,  2.67it/s]\n",
      "Training: (loss 0.0354): 100%|█████████▉| 775/777 [04:46<00:00,  2.69it/s]\n",
      "Training: (loss 0.3197): 100%|█████████▉| 775/777 [04:47<00:00,  2.69it/s]\n",
      "Training: (loss 0.3197): 100%|█████████▉| 776/777 [04:47<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg training loss: 0.07060269075547713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: (loss 0.3248): 100%|█████████▉| 776/777 [04:47<00:00,  2.81it/s]\n",
      "Training: (loss 0.3248): 100%|██████████| 777/777 [04:47<00:00,  2.83it/s]\n",
      "                                                                          \n",
      "Validation:   0%|          | 0/87 [00:00<?, ?it/s]\n",
      "Validation: (loss -0.2190):   0%|          | 0/87 [00:00<?, ?it/s]\n",
      "Validation: (loss -0.2190):   1%|          | 1/87 [00:00<00:32,  2.68it/s]\n",
      "Validation: (loss 0.3290):   1%|          | 1/87 [00:00<00:32,  2.68it/s] \n",
      "Validation: (loss 0.3290):   2%|▏         | 2/87 [00:00<00:30,  2.80it/s]\n",
      "Validation: (loss 0.1666):   2%|▏         | 2/87 [00:00<00:30,  2.80it/s]\n",
      "Validation: (loss 0.1666):   3%|▎         | 3/87 [00:00<00:26,  3.15it/s]\n",
      "Validation: (loss -0.0678):   3%|▎         | 3/87 [00:01<00:26,  3.15it/s]\n",
      "Validation: (loss -0.0678):   5%|▍         | 4/87 [00:01<00:25,  3.25it/s]\n",
      "Validation: (loss 0.0450):   5%|▍         | 4/87 [00:01<00:25,  3.25it/s] \n",
      "Validation: (loss 0.0450):   6%|▌         | 5/87 [00:01<00:25,  3.17it/s]\n",
      "Validation: (loss -0.1900):   6%|▌         | 5/87 [00:01<00:25,  3.17it/s]\n",
      "Validation: (loss -0.1900):   7%|▋         | 6/87 [00:01<00:26,  3.05it/s]\n",
      "Validation: (loss 0.0825):   7%|▋         | 6/87 [00:02<00:26,  3.05it/s] \n",
      "Validation: (loss 0.0825):   8%|▊         | 7/87 [00:02<00:27,  2.86it/s]\n",
      "Validation: (loss -0.0351):   8%|▊         | 7/87 [00:02<00:27,  2.86it/s]\n",
      "Validation: (loss -0.0351):   9%|▉         | 8/87 [00:02<00:27,  2.85it/s]\n",
      "Validation: (loss -0.0435):   9%|▉         | 8/87 [00:03<00:27,  2.85it/s]\n",
      "Validation: (loss -0.0435):  10%|█         | 9/87 [00:03<00:27,  2.88it/s]\n",
      "Validation: (loss 0.1366):  10%|█         | 9/87 [00:03<00:27,  2.88it/s] \n",
      "Validation: (loss 0.1366):  11%|█▏        | 10/87 [00:03<00:25,  2.98it/s]\n",
      "Validation: (loss 0.1553):  11%|█▏        | 10/87 [00:03<00:25,  2.98it/s]\n",
      "Validation: (loss 0.1553):  13%|█▎        | 11/87 [00:03<00:25,  2.97it/s]\n",
      "Validation: (loss 0.2866):  13%|█▎        | 11/87 [00:03<00:25,  2.97it/s]\n",
      "Validation: (loss 0.2866):  14%|█▍        | 12/87 [00:03<00:24,  3.11it/s]\n",
      "Validation: (loss 0.0545):  14%|█▍        | 12/87 [00:04<00:24,  3.11it/s]\n",
      "Validation: (loss 0.0545):  15%|█▍        | 13/87 [00:04<00:23,  3.10it/s]\n",
      "Validation: (loss 0.0836):  15%|█▍        | 13/87 [00:04<00:23,  3.10it/s]\n",
      "Validation: (loss 0.0836):  16%|█▌        | 14/87 [00:04<00:24,  3.01it/s]\n",
      "Validation: (loss -0.0402):  16%|█▌        | 14/87 [00:05<00:24,  3.01it/s]\n",
      "Validation: (loss -0.0402):  17%|█▋        | 15/87 [00:05<00:26,  2.73it/s]\n",
      "Validation: (loss -0.1484):  17%|█▋        | 15/87 [00:05<00:26,  2.73it/s]\n",
      "Validation: (loss -0.1484):  18%|█▊        | 16/87 [00:05<00:26,  2.64it/s]\n",
      "Validation: (loss 0.2112):  18%|█▊        | 16/87 [00:05<00:26,  2.64it/s] \n",
      "Validation: (loss 0.2112):  20%|█▉        | 17/87 [00:05<00:26,  2.65it/s]\n",
      "Validation: (loss -0.0354):  20%|█▉        | 17/87 [00:06<00:26,  2.65it/s]\n",
      "Validation: (loss -0.0354):  21%|██        | 18/87 [00:06<00:24,  2.77it/s]\n",
      "Validation: (loss -0.2107):  21%|██        | 18/87 [00:06<00:24,  2.77it/s]\n",
      "Validation: (loss -0.2107):  22%|██▏       | 19/87 [00:06<00:24,  2.76it/s]\n",
      "Validation: (loss -0.1795):  22%|██▏       | 19/87 [00:06<00:24,  2.76it/s]\n",
      "Validation: (loss -0.1795):  23%|██▎       | 20/87 [00:06<00:23,  2.80it/s]\n",
      "Validation: (loss 0.0056):  23%|██▎       | 20/87 [00:07<00:23,  2.80it/s] \n",
      "Validation: (loss 0.0056):  24%|██▍       | 21/87 [00:07<00:22,  2.96it/s]\n",
      "Validation: (loss -0.2513):  24%|██▍       | 21/87 [00:07<00:22,  2.96it/s]\n",
      "Validation: (loss -0.2513):  25%|██▌       | 22/87 [00:07<00:22,  2.86it/s]\n",
      "Validation: (loss 0.2523):  25%|██▌       | 22/87 [00:07<00:22,  2.86it/s] \n",
      "Validation: (loss 0.2523):  26%|██▋       | 23/87 [00:07<00:21,  2.92it/s]\n",
      "Validation: (loss 0.3277):  26%|██▋       | 23/87 [00:08<00:21,  2.92it/s]\n",
      "Validation: (loss 0.3277):  28%|██▊       | 24/87 [00:08<00:22,  2.81it/s]\n",
      "Validation: (loss 0.1073):  28%|██▊       | 24/87 [00:08<00:22,  2.81it/s]\n",
      "Validation: (loss 0.1073):  29%|██▊       | 25/87 [00:08<00:21,  2.83it/s]\n",
      "Validation: (loss -0.0152):  29%|██▊       | 25/87 [00:08<00:21,  2.83it/s]\n",
      "Validation: (loss -0.0152):  30%|██▉       | 26/87 [00:08<00:21,  2.85it/s]\n",
      "Validation: (loss -0.1645):  30%|██▉       | 26/87 [00:09<00:21,  2.85it/s]\n",
      "Validation: (loss -0.1645):  31%|███       | 27/87 [00:09<00:21,  2.84it/s]\n",
      "Validation: (loss -0.0207):  31%|███       | 27/87 [00:09<00:21,  2.84it/s]\n",
      "Validation: (loss -0.0207):  32%|███▏      | 28/87 [00:09<00:21,  2.77it/s]\n",
      "Validation: (loss -0.0046):  32%|███▏      | 28/87 [00:10<00:21,  2.77it/s]\n",
      "Validation: (loss -0.0046):  33%|███▎      | 29/87 [00:10<00:21,  2.71it/s]\n",
      "Validation: (loss 0.3742):  33%|███▎      | 29/87 [00:10<00:21,  2.71it/s] \n",
      "Validation: (loss 0.3742):  34%|███▍      | 30/87 [00:10<00:21,  2.64it/s]\n",
      "Validation: (loss -0.1782):  34%|███▍      | 30/87 [00:10<00:21,  2.64it/s]\n",
      "Validation: (loss -0.1782):  36%|███▌      | 31/87 [00:10<00:20,  2.75it/s]\n",
      "Validation: (loss -0.1084):  36%|███▌      | 31/87 [00:11<00:20,  2.75it/s]\n",
      "Validation: (loss -0.1084):  37%|███▋      | 32/87 [00:11<00:20,  2.67it/s]\n",
      "Validation: (loss 0.3235):  37%|███▋      | 32/87 [00:11<00:20,  2.67it/s] \n",
      "Validation: (loss 0.3235):  38%|███▊      | 33/87 [00:11<00:20,  2.58it/s]\n",
      "Validation: (loss 0.1878):  38%|███▊      | 33/87 [00:12<00:20,  2.58it/s]\n",
      "Validation: (loss 0.1878):  39%|███▉      | 34/87 [00:12<00:19,  2.66it/s]\n",
      "Validation: (loss 0.3038):  39%|███▉      | 34/87 [00:12<00:19,  2.66it/s]\n",
      "Validation: (loss 0.3038):  40%|████      | 35/87 [00:12<00:18,  2.76it/s]\n",
      "Validation: (loss -0.1047):  40%|████      | 35/87 [00:12<00:18,  2.76it/s]\n",
      "Validation: (loss -0.1047):  41%|████▏     | 36/87 [00:12<00:17,  2.98it/s]\n",
      "Validation: (loss 0.1852):  41%|████▏     | 36/87 [00:12<00:17,  2.98it/s] \n",
      "Validation: (loss 0.1852):  43%|████▎     | 37/87 [00:12<00:16,  3.09it/s]\n",
      "Validation: (loss 0.0844):  43%|████▎     | 37/87 [00:13<00:16,  3.09it/s]\n",
      "Validation: (loss 0.0844):  44%|████▎     | 38/87 [00:13<00:15,  3.14it/s]\n",
      "Validation: (loss 0.3348):  44%|████▎     | 38/87 [00:13<00:15,  3.14it/s]\n",
      "Validation: (loss 0.3348):  45%|████▍     | 39/87 [00:13<00:15,  3.01it/s]\n",
      "Validation: (loss -0.0581):  45%|████▍     | 39/87 [00:14<00:15,  3.01it/s]\n",
      "Validation: (loss -0.0581):  46%|████▌     | 40/87 [00:14<00:16,  2.81it/s]\n",
      "Validation: (loss -0.1807):  46%|████▌     | 40/87 [00:14<00:16,  2.81it/s]\n",
      "Validation: (loss -0.1807):  47%|████▋     | 41/87 [00:14<00:16,  2.72it/s]\n",
      "Validation: (loss -0.1244):  47%|████▋     | 41/87 [00:14<00:16,  2.72it/s]\n",
      "Validation: (loss -0.1244):  48%|████▊     | 42/87 [00:14<00:16,  2.76it/s]\n",
      "Validation: (loss -0.0241):  48%|████▊     | 42/87 [00:15<00:16,  2.76it/s]\n",
      "Validation: (loss -0.0241):  49%|████▉     | 43/87 [00:15<00:15,  2.87it/s]\n",
      "Validation: (loss 0.1232):  49%|████▉     | 43/87 [00:15<00:15,  2.87it/s] \n",
      "Validation: (loss 0.1232):  51%|█████     | 44/87 [00:15<00:15,  2.72it/s]\n",
      "Validation: (loss -0.0905):  51%|█████     | 44/87 [00:15<00:15,  2.72it/s]\n",
      "Validation: (loss -0.0905):  52%|█████▏    | 45/87 [00:15<00:14,  2.86it/s]\n",
      "Validation: (loss 0.0280):  52%|█████▏    | 45/87 [00:16<00:14,  2.86it/s] \n",
      "Validation: (loss 0.0280):  53%|█████▎    | 46/87 [00:16<00:14,  2.78it/s]\n",
      "Validation: (loss 0.0017):  53%|█████▎    | 46/87 [00:16<00:14,  2.78it/s]\n",
      "Validation: (loss 0.0017):  54%|█████▍    | 47/87 [00:16<00:14,  2.83it/s]\n",
      "Validation: (loss 0.3351):  54%|█████▍    | 47/87 [00:16<00:14,  2.83it/s]\n",
      "Validation: (loss 0.3351):  55%|█████▌    | 48/87 [00:16<00:13,  2.86it/s]\n",
      "Validation: (loss -0.0405):  55%|█████▌    | 48/87 [00:17<00:13,  2.86it/s]\n",
      "Validation: (loss -0.0405):  56%|█████▋    | 49/87 [00:17<00:13,  2.87it/s]\n",
      "Validation: (loss -0.0617):  56%|█████▋    | 49/87 [00:17<00:13,  2.87it/s]\n",
      "Validation: (loss -0.0617):  57%|█████▋    | 50/87 [00:17<00:13,  2.83it/s]\n",
      "Validation: (loss -0.0852):  57%|█████▋    | 50/87 [00:17<00:13,  2.83it/s]\n",
      "Validation: (loss -0.0852):  59%|█████▊    | 51/87 [00:17<00:12,  2.84it/s]\n",
      "Validation: (loss 0.2875):  59%|█████▊    | 51/87 [00:18<00:12,  2.84it/s] \n",
      "Validation: (loss 0.2875):  60%|█████▉    | 52/87 [00:18<00:11,  2.93it/s]\n",
      "Validation: (loss 0.0469):  60%|█████▉    | 52/87 [00:18<00:11,  2.93it/s]\n",
      "Validation: (loss 0.0469):  61%|██████    | 53/87 [00:18<00:11,  2.88it/s]\n",
      "Validation: (loss 0.0537):  61%|██████    | 53/87 [00:18<00:11,  2.88it/s]\n",
      "Validation: (loss 0.0537):  62%|██████▏   | 54/87 [00:18<00:11,  2.92it/s]\n",
      "Validation: (loss 0.1682):  62%|██████▏   | 54/87 [00:19<00:11,  2.92it/s]\n",
      "Validation: (loss 0.1682):  63%|██████▎   | 55/87 [00:19<00:10,  3.14it/s]\n",
      "Validation: (loss -0.2438):  63%|██████▎   | 55/87 [00:19<00:10,  3.14it/s]\n",
      "Validation: (loss -0.2438):  64%|██████▍   | 56/87 [00:19<00:09,  3.11it/s]\n",
      "Validation: (loss 0.1284):  64%|██████▍   | 56/87 [00:19<00:09,  3.11it/s] \n",
      "Validation: (loss 0.1284):  66%|██████▌   | 57/87 [00:19<00:10,  2.95it/s]\n",
      "Validation: (loss -0.1667):  66%|██████▌   | 57/87 [00:20<00:10,  2.95it/s]\n",
      "Validation: (loss -0.1667):  67%|██████▋   | 58/87 [00:20<00:09,  2.98it/s]\n",
      "Validation: (loss 0.0964):  67%|██████▋   | 58/87 [00:20<00:09,  2.98it/s] \n",
      "Validation: (loss 0.0964):  68%|██████▊   | 59/87 [00:20<00:09,  3.07it/s]\n",
      "Validation: (loss -0.0900):  68%|██████▊   | 59/87 [00:20<00:09,  3.07it/s]\n",
      "Validation: (loss -0.0900):  69%|██████▉   | 60/87 [00:20<00:08,  3.11it/s]\n",
      "Validation: (loss -0.0451):  69%|██████▉   | 60/87 [00:21<00:08,  3.11it/s]\n",
      "Validation: (loss -0.0451):  70%|███████   | 61/87 [00:21<00:09,  2.80it/s]\n",
      "Validation: (loss -0.1667):  70%|███████   | 61/87 [00:21<00:09,  2.80it/s]\n",
      "Validation: (loss -0.1667):  71%|███████▏  | 62/87 [00:21<00:09,  2.64it/s]\n",
      "Validation: (loss -0.0633):  71%|███████▏  | 62/87 [00:22<00:09,  2.64it/s]\n",
      "Validation: (loss -0.0633):  72%|███████▏  | 63/87 [00:22<00:08,  2.69it/s]\n",
      "Validation: (loss -0.0769):  72%|███████▏  | 63/87 [00:22<00:08,  2.69it/s]\n",
      "Validation: (loss -0.0769):  74%|███████▎  | 64/87 [00:22<00:08,  2.69it/s]\n",
      "Validation: (loss 0.0751):  74%|███████▎  | 64/87 [00:22<00:08,  2.69it/s] \n",
      "Validation: (loss 0.0751):  75%|███████▍  | 65/87 [00:22<00:07,  2.83it/s]\n",
      "Validation: (loss 0.2015):  75%|███████▍  | 65/87 [00:23<00:07,  2.83it/s]\n",
      "Validation: (loss 0.2015):  76%|███████▌  | 66/87 [00:23<00:07,  2.96it/s]\n",
      "Validation: (loss 0.3134):  76%|███████▌  | 66/87 [00:23<00:07,  2.96it/s]\n",
      "Validation: (loss 0.3134):  77%|███████▋  | 67/87 [00:23<00:06,  2.90it/s]\n",
      "Validation: (loss -0.1613):  77%|███████▋  | 67/87 [00:23<00:06,  2.90it/s]\n",
      "Validation: (loss -0.1613):  78%|███████▊  | 68/87 [00:23<00:06,  2.79it/s]\n",
      "Validation: (loss 0.3646):  78%|███████▊  | 68/87 [00:24<00:06,  2.79it/s] \n",
      "Validation: (loss 0.3646):  79%|███████▉  | 69/87 [00:24<00:06,  2.65it/s]\n",
      "Validation: (loss 0.3219):  79%|███████▉  | 69/87 [00:24<00:06,  2.65it/s]\n",
      "Validation: (loss 0.3219):  80%|████████  | 70/87 [00:24<00:06,  2.62it/s]\n",
      "Validation: (loss 0.0637):  80%|████████  | 70/87 [00:24<00:06,  2.62it/s]\n",
      "Validation: (loss 0.0637):  82%|████████▏ | 71/87 [00:24<00:06,  2.64it/s]\n",
      "Validation: (loss 0.3379):  82%|████████▏ | 71/87 [00:25<00:06,  2.64it/s]\n",
      "Validation: (loss 0.3379):  83%|████████▎ | 72/87 [00:25<00:05,  2.80it/s]\n",
      "Validation: (loss -0.0625):  83%|████████▎ | 72/87 [00:25<00:05,  2.80it/s]\n",
      "Validation: (loss -0.0625):  84%|████████▍ | 73/87 [00:25<00:05,  2.67it/s]\n",
      "Validation: (loss -0.0765):  84%|████████▍ | 73/87 [00:26<00:05,  2.67it/s]\n",
      "Validation: (loss -0.0765):  85%|████████▌ | 74/87 [00:26<00:04,  2.74it/s]\n",
      "Validation: (loss -0.1177):  85%|████████▌ | 74/87 [00:26<00:04,  2.74it/s]\n",
      "Validation: (loss -0.1177):  86%|████████▌ | 75/87 [00:26<00:04,  2.88it/s]\n",
      "Validation: (loss -0.0339):  86%|████████▌ | 75/87 [00:26<00:04,  2.88it/s]\n",
      "Validation: (loss -0.0339):  87%|████████▋ | 76/87 [00:26<00:03,  2.88it/s]\n",
      "Validation: (loss 0.0319):  87%|████████▋ | 76/87 [00:27<00:03,  2.88it/s] \n",
      "Validation: (loss 0.0319):  89%|████████▊ | 77/87 [00:27<00:03,  2.87it/s]\n",
      "Validation: (loss 0.2418):  89%|████████▊ | 77/87 [00:27<00:03,  2.87it/s]\n",
      "Validation: (loss 0.2418):  90%|████████▉ | 78/87 [00:27<00:03,  2.92it/s]\n",
      "Validation: (loss -0.2225):  90%|████████▉ | 78/87 [00:27<00:03,  2.92it/s]\n",
      "Validation: (loss -0.2225):  91%|█████████ | 79/87 [00:27<00:02,  3.14it/s]\n",
      "Validation: (loss 0.1265):  91%|█████████ | 79/87 [00:27<00:02,  3.14it/s] \n",
      "Validation: (loss 0.1265):  92%|█████████▏| 80/87 [00:27<00:02,  3.18it/s]\n",
      "Validation: (loss -0.0731):  92%|█████████▏| 80/87 [00:28<00:02,  3.18it/s]\n",
      "Validation: (loss -0.0731):  93%|█████████▎| 81/87 [00:28<00:01,  3.15it/s]\n",
      "Validation: (loss 0.1199):  93%|█████████▎| 81/87 [00:28<00:01,  3.15it/s] \n",
      "Validation: (loss 0.1199):  94%|█████████▍| 82/87 [00:28<00:01,  2.90it/s]\n",
      "Validation: (loss -0.0512):  94%|█████████▍| 82/87 [00:29<00:01,  2.90it/s]\n",
      "Validation: (loss -0.0512):  95%|█████████▌| 83/87 [00:29<00:01,  2.73it/s]\n",
      "Validation: (loss -0.0458):  95%|█████████▌| 83/87 [00:29<00:01,  2.73it/s]\n",
      "Validation: (loss -0.0458):  97%|█████████▋| 84/87 [00:29<00:01,  2.79it/s]\n",
      "Validation: (loss 0.3308):  97%|█████████▋| 84/87 [00:29<00:01,  2.79it/s] \n",
      "Validation: (loss 0.3308):  98%|█████████▊| 85/87 [00:29<00:00,  2.72it/s]\n",
      "Validation: (loss 0.3355):  98%|█████████▊| 85/87 [00:30<00:00,  2.72it/s]\n",
      "Validation: (loss 0.3355):  99%|█████████▉| 86/87 [00:30<00:00,  2.71it/s]\n",
      "Validation: (loss -0.2206):  99%|█████████▉| 86/87 [00:30<00:00,  2.71it/s]\n",
      "Validation: (loss -0.2206): 100%|██████████| 87/87 [00:30<00:00,  2.84it/s]\n",
      "Progress: 100%|██████████| 1/1 [05:18<00:00, 318.19s/it]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg validation loss: 0.041046369007264064\n",
      "  0%|          | 0/87 [00:00<?, ?it/s]\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.73551875, 0.4410486, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.65060896, 0.47677174, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.6854805, 0.46431676, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.6556693, 0.47514606, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.84439284, 0.3624758, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.737639, 0.43991077, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.7804877, 0.4139081, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.68741107, 0.4635434, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.79825735, 0.40129423, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.8131613, 0.38977385, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "torch.Size([1, 1, 64, 64, 64])\n",
      "(1, 2, 64, 64, 64)\n",
      "(0.0, 1.0, 0.74272096, 0.4371264, 1.0, (1, 2, 64, 64, 64))\n",
      "Figure(900x900)\n",
      "Figure(900x900)\n",
      "Saved model unet3d_fullblob0402_1307.pt\n",
      "Using unet3d\n",
      "Predicting segmentation on volume of shape (128, 1450, 1450)\n",
      "UNet3D(\n",
      "  (encoder): Encoder(\n",
      "    (encoding_blocks): ModuleList(\n",
      "      (0): EncodingBlock(\n",
      "        (conv1): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (conv2): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv_residual): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(1, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (block): Sequential(\n",
      "            (0): Conv3d(1, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (downsample): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): EncodingBlock(\n",
      "        (conv1): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv2): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv_residual): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (block): Sequential(\n",
      "            (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (downsample): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bottom_block): EncodingBlock(\n",
      "    (conv1): ConvolutionalBlock(\n",
      "      (conv_layer): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm_layer): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation_layer): ReLU()\n",
      "      (block): Sequential(\n",
      "        (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "    (conv2): ConvolutionalBlock(\n",
      "      (conv_layer): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm_layer): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation_layer): ReLU()\n",
      "      (block): Sequential(\n",
      "        (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "    (conv_residual): ConvolutionalBlock(\n",
      "      (conv_layer): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (block): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoding_blocks): ModuleList(\n",
      "      (0): DecodingBlock(\n",
      "        (upsample): Upsample(scale_factor=2.0, mode=trilinear)\n",
      "        (conv1): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv2): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv_residual): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(384, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (block): Sequential(\n",
      "            (0): Conv3d(384, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): DecodingBlock(\n",
      "        (upsample): Upsample(scale_factor=2.0, mode=trilinear)\n",
      "        (conv1): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv2): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (norm_layer): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation_layer): ReLU()\n",
      "          (block): Sequential(\n",
      "            (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (conv_residual): ConvolutionalBlock(\n",
      "          (conv_layer): Conv3d(192, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (block): Sequential(\n",
      "            (0): Conv3d(192, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ConvolutionalBlock(\n",
      "    (conv_layer): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    (block): Sequential(\n",
      "      (0): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loaded model from /ceph/users/fot15858/chroot/vf_main_feb2023/fcn/04022023_13_02_18_trained_fcn_model/unet3d_fullblob0402_1307.pt\n",
      "(128, 1450, 1450)\n",
      "Predict and aggregate on volume of torch.Size([1, 128, 1450, 1450])\n",
      "  0%|          | 0/2700 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO - Shutting down  - uvicorn.server:shutdown:253\n",
      "INFO:     Waiting for background tasks to complete. (CTRL+C to force quit)\n",
      "INFO - Waiting for background tasks to complete. (CTRL+C to force quit)  - uvicorn.server:shutdown:278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU_NET_PLUS_PLUS\u001b[39m\u001b[38;5;124m'\u001b[39m,]: \u001b[38;5;66;03m#['U_NET','DEEPLABV3', 'U_NET_PLUS_PLUS', 'FPN', 'MA_NET']:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning notebook for: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(method))\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSeg_Basic_3dCNN.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martifact_dir/notebooks/basic_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mworkspace_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_criterion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mloss_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mencoder_type\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/papermill/execute.py:93\u001b[0m, in \u001b[0;36mexecute_notebook\u001b[0;34m(input_path, output_path, parameters, engine_name, request_save_on_cell_execute, prepare_only, kernel_name, progress_bar, log_output, stdout_file, stderr_file, start_timeout, report_mode, cwd, **engine_kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Execute the Notebook in `cwd` if it is set\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m chdir(cwd):\n\u001b[0;32m---> 93\u001b[0m     nb \u001b[38;5;241m=\u001b[39m \u001b[43mpapermill_engines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_notebook_with_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_save_on_cell_execute\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdout_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstderr_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Check for errors first (it saves on error before raising)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m raise_for_execution_errors(nb, output_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/papermill/engines.py:49\u001b[0m, in \u001b[0;36mPapermillEngines.execute_notebook_with_engine\u001b[0;34m(self, engine_name, nb, kernel_name, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_notebook_with_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m, engine_name, nb, kernel_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124;03m\"\"\"Fetch a named engine and execute the nb object against it.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/papermill/engines.py:343\u001b[0m, in \u001b[0;36mEngine.execute_notebook\u001b[0;34m(cls, nb, kernel_name, output_path, progress_bar, log_output, autosave_cell_every, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m nb_man\u001b[38;5;241m.\u001b[39mnotebook_start()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_managed_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_man\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     nb_man\u001b[38;5;241m.\u001b[39mcleanup_pbar()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/papermill/engines.py:402\u001b[0m, in \u001b[0;36mNBClientEngine.execute_managed_notebook\u001b[0;34m(cls, nb_man, kernel_name, log_output, stdout_file, stderr_file, start_timeout, execution_timeout, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Nicely handle preprocessor arguments prioritizing values set by engine\u001b[39;00m\n\u001b[1;32m    392\u001b[0m final_kwargs \u001b[38;5;241m=\u001b[39m merge_kwargs(\n\u001b[1;32m    393\u001b[0m     safe_kwargs,\n\u001b[1;32m    394\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mexecution_timeout \u001b[38;5;28;01mif\u001b[39;00m execution_timeout \u001b[38;5;28;01melse\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     stderr_file\u001b[38;5;241m=\u001b[39mstderr_file,\n\u001b[1;32m    401\u001b[0m )\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPapermillNotebookClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_man\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinal_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/papermill/clientwrap.py:38\u001b[0m, in \u001b[0;36mPapermillNotebookClient.execute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_kernel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting notebook with kernel: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_name)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpapermill_execute_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     info_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_reply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkc\u001b[38;5;241m.\u001b[39mkernel_info())\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m info_msg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/papermill/clientwrap.py:65\u001b[0m, in \u001b[0;36mPapermillNotebookClient.papermill_execute_cells\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_man\u001b[38;5;241m.\u001b[39mcell_start(cell, index)\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CellExecutionError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_man\u001b[38;5;241m.\u001b[39mcell_exception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells[index], cell_index\u001b[38;5;241m=\u001b[39mindex, exception\u001b[38;5;241m=\u001b[39mex)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jupyter_core/utils/__init__.py:160\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _runner_map:\n\u001b[1;32m    159\u001b[0m         _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runner_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jupyter_core/utils/__init__.py:128\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__runner_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    127\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for method in ['U_NET_PLUS_PLUS',]: #['U_NET','DEEPLABV3', 'U_NET_PLUS_PLUS', 'FPN', 'MA_NET']:\n",
    "    logger.info('Running notebook for: {}'.format(method))\n",
    "    pm.execute_notebook(input_path='Seg_Basic_3dCNN.ipynb', \n",
    "                        output_path='artifact_dir/notebooks/basic_{}.ipynb'.format(method),\n",
    "                        parameters={'workspace_name': workspace_name, \n",
    "                                    'port':port, \n",
    "                                    'method':method,\n",
    "                                    'loss_criterion':loss_criterion,\n",
    "                                    'encoder_type':encoder_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55339000-4132-4bea-a8d1-993650116f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd76794-57d6-4bf1-9c27-46895d853739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4514fb-c26e-431b-bf0d-1a8c1e8dbcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9cc45-04f3-41e6-a188-8803a3d74dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_server(server_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
