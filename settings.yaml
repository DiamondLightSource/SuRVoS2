model:
  chroot: C:/dls/science/groups/das/SuRVoS/s2/data
  dbtype: yaml
api:
  host: 127.0.0.1
  plugins:
  - workspace
  - workspaces
  - features
  - superregions
  - annotations
  - pipelines
  - objects
  - analyzer
  - roi
  - export
  port: 8130
  renderer: mpl
computing:
  chunk_padding: 8
  chunk_size: 10
  chunk_size_sparse: 10
  chunks: true
  scale: false
  stretch: false
filters:
  filter1:
    feature: gaussian
    params:
      sigma: 2
    plugin: features
logging:
  file: ''
  file_format: '%(asctime)s - | %(levelname)8s | %(message)s'
  level: debug
  overall_level: INFO
  std: true
  std_format: '%(levelname)8s | %(message)s'
pipeline:
  calculate_features: true
  calculate_supervoxels: true
  load_annotation: false
  load_pretrained_classifier: true
  mask_params:
    mask_radius: 10
  predict_params:
    clf: Ensemble
    max_depth: 20
    n_estimators: 10
    n_jobs: 1
    proj: rproj
    type: rf
qtui:
  maximized: false
  menuKey: \
slic: skimage
title: 'SuRVoS2: Super-Region Volume Segmentation Workbench'
volume_mode: volume
volume_segmantics:
  train_settings:
    # Settings for image and model output
    data_im_dirname: data # Name of folder that sliced data 2D images will be output to
    seg_im_out_dirname: seg # Name of folder that sliced segmentation 2D images with be output to 
    model_output_fn: trained_2d_model # Suffix for the saved model filename
    clip_data: true # Clip and rescale the image data intensities before saving to disk
    st_dev_factor: 2.575 # The number of standard deviations from the mean to clip data to
    data_hdf5_path: /data # The internal HDF5 path to the image data
    seg_hdf5_path: /data # The internal HDF5 path to the label data

    # Settings for model training
    image_size: 256 # size of images in databunch used for training (must be multiple of 32)
    downsample: false # If True, data will be downsampled by 2
    training_set_proportion: 0.8 # Proportion of images to use the training, rest are used for validation
    cuda_device: 0 # The graphics card to use (between 0 and 3 for a machine with 4 GPUs)
    num_cyc_frozen: 8 # Number of times to run fit_one_cycle on frozen model
    num_cyc_unfrozen: 5 # Number of times to run fit_one_cycle on unfrozen model
    patience: 3 # Number of epoch to wait before early stopping if validation loss does not improve

    loss_criterion: "DiceLoss" # Choose from one of [BCEDiceLoss, BCELoss, DiceLoss, GeneralizedDiceLoss, CrossEntropyLoss]
    alpha: 0.75 # When BCEDiceLoss selected, weighting for BCELoss
    beta: 0.25 # When BCEDiceLoss selected, weighting for DiceLoss
    eval_metric: "MeanIoU" # Choose from one of [MeanIoU, GenericAveragePrecision]
    pct_lr_inc: 0.3 # the percentage of overall iterations where the LR is increasing
    # Parameters for finding learning rate
    starting_lr: 1e-6 # Lower bound of learning rate search
    end_lr: 50 # Upper Bound of learning rate search
    lr_find_epochs: 1 # Number of training epochs for learning rate search
    lr_reduce_factor: 500 # Divisor for start and end LR when finding LR on reloaded model
    plot_lr_graph: false # Set to True if gnuplot is installed and a terminal plot of learning rate is required 

    # Parameters to control model architecture
    model:
      # Choose type of segmenation model from the list of those tested so far
      # ["U_Net", "U_Net_Plus_plus", "FPN", "DeepLabV3", "DeepLabV3_Plus"]
      # "MA_Net", "Linknet"]
      type: "U_Net"
      # For more details on encoder types please see smp.readthedocs.io
      # choose encoder, those tested so far include the following:
      # ["resnet34", "resnet50", "resnext50_32x4d"]
      encoder_name: "resnet34"
      # use `imagenet` pre-trained weights for encoder initialization
      encoder_weights: "imagenet"
  predict_settings:
    # Settings for prediction of a 3d volume using a 2d model
    quality: medium # One of [low, medium, high]. low = Single axis, medium = 3 axis, high = 12 ways
    output_probs: false # If set to True a separate volume of probabilty values for the predictions will be saved alongside the labels.
    clip_data: true # If set to True, the image data intensities will be clipped and rescaled before prediction. Set to False if data is already clipped.
    st_dev_factor: 2.575 # Number of standard deviations from the mean to clip data to.
    data_hdf5_path: /data # Internal path in the HDF5 file where the image data for prediction is stored.
    cuda_device: 0 # Change this if you would like to run the prediction on a different GPU
    downsample: false # If True, data will be downsampled by 2 and segmentation will be upsampled by 2
    one_hot: false # Output one-hot encoded data
